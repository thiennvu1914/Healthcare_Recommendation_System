"""Core RAG Engine - Healthcare Consultation System"""
import re
import numpy as np
import pandas as pd
import torch
import pickle
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
from underthesea import word_tokenize
import faiss
from html import unescape
import logging

logger = logging.getLogger(__name__)

from api.config import settings

class HealthcareRAGEngine:
    """Main RAG Engine for Healthcare Consultation"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() and settings.USE_GPU else "cpu"
        print(f"[RAG Engine] Initializing on device: {self.device}")
        
        # Models
        self.tokenizer_phobert = None
        self.model_phobert = None
        self.gen_tokenizer = None
        self.gen_model = None
        self.gen_pipe = None
        
        # Indices
        self.qa_index = None
        self.article_index = None
        
        # Data
        self.df_qa = None
        self.df_articles = None
        self.article_texts = []
        
        # Action verbs for filtering
        self.action_verbs = self._load_action_verbs()
        
        # Text cleaning patterns
        self._init_text_patterns()
        
    def _init_text_patterns(self):
        """Initialize regex patterns for text cleaning"""
        self.re_at_prefix = re.compile(r'^@[^:]{0,60}:\s*', flags=re.IGNORECASE)
        self.name_pattern = re.compile(r'\b([A-ZÃ€-á»¸][a-zÃ -á»¹]+(?:_[A-ZÃ€-á»¸][a-zÃ -á»¹]+)+)\b')
        self.doctor_pattern = re.compile(r'\b(BS|BÃ¡c sÄ©|LÆ°Æ¡ng y|Dr)\.?\s+([A-ZÃ€-á»¸][a-zÃ -á»¹_]+(\s+[A-ZÃ€-á»¸][a-zÃ -á»¹_]+)*)', flags=re.IGNORECASE)
        self.pronoun_pattern = re.compile(r'\b(chÃ¡u|em|tá»›|mÃ¬nh|con|anh|chá»‹)\b', flags=re.IGNORECASE)
        
        self.connectives = [
            r'vÃ¬ váº­y', r'vÃ¬ tháº¿', r'váº­y nÃªn', r'do váº­y', r'vÃ¬ váº­y nÃªn', 
            r'vÃ¬ tháº¿ nÃªn', r'cho nÃªn', r'tÃ³m láº¡i', r'tÃ³m táº¯t', r'nhÆ°ng', r'tuy nhiÃªn'
        ]
        self.connective_pattern = re.compile("|".join([re.escape(x) for x in self.connectives]), flags=re.IGNORECASE)
        
    def _load_action_verbs(self) -> set:
        """Load action verbs for filtering actionable sentences"""
        return set([
            # NhÃ³m dÃ¹ng thuá»‘c / Ä‘iá»u trá»‹
            "uá»‘ng", "uá»‘ng thuá»‘c", "dÃ¹ng", "dÃ¹ng thuá»‘c", "xá»‹t", "bÃ´i", "thoa", "nhá»", "ngáº­m", 
            "tiÃªm", "chÃ­ch", "truyá»n", "pháº«u thuáº­t", "má»•", "tiá»ƒu pháº«u", "kÃª Ä‘Æ¡n", "Ä‘iá»u trá»‹",
            "chÆ°á»m", "chÆ°á»m nÃ³ng", "chÆ°á»m láº¡nh", "bÄƒng bÃ³", "sÃ¡t trÃ¹ng", "rá»­a váº¿t thÆ°Æ¡ng",
            "hÃºt rá»­a", "xÃ´ng", "khÃ­ dung", "chÃ¢m cá»©u", "báº¥m huyá»‡t", "massage", "xoa bÃ³p",
            
            # NhÃ³m khÃ¡m / xÃ©t nghiá»‡m
            "khÃ¡m", "Ä‘i khÃ¡m", "tÃ¡i khÃ¡m", "thÄƒm khÃ¡m", "kiá»ƒm tra", "xÃ©t nghiá»‡m", "láº¥y máº«u",
            "siÃªu Ã¢m", "chá»¥p", "chá»¥p x-quang", "chá»¥p ct", "chá»¥p mri", "ná»™i soi", "Ä‘o huyáº¿t Ã¡p",
            "Ä‘o Ä‘Æ°á»ng huyáº¿t", "theo dÃµi", "Ä‘Ã¡nh giÃ¡", "táº§m soÃ¡t",
            
            # NhÃ³m sinh hoáº¡t / dinh dÆ°á»¡ng
            "Äƒn", "Äƒn kiÃªng", "kiÃªng", "trÃ¡nh", "háº¡n cháº¿", "bá»• sung", "tÄƒng cÆ°á»ng", "giáº£m",
            "uá»‘ng nÆ°á»›c", "ngá»§", "nghá»‰ ngÆ¡i", "kÃª gá»‘i", "náº±m nghiÃªng", "táº­p", "táº­p luyá»‡n", 
            "váº­n Ä‘á»™ng", "táº­p váº­t lÃ½ trá»‹ liá»‡u", "thá»ƒ dá»¥c", "vá»‡ sinh", "sÃºc miá»‡ng", "sÃºc há»ng",
            "rá»­a tay", "rá»­a mÅ©i", "Ä‘eo kháº©u trang", "cÃ¡ch ly", "nháº­p viá»‡n", "cáº¥p cá»©u",

            # Bá»• sung tá»« dataset
            'Ä‘i', 'siÃªu', 'ná»™i', 'Ä‘áº·t', 'nhá»', 'bá»•', 'khÃ¡m vÃ ', 'trÃ¡nh thai', 'khÃ¡m bá»‡nh', 
            'Äƒn uá»‘ng', 'khÃ¡m bÃ¡c sÄ©', 'khÃ¡m sá»©c', 'khÃ¡m sá»©c khá»e', 'Ä‘i ngoÃ i', 'kÃª', 
            'Ä‘i siÃªu Ã¢m', 'khÃ¡m thai', 'Ä‘áº·t lá»‹ch', 'khÃ¡m láº¡i', 'Ä‘áº·t lá»‹ch khÃ¡m', 'tiÃªm chá»§ng',
            'Ä‘i khÃ¡m bÃ¡c', 'khÃ¡m phá»¥ khoa', 'Ä‘i khÃ¡m Ä‘á»ƒ', 'khÃ¡m chuyÃªn khoa', 'khÃ¡m phá»¥',
            'Ä‘i khÃ¡m vÃ ', 'khÃ¡m chuyÃªn', 'tiÃªm ngá»«a', 'Ä‘i láº¡i', 'rá»­a', 'Ä‘i tiá»ƒu', 'kiÃªng',
            'tiÃªm mÅ©i', 'khÃ¡m trá»±c', 'chá»¥p x quang', 'tiÃªm vacxin', 'khÃ¡m vÃ  Ä‘iá»u',
            'Ä‘i xÃ©t nghiá»‡m', 'Ä‘i khÃ¡m thai', 'Ä‘i khÃ¡m chuyÃªn', 'tiÃªm phÃ²ng', 'chá»¥p x',
            'cho bÃ© Ä‘i', 'Ä‘i tÃ¡i', 'Ä‘Æ°a bÃ© Ä‘i', 'Ä‘i xÃ©t', 'Ä‘i phÃ¢n', 'khÃ¡m tÆ°',
            'Ä‘i cáº§u', 'Ä‘i ngoÃ i phÃ¢n', 'Ä‘i tÃ¡i khÃ¡m', 'Ä‘áº¿n bá»‡nh viá»‡n', 'khÃ¡m thai Ä‘á»‹nh',
            'khÃ¡m vá»›i', 'tiÃªm vaccine', 'Ä‘i kiá»ƒm tra', 'Ä‘i tiÃªm', 'Ä‘i tiÃªu', 'Ä‘Æ°a bÃ© Ä‘áº¿n',
            'khÃ¡m trá»±c tiáº¿p', 'Ä‘áº·t khÃ¡m', 'Ä‘i khÃ¡m ngay', 'tiÃªm váº¯c xin', 'Ä‘áº·t tÆ° váº¥n',
            'khÃ¡m em', 'Ä‘áº·t thuá»‘c', 'khÃ¡m vá»›i bÃ¡c', 'tiÃªm váº¯c', 'khÃ¡m Ä‘á»ƒ Ä‘Æ°á»£c', 'Ä‘i lÃ m',
            'hÃºt thai', 'siÃªu Ã¢m tim', 'Ä‘i khÃ¡m táº¡i', 'tiÃªm Ä‘Æ°á»£c', 'khÃ¡m khÃ´ng', 'Äƒn vÃ ',
            'hÃºt thuá»‘c', 'lÃ m gÃ¬', 'Ä‘i khÃ¡m bá»‡nh', 'Ä‘i kiá»ƒm', 'siÃªu Ã¢m láº¡i', 'khÃ¡m Ä‘á»‹nh',
            'Ä‘áº¿n khÃ¡m táº¡i', 'Ä‘i vá»‡', 'Ä‘i vá»‡ sinh', 'tiÃªm thuá»‘c', 'dÃ¹ng biá»‡n phÃ¡p',
            'tiÃªm chá»§ng chuyÃªn', 'siÃªu Ã¢m thai', 'khÃ¡m Ä‘á»‹nh ká»³', 'kÃª Ä‘Æ¡n thuá»‘c', 'Äƒn Ä‘Æ°á»£c',
            'Äƒn khÃ´ng', 'liÃªn há»‡ vá»›i', 'khÃ¡m Ä‘á»ƒ bÃ¡c', 'kÃª thuá»‘c', 'uá»‘ng Ä‘á»§ nÆ°á»›c',
            'khÃ¡m táº¡i khoa', 'Ä‘áº·t vÃ²ng', 'khÃ¡m bá»‡nh chuyÃªn', 'khÃ¡m hiáº¿m muá»™n', 'thai',
            'dÃ¹ng biá»‡n', 'Ä‘áº·t cÃ¢u', 'Ä‘áº·t cÃ¢u há»i', 'uá»‘ng vÃ ', 'chá»¥p mri', 'khÃ¡m sá»›m',
            'khÃ¡m cho', 'uá»‘ng cÃ³', 'lÃ m gÃ¬ Ä‘á»ƒ', 'uá»‘ng bá»• sung', 'kÃª toa', 'siÃªu Ã¢m á»Ÿ',
            'ná»™i soi bÃ³c', 'siÃªu Ã¢m tháº¥y', 'khÃ¡m tÆ° váº¥n', 'khÃ¡m hiáº¿m', 'dÃ¹ng cho',
            'Ä‘i kÃ¨m', 'Äƒn Ä‘á»§', 'Äƒn cá»§a', 'khÃ¡m tá»•ng', 'khÃ¡m tá»•ng quÃ¡t', 'khÃ¡m vÃ  siÃªu'
        ])
    
    def load_models(self):
        """Load PhoBERT and Generation models"""
        print("[RAG Engine] Loading PhoBERT...")
        self.tokenizer_phobert = AutoTokenizer.from_pretrained(settings.RETRIEVAL_MODEL, use_fast=True)
        
        try:
            if self.device == "cuda":
                self.model_phobert = AutoModel.from_pretrained(
                    settings.RETRIEVAL_MODEL,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True
                )
            else:
                self.model_phobert = AutoModel.from_pretrained(settings.RETRIEVAL_MODEL)
            self.model_phobert.eval()
            print("[RAG Engine] PhoBERT loaded successfully")
        except Exception as e:
            print(f"[RAG Engine] Error loading PhoBERT: {e}")
            raise
        
        # Generation model (optional, load on demand)
        self.generation_model = None
        self.generation_tokenizer = None
        print("[RAG Engine] Generation model will be loaded on first use")
    
    def _load_generation_model(self):
        """Load Vistral-7B-Chat generation model"""
        if self.generation_model is not None:
            return  # Already loaded
        
        try:
            from transformers import AutoModelForCausalLM
            
            model_name = "Viet-Mistral/Vistral-7B-Chat"
            print(f"[RAG Engine] Loading generation model: {model_name}")
            
            self.generation_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
            
            if self.device == "cuda":
                self.generation_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True
                )
            else:
                self.generation_model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32
                )
                self.generation_model.to(self.device)
            
            self.generation_model.eval()
            print("[RAG Engine] Generation model loaded successfully")
        except Exception as e:
            print(f"[RAG Engine] Failed to load generation model: {e}")
            print("[RAG Engine] Will use template-based generation")
            self.generation_model = None
            self.generation_tokenizer = None
    
    def load_data(self):
        """Load Q&A and Articles data"""
        print(f"[RAG Engine] Loading data from {settings.DATA_DIR}")
        
        # Load QA
        self.df_qa = pd.read_csv(settings.QA_CSV_PATH)
        if len(self.df_qa) > settings.SAMPLE_SIZE:
            self.df_qa = self.df_qa.sample(n=settings.SAMPLE_SIZE, random_state=42).reset_index(drop=True)
        print(f"[RAG Engine] Loaded {len(self.df_qa)} Q&A records")
        
        # Preprocess QA
        for col in ["question", "answer", "advice"]:
            if col in self.df_qa.columns:
                self.df_qa[col] = self.df_qa[col].fillna("").astype(str).apply(self._preprocess_text)
        
        # Load Articles
        if settings.ARTICLES_CSV_PATH.exists():
            self.df_articles = pd.read_csv(settings.ARTICLES_CSV_PATH)
            if len(self.df_articles) > settings.SAMPLE_SIZE:
                self.df_articles = self.df_articles.sample(n=settings.SAMPLE_SIZE, random_state=42).reset_index(drop=True)
            print(f"[RAG Engine] Loaded {len(self.df_articles)} articles")
            
            # Prepare article texts
            for _, row in self.df_articles.iterrows():
                title = str(row.get("title", "")).strip()
                text = str(row.get("text", "")).strip()
                self.article_texts.append((
                    str(row.get("id", "")),
                    title,
                    f"{title}\n\n{text}"
                ))
        else:
            print("[RAG Engine] Articles file not found, skipping")
            self.df_articles = pd.DataFrame()
    
    def build_indices(self):
        """Build HNSW indices for fast retrieval with caching support"""
        # Try to load from cache first (fix issue #6: avoid rebuild every time)
        if settings.ENABLE_CACHE and self._try_load_indices():
            logger.info("[RAG Engine] Successfully loaded indices from cache")
            return
        
        logger.info("[RAG Engine] Building HNSW indices from scratch...")
        
        # Build QA index
        qa_texts = (self.df_qa["question"].fillna("") + " " + self.df_qa["answer"].fillna("")).tolist()
        qa_embeddings = []
        
        print(f"[RAG Engine] Encoding {len(qa_texts)} Q&A pairs...")
        for i, text in enumerate(qa_texts):
            if i % 500 == 0:
                print(f"  Progress: {i}/{len(qa_texts)}")
            qa_embeddings.append(self._sentence_embedding(text))
        
        qa_embeddings = np.vstack(qa_embeddings).astype("float32")
        self.qa_index = self._build_hnsw_index(qa_embeddings)
        print(f"[RAG Engine] QA index built with {self.qa_index.ntotal} elements")
        
        # Build Article index
        article_embeddings = None
        if self.article_texts:
            print(f"[RAG Engine] Encoding {len(self.article_texts)} articles...")
            article_embeddings = []
            for i, (_, _, content) in enumerate(self.article_texts):
                if i % 500 == 0:
                    print(f"  Progress: {i}/{len(self.article_texts)}")
                article_embeddings.append(self._sentence_embedding(content))
            
            article_embeddings = np.vstack(article_embeddings).astype("float32")
            self.article_index = self._build_hnsw_index(article_embeddings)
            print(f"[RAG Engine] Article index built with {self.article_index.ntotal} elements")
        
        # Save to cache
        if settings.ENABLE_CACHE:
            self._save_indices(qa_embeddings, article_embeddings)
    
    def _save_indices(self, qa_embeddings: np.ndarray, article_embeddings: Optional[np.ndarray] = None):
        """Save HNSW indices and embeddings to cache"""
        try:
            settings.CACHE_DIR.mkdir(parents=True, exist_ok=True)
            
            # Save HNSW indices
            if self.qa_index:
                faiss.write_index(self.qa_index, str(settings.QA_INDEX_CACHE))
                logger.info(f"Saved QA index to {settings.QA_INDEX_CACHE}")
            
            if self.article_index:
                faiss.write_index(self.article_index, str(settings.ARTICLE_INDEX_CACHE))
                logger.info(f"Saved article index to {settings.ARTICLE_INDEX_CACHE}")
            
            # Save embeddings
            if qa_embeddings is not None:
                np.save(settings.QA_EMBEDDINGS_CACHE, qa_embeddings)
                logger.info(f"Saved QA embeddings to {settings.QA_EMBEDDINGS_CACHE}")
            
            if article_embeddings is not None:
                np.save(settings.ARTICLE_EMBEDDINGS_CACHE, article_embeddings)
                logger.info(f"Saved article embeddings to {settings.ARTICLE_EMBEDDINGS_CACHE}")
            
            # Save metadata (data hash for validation)
            metadata = {
                "qa_count": len(self.df_qa),
                "article_count": len(self.article_texts),
                "sample_size": settings.SAMPLE_SIZE,
                "data_hash": self._compute_data_hash()
            }
            with open(settings.METADATA_CACHE, "wb") as f:
                pickle.dump(metadata, f)
            
            logger.info("[RAG Engine] All indices saved to cache successfully")
        except Exception as e:
            logger.warning(f"Failed to save indices to cache: {e}")
    
    def _try_load_indices(self) -> bool:
        """Try to load indices from cache"""
        try:
            # Check if cache files exist
            if not all([
                settings.QA_INDEX_CACHE.exists(),
                settings.METADATA_CACHE.exists()
            ]):
                logger.info("Cache files not found, will build from scratch")
                return False
            
            # Load and validate metadata
            with open(settings.METADATA_CACHE, "rb") as f:
                metadata = pickle.load(f)
            
            # Validate data hasn't changed
            current_hash = self._compute_data_hash()
            if metadata.get("data_hash") != current_hash:
                logger.info("Data has changed, invalidating cache")
                return False
            
            # Load QA index
            self.qa_index = faiss.read_index(str(settings.QA_INDEX_CACHE))
            logger.info(f"Loaded QA index with {self.qa_index.ntotal} elements")
            
            # Load article index if exists
            if settings.ARTICLE_INDEX_CACHE.exists() and len(self.article_texts) > 0:
                self.article_index = faiss.read_index(str(settings.ARTICLE_INDEX_CACHE))
                logger.info(f"Loaded article index with {self.article_index.ntotal} elements")
            
            return True
            
        except Exception as e:
            logger.warning(f"Failed to load indices from cache: {e}")
            return False
    
    def _compute_data_hash(self) -> str:
        """Compute hash of data files to detect changes"""
        try:
            hash_obj = hashlib.md5()
            
            # Hash QA file
            if settings.QA_CSV_PATH.exists():
                hash_obj.update(str(settings.QA_CSV_PATH.stat().st_mtime).encode())
                hash_obj.update(str(settings.QA_CSV_PATH.stat().st_size).encode())
            
            # Hash articles file
            if settings.ARTICLES_CSV_PATH.exists():
                hash_obj.update(str(settings.ARTICLES_CSV_PATH.stat().st_mtime).encode())
                hash_obj.update(str(settings.ARTICLES_CSV_PATH.stat().st_size).encode())
            
            # Include sample size in hash
            hash_obj.update(str(settings.SAMPLE_SIZE).encode())
            
            return hash_obj.hexdigest()
        except Exception as e:
            logger.warning(f"Failed to compute data hash: {e}")
            return "unknown"
    
    def _preprocess_text(self, s: str) -> str:
        """Preprocess Vietnamese text"""
        if not isinstance(s, str):
            return ""
        s = re.sub(r'\s+', ' ', s.strip())
        try:
            return word_tokenize(s, format="text")
        except:
            return s
    
    def _preprocess_reference_sentence(self, s: str) -> str:
        """Preprocess reference sentence for embedding (remove names, pronouns, etc.)"""
        if not s:
            return ""
        
        s = s.strip()
        
        # Skip questions
        if s.endswith('?'):
            return ""
        
        # Remove @ prefix
        s = self.re_at_prefix.sub("", s)
        
        # Remove "tráº£ lá»i"
        s = re.sub(r'^tráº£[_\s]lá»i\s*[:.]?\s*', '', s, flags=re.IGNORECASE)
        
        # Remove names with underscore
        s = self.name_pattern.sub("", s)
        
        # Remove doctor names
        s = self.doctor_pattern.sub("", s)
        
        # Replace pronouns with "báº¡n"
        s = self.pronoun_pattern.sub("báº¡n", s)
        
        # Remove connectives
        s = self.connective_pattern.sub("", s)
        
        # Clean whitespace
        s = re.sub(r'\s+', ' ', s).strip()
        
        return s
    
    def _clean_text(self, t: str) -> str:
        """Clean text - unescape HTML and normalize whitespace"""
        return re.sub(r'\s+', ' ', unescape(t.strip())).strip()
    
    def _sentence_has_action(self, s: str) -> bool:
        """Check if sentence contains action verbs"""
        if not s:
            return False
        
        sl = s.lower()
        for act in self.action_verbs:
            act_norm = act.replace("_", " ").lower()
            if re.search(r'\b' + re.escape(act_norm) + r'\b', sl):
                return True
        return False
    
    def _sentence_embedding(self, text: str) -> np.ndarray:
        """Generate sentence embedding using PhoBERT"""
        if not text:
            return np.zeros(768, dtype=np.float32)
        
        try:
            inputs = self.tokenizer_phobert(text, return_tensors="pt", truncation=True, max_length=256)
            model_device = next(self.model_phobert.parameters()).device
            inputs = {k: v.to(model_device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model_phobert(**inputs)
                last_hidden = outputs.last_hidden_state
                attention_mask = inputs.get("attention_mask")
                
                if attention_mask is not None:
                    attention_mask = attention_mask.unsqueeze(-1)
                    masked = last_hidden * attention_mask
                    summed = masked.sum(dim=1)
                    counts = attention_mask.sum(dim=1).clamp(min=1e-9)
                    mean_pooled = (summed / counts).squeeze().cpu().numpy().astype("float32")
                else:
                    mean_pooled = last_hidden.mean(dim=1).squeeze().cpu().numpy().astype("float32")
            
            return mean_pooled
        except Exception as e:
            print(f"[Embedding Error] {e}")
            return np.zeros(768, dtype=np.float32)
    
    def _build_hnsw_index(self, vectors: np.ndarray) -> Optional[faiss.Index]:
        """Build HNSW index from vectors using faiss"""
        if vectors is None or vectors.size == 0:
            return None
        
        try:
            dim = vectors.shape[1]
            # Use IndexFlatIP for inner product (similar to cosine similarity)
            index = faiss.IndexFlatIP(dim)
            # Normalize vectors for cosine similarity
            faiss.normalize_L2(vectors)
            index.add(vectors)
            return index
        except Exception as e:
            print(f"[HNSW Error] {e}")
            return None
    
    def retrieve_qa(self, query: str, k: int = 5) -> List[Dict]:
        """Retrieve top-k Q&A pairs"""
        if self.qa_index is None:
            return []
        
        query_processed = self._preprocess_text(query)
        user_emb = self._sentence_embedding(query_processed).reshape(1, -1)
        
        # Normalize query embedding for cosine similarity
        faiss.normalize_L2(user_emb)
        
        raw_k = min(k * 3, self.qa_index.ntotal)
        distances, labels = self.qa_index.search(user_emb, k=raw_k)
        
        results = []
        for dist, idx in zip(distances[0], labels[0]):
            if idx < 0 or idx >= len(self.df_qa):
                continue
            
            row = self.df_qa.iloc[int(idx)]
            score = float(1.0 - dist)
            
            if score >= settings.QUESTION_SIM_THRESHOLD:
                results.append({
                    "score": score,
                    "index": int(idx),
                    "question": row.get("question", ""),
                    "answer": row.get("answer", ""),
                    "topic": row.get("topic", "KhÃ¡c")
                })
            
            if len(results) >= k:
                break
        
        return results
    
    def retrieve_articles(self, query: str, k: int = 1) -> List[Dict]:
        """Retrieve top-k articles with re-ranking"""
        if self.article_index is None or not self.article_texts:
            return []
        
        query_processed = self._preprocess_text(query)
        user_emb = self._sentence_embedding(query_processed).reshape(1, -1)
        q_tokens_set = set([t.lower() for t in re.findall(r'\w+', query) if len(t) >= 2])
        
        # Normalize query embedding for cosine similarity
        faiss.normalize_L2(user_emb)
        
        raw_k = min(k * 3, self.article_index.ntotal)
        distances, labels = self.article_index.search(user_emb, k=raw_k)
        
        # Get raw candidates
        raw_candidates = []
        for dist, idx in zip(distances[0], labels[0]):
            if idx < 0 or idx >= len(self.article_texts):
                continue
            
            raw_candidates.append({
                "index": int(idx),
                "score": float(1.0 - dist)
            })
        
        # Re-rank with lexical overlap and title boost
        reranked = []
        w_sim = 0.75
        w_lex = 0.20
        w_title_boost = 0.05
        
        for c in raw_candidates:
            idx = int(c['index'])
            link, title, content = self.article_texts[idx]
            baseline_sim = float(c.get('score', 0.0))
            
            # Lexical overlap with title + snippet
            article_snippet = title + " " + (content[:1000] if content else "")
            art_tokens = set([t.lower() for t in re.findall(r'\w+', article_snippet) if len(t) >= 2])
            
            lex_overlap = 0.0
            if q_tokens_set:
                common = len(q_tokens_set & art_tokens)
                lex_overlap = common / max(1, len(q_tokens_set))
            
            # Title boost
            title_tokens = set([t.lower() for t in re.findall(r'\w+', title) if len(t) >= 2])
            title_boost_flag = 1.0 if (q_tokens_set & title_tokens) else 0.0
            
            # Combined score
            combined = w_sim * baseline_sim + w_lex * lex_overlap + w_title_boost * title_boost_flag
            
            # Find best passage
            passages = self._chunk_text(content, max_chars=600)
            best_passage = ""
            best_passage_sim = -1.0
            
            for p in passages[:6]:
                p_proc = self._preprocess_text(p)
                p_emb = self._sentence_embedding(p_proc).astype("float32")
                sim_p = self._cosine_sim(user_emb.reshape(-1), p_emb)
                
                if sim_p > best_passage_sim:
                    best_passage_sim = sim_p
                    best_passage = p
            
            if not best_passage and content:
                best_passage = content[:600]
            
            reranked.append({
                "index": idx,
                "link": link,
                "title": title,
                "txt": content,
                "baseline_sim": baseline_sim,
                "lex_overlap": lex_overlap,
                "title_boost": title_boost_flag,
                "combined_score": combined,
                "best_passage": best_passage,
                "best_passage_sim": best_passage_sim,
                "score": combined  # For compatibility
            })
        
        # Sort by combined score
        reranked_sorted = sorted(reranked, key=lambda x: x["combined_score"], reverse=True)
        
        # Filter by minimum score
        final = [r for r in reranked_sorted if r["combined_score"] >= settings.COMBINED_SCORE_THRESHOLD]
        
        if not final:
            return []
        
        # Return top-k
        results = []
        for r in final[:k]:
            results.append({
                "score": r["combined_score"],
                "index": r["index"],
                "title": r["title"],
                "link": r["link"],
                "snippet": r["best_passage"]
            })
        
        return results
    
    def _chunk_text(self, text: str, max_chars: int = 500, overlap_ratio: float = 0.15) -> List[str]:
        """Chunk text into overlapping passages"""
        if not text:
            return []
        
        passages = []
        start = 0
        L = len(text)
        overlap_chars = int(max_chars * overlap_ratio)
        
        while start < L:
            end = start + max_chars
            if end >= L:
                passages.append(text[start:L].strip())
                break
            
            # Find sentence boundary
            cut = text.rfind('.', start, end)
            if cut <= start:
                cut = end
            
            passages.append(text[start:cut].strip())
            
            # Move with overlap
            if overlap_chars > 0:
                start = max(cut - overlap_chars, start + 1)
            else:
                start = cut
        
        return [p for p in passages if p]
    
    def _cosine_sim(self, a: np.ndarray, b: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors"""
        if a is None or b is None:
            return 0.0
        na = np.linalg.norm(a)
        nb = np.linalg.norm(b)
        if na == 0 or nb == 0:
            return 0.0
        return float(np.dot(a, b) / (na * nb))
    
    def find_best_action_sentence(
        self, 
        user_text: str,
        topk_rows: List[Dict],
        sent_sim_thresh: float = 0.6,
        combined_thresh: float = 0.68,
        alpha: float = 0.75,
        beta: float = 0.2,
        gamma: float = 0.05
    ) -> Tuple[Optional[str], Optional[int], float]:
        """
        Find best action sentence from top-k Q&A results
        
        Args:
            user_text: User query
            topk_rows: List of top-k Q&A results
            sent_sim_thresh: Sentence similarity threshold
            combined_thresh: Combined score threshold
            alpha: Weight for sentence similarity
            beta: Weight for question similarity
            gamma: Weight for lexical overlap
            
        Returns:
            (action_text, ref_index, score)
        """
        if not topk_rows:
            return None, None, 0.0
        
        # Extract and preprocess all sentences
        all_sents = []
        for ref_pos, r in enumerate(topk_rows, start=1):
            question_text = r.get("question", "") or ""
            raw_answer = r.get("answer", "") or ""
            
            # Split answer into sentences
            sents = re.split(r'(?<=[.!?])\s+', raw_answer.strip()) if raw_answer else []
            
            for s in sents:
                s_orig = self._clean_text(s)
                s_proc = self._preprocess_reference_sentence(s_orig)
                
                if len(s_proc) >= 6:
                    all_sents.append((ref_pos, question_text, s_orig, s_proc))
        
        if not all_sents:
            return None, None, 0.0
        
        # Get user embedding
        user_q = self._preprocess_text(user_text)
        user_emb = self._sentence_embedding(user_q).astype("float32")
        user_tokens_set = set([t.lower() for t in re.findall(r'\w+', user_text) if len(t) >= 2])
        
        # Cache question embeddings
        question_emb_cache = {}
        scored = []
        
        for ref_pos, question_text, sent_orig, sent_proc in all_sents:
            # Get question embedding
            if ref_pos not in question_emb_cache:
                q_text_proc = self._preprocess_text(question_text) if question_text else ""
                if q_text_proc:
                    question_emb_cache[ref_pos] = self._sentence_embedding(q_text_proc).astype("float32")
                else:
                    question_emb_cache[ref_pos] = np.zeros(user_emb.shape, dtype=np.float32)
            
            # Get sentence embedding
            s_emb = self._sentence_embedding(sent_proc).astype("float32")
            
            # Calculate similarities
            sim_sent = self._cosine_sim(user_emb, s_emb)
            sim_q = self._cosine_sim(user_emb, question_emb_cache[ref_pos])
            
            # Lexical overlap
            sent_tokens_set = set([t.lower() for t in re.findall(r'\w+', sent_proc) if len(t) >= 2])
            lex_overlap = float(len(user_tokens_set & sent_tokens_set)) / max(1, len(user_tokens_set)) if user_tokens_set else 0.0
            
            # Combined score
            combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap
            
            scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))
        
        # Sort by combined score
        scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)
        
        # Find best action sentence
        best_ref_pos = None
        best_combined_score = 0.0
        
        for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:
            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:
                if self._sentence_has_action(sent_proc) or self._sentence_has_action(sent_orig):
                    best_ref_pos = ref_pos
                    best_combined_score = combined
                    break
        
        if best_ref_pos is None:
            return None, None, 0.0
        
        # Collect all action sentences from best reference
        final_sentences_list = []
        for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:
            if ref_pos == best_ref_pos:
                if sim_sent >= sent_sim_thresh and combined >= combined_thresh:
                    if self._sentence_has_action(sent_proc) or self._sentence_has_action(sent_orig):
                        # Replace pronouns
                        s_final = self.pronoun_pattern.sub("báº¡n", sent_orig)
                        s_final = re.sub(r'\s+', ' ', s_final).strip()
                        
                        if s_final not in final_sentences_list:
                            final_sentences_list.append(s_final)
        
        if not final_sentences_list:
            return None, None, 0.0
        
        final_paragraph = " ".join(final_sentences_list)
        return final_paragraph, best_ref_pos, best_combined_score
    
    def generate_answer(self, query: str, qa_results: List[Dict], article_results: List[Dict]) -> Tuple[str, str, float]:
        """Generate natural answer using retrieved context with medical safety checks"""
        # Determine specialty
        specialty = "Y táº¿ tá»•ng quÃ¡t"
        if qa_results:
            specialty = qa_results[0].get("topic", "Y táº¿ tá»•ng quÃ¡t")
        
        # Medical safety check (fix issue #9: detect emergencies)
        emergency_detected, emergency_type = self._check_emergency_keywords(query)
        
        # Build context from retrieved results
        context_parts = []
        
        # Add Q&A context
        for i, qa in enumerate(qa_results[:3], 1):
            answer = qa.get('answer', '')
            if answer:
                context_parts.append(f"ThÃ´ng tin {i}: {self._truncate_text(answer, 300)}")
        
        # Add article context
        if article_results:
            snippet = article_results[0].get('snippet', '')
            if snippet:
                context_parts.append(f"BÃ i viáº¿t tham kháº£o: {self._truncate_text(snippet, 200)}")
        
        retrieved_content = "\n\n".join(context_parts)
        
        # Calculate confidence
        confidence = 0.5
        if qa_results:
            confidence = min(0.95, qa_results[0]['score'] + 0.1)
        
        # If emergency detected, return urgent warning
        if emergency_detected:
            logger.warning(f"Emergency detected: {emergency_type} in query: {query[:50]}...")
            answer = self._generate_emergency_response(emergency_type, specialty)
            return answer, specialty, 0.95  # High confidence for emergency warnings
        
        # Try to use LLM generation, fallback to template
        if self.generation_model is None and not settings.FORCE_CPU:
            try:
                self._load_generation_model()
            except:
                pass
        
        if self.generation_model is not None:
            answer = self._generate_with_llm(query, retrieved_content, specialty)
        else:
            answer = self._generate_template_answer(query, retrieved_content, specialty)
        
        # Add disclaimer to all medical responses (fix issue #9: guardrails)
        answer = self._add_medical_disclaimer(answer, confidence)
        
        return answer, specialty, confidence
    
    def _check_emergency_keywords(self, query: str) -> Tuple[bool, str]:
        """Detect emergency medical situations (fix issue #9: safety guardrails)"""
        query_lower = query.lower()
        
        # Critical emergency keywords
        critical_keywords = {
            "nguy_ká»‹ch": ["nguy ká»‹ch", "hÃ´n mÃª", "báº¥t tá»‰nh", "ngáº¥t xá»‰u", "thá»Ÿ gáº¥p", "khÃ³ thá»Ÿ náº·ng", "co giáº­t"],
            "cháº£y_mÃ¡u": ["cháº£y mÃ¡u nhiá»u", "xuáº¥t huyáº¿t", "mÃ¡u cháº£y khÃ´ng ngá»«ng", "mÃ¡u Ä‘á» tÆ°Æ¡i"],
            "Ä‘au_ngá»±c": ["Ä‘au ngá»±c dá»¯ dá»™i", "Ä‘au tháº¯t ngá»±c", "Ä‘au tim", "ngháº¹t ngá»±c"],
            "Ä‘á»™t_quá»µ": ["liá»‡t ná»­a ngÆ°á»i", "mÃ©o miá»‡ng", "nÃ³i láº¯p", "yáº¿u má»™t bÃªn", "Ä‘á»™t quá»µ"],
            "tai_náº¡n": ["tai náº¡n nghiÃªm trá»ng", "gÃ£y xÆ°Æ¡ng", "cháº¥n thÆ°Æ¡ng náº·ng", "xe Ä‘Ã¢m"],
            "ngá»™_Ä‘á»™c": ["ngá»™ Ä‘á»™c", "uá»‘ng nháº§m", "Äƒn pháº£i", "ngáº¥t sau khi Äƒn"]
        }
        
        for emergency_type, keywords in critical_keywords.items():
            if any(kw in query_lower for kw in keywords):
                return True, emergency_type
        
        return False, ""
    
    def _generate_emergency_response(self, emergency_type: str, specialty: str) -> str:
        """Generate urgent response for emergency situations"""
        responses = {
            "nguy_ká»‹ch": "âš ï¸ TÃŒNH HUá»NG KHáº¨N Cáº¤P: Triá»‡u chá»©ng báº¡n mÃ´ táº£ CÃ“ THá»‚ nghiÃªm trá»ng. NGAY Láº¬P Tá»¨C:\n1. Gá»ŒI 115 (cáº¥p cá»©u) hoáº·c Ä‘Æ°a ngÆ°á»i bá»‡nh Ä‘áº¿n bá»‡nh viá»‡n Gáº¦N NHáº¤T\n2. Giá»¯ bÃ¬nh tÄ©nh, theo dÃµi Ã½ thá»©c vÃ  nhá»‹p thá»Ÿ\n3. KHÃ”NG tá»± Ã½ cho uá»‘ng thuá»‘c\n\nÄÃ¢y KHÃ”NG pháº£i lá»i khuyÃªn thay tháº¿ cáº¥p cá»©u y táº¿ chuyÃªn nghiá»‡p.",
            
            "cháº£y_mÃ¡u": "âš ï¸ Cáº¢NH BÃO: Cháº£y mÃ¡u nhiá»u cáº§n Xá»¬ TRÃ NGAY:\n1. áº¤n trá»±c tiáº¿p vÃ o váº¿t thÆ°Æ¡ng báº±ng váº£i sáº¡ch\n2. NÃ¢ng cao vá»‹ trÃ­ bá»‹ thÆ°Æ¡ng (náº¿u cÃ³ thá»ƒ)\n3. Gá»ŒI 115 hoáº·c Ä‘áº¿n cáº¥p cá»©u NGAY náº¿u mÃ¡u khÃ´ng cáº§m\n4. KHÃ”NG bá» bÄƒng ra khi mÃ¡u Ä‘Ã£ Ä‘Ã´ng\n\nCáº§n Ä‘Ã¡nh giÃ¡ y táº¿ KHáº¨N Cáº¤P.",
            
            "Ä‘au_ngá»±c": "âš ï¸ KHáº¨N Cáº¤P TIM Máº CH: Äau ngá»±c cÃ³ thá»ƒ lÃ  dáº¥u hiá»‡u nhá»“i mÃ¡u cÆ¡ tim.\nHÃ€NH Äá»˜NG NGAY:\n1. Gá»ŒI 115 hoáº·c Ä‘áº¿n cáº¥p cá»©u NGAY Láº¬P Tá»¨C\n2. Ngá»“i nghá»‰, KHÃ”NG váº­n Ä‘á»™ng\n3. Náº¿u cÃ³ sáºµn: nhai 1 viÃªn aspirin 300mg (trá»« khi dá»‹ á»©ng)\n4. Theo dÃµi nhá»‹p tim, hÃ´ háº¥p\n\nThá»i gian lÃ  vÃ ng - Má»–I PHÃšT trÃ¬ hoÃ£n lÃ m tÄƒng nguy cÆ¡.",
            
            "Ä‘á»™t_quá»µ": "âš ï¸ Dáº¤U HIá»†U Äá»˜T QUá»´ - HÃ€NH Äá»˜NG NGAY:\nTEST NHANH (FAST):\n- Face (máº·t): cÆ°á»i cÃ³ mÃ©o miá»‡ng?\n- Arms (tay): giÆ¡ 2 tay cÃ³ tay nÃ o yáº¿u?\n- Speech (nÃ³i): nÃ³i cÃ³ láº¯p?\n- Time (thá»i gian): Gá»ŒI 115 NGAY!\n\nâœ… ÄÆ¯A Bá»†NH NHÃ‚N Äáº¾N Bá»†NH VIá»†N TRONG 4.5 GIá»œ Äáº¦U\nâŒ KHÃ”NG cho Äƒn, uá»‘ng (nguy cÆ¡ sáº·c)\n\nÄá»™t quá»µ lÃ  KHáº¨N Cáº¤P Y Táº¾!",
            
            "tai_náº¡n": "âš ï¸ TAI Náº N - Cáº¦N Há»– TRá»¢ Y Táº¾:\n1. Äáº£m báº£o an toÃ n hiá»‡n trÆ°á»ng\n2. Gá»ŒI 115 náº¿u: cháº¥n thÆ°Æ¡ng Ä‘áº§u/cá»™t sá»‘ng, gÃ£y xÆ°Æ¡ng, cháº£y mÃ¡u nhiá»u\n3. KHÃ”NG di chuyá»ƒn ngÆ°á»i bá»‹ thÆ°Æ¡ng (trá»« khi nguy hiá»ƒm)\n4. Giá»¯ áº¥m, theo dÃµi Ã½ thá»©c\n\nCháº¥n thÆ°Æ¡ng cáº§n Ä‘Æ°á»£c bÃ¡c sÄ© ÄÃNH GIÃ CHUYÃŠN MÃ”N.",
            
            "ngá»™_Ä‘á»™c": "âš ï¸ NGá»˜ Äá»˜C - Xá»¬ TRÃ KHáº¨N:\n1. Gá»ŒI 115 hoáº·c Trung tÃ¢m Chá»‘ng Ä‘á»™c: (028) 3829 2345\n2. Mang theo bao bÃ¬/máº«u cháº¥t nghi ngá»\n3. KHÃ”NG tá»± Ã½ gÃ¢y nÃ´n (trá»« khi bÃ¡c sÄ© chá»‰ dáº«n)\n4. Náº¿u hÃ³a cháº¥t dÃ­nh da: rá»­a sáº¡ch báº±ng nÆ°á»›c 15-20 phÃºt\n\nNgá»™ Ä‘á»™c Cáº¦N Ä‘iá»u trá»‹ chuyÃªn khoa NGAY."
        }
        
        return responses.get(emergency_type, 
            "âš ï¸ Triá»‡u chá»©ng báº¡n mÃ´ táº£ cÃ³ thá»ƒ nghiÃªm trá»ng. Vui lÃ²ng Gá»ŒI 115 hoáº·c Ä‘áº¿n cÆ¡ sá»Ÿ y táº¿ Gáº¦N NHáº¤T Ä‘á»ƒ Ä‘Æ°á»£c khÃ¡m vÃ  tÆ° váº¥n chuyÃªn mÃ´n.")
    
    def _add_medical_disclaimer(self, answer: str, confidence: float) -> str:
        """Add appropriate medical disclaimer based on confidence (fix issue #9)"""
        if confidence < 0.6:
            disclaimer = "\n\nâš ï¸ LÆ¯U Ã QUAN TRá»ŒNG: ThÃ´ng tin trÃªn chá»‰ mang tÃ­nh tham kháº£o vá»›i Ä‘á»™ tin cáº­y THáº¤P. Báº¡n NÃŠN Ä‘i khÃ¡m trá»±c tiáº¿p táº¡i cÆ¡ sá»Ÿ y táº¿ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c."
        elif confidence < 0.8:
            disclaimer = "\n\nðŸ“‹ LÆ°u Ã½: ThÃ´ng tin trÃªn mang tÃ­nh tham kháº£o. Náº¿u triá»‡u chá»©ng kÃ©o dÃ i hoáº·c náº·ng hÆ¡n, vui lÃ²ng Ä‘áº¿n gáº·p bÃ¡c sÄ© Ä‘á»ƒ Ä‘Æ°á»£c khÃ¡m vÃ  Ä‘iá»u trá»‹ phÃ¹ há»£p."
        else:
            disclaimer = "\n\nðŸ’¡ Lá»i khuyÃªn tá»« há»‡ thá»‘ng AI chá»‰ mang tÃ­nh cháº¥t tham kháº£o. Äá»ƒ Ä‘Æ°á»£c cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c vÃ  Ä‘iá»u trá»‹ an toÃ n, báº¡n nÃªn Ä‘i khÃ¡m trá»±c tiáº¿p táº¡i cÆ¡ sá»Ÿ y táº¿."
        
        # Add general disclaimer
        disclaimer += "\n\nðŸ¥ Há»‡ thá»‘ng nÃ y KHÃ”NG thay tháº¿ cho Ã½ kiáº¿n cá»§a bÃ¡c sÄ©. Trong trÆ°á»ng há»£p kháº©n cáº¥p, hÃ£y gá»i 115 hoáº·c Ä‘áº¿n bá»‡nh viá»‡n ngay."
        
        return answer + disclaimer
    
    def _truncate_text(self, text: str, max_chars: int) -> str:
        """Truncate text to complete sentences"""
        if len(text) <= max_chars:
            return text
        
        # Find last complete sentence
        truncated = text[:max_chars]
        for sep in ['. ', '! ', '? ', '; ']:
            idx = truncated.rfind(sep)
            if idx > max_chars * 0.5:  # At least 50% of max_chars
                return truncated[:idx + 1].strip()
        
        # No sentence boundary, cut at word
        idx = truncated.rfind(' ')
        if idx > 0:
            return truncated[:idx] + "..."
        
        return truncated + "..."
    
    def _generate_with_llm(self, query: str, context: str, specialty: str) -> str:
        """Generate answer using Vistral-7B-Chat"""
        if self.generation_model is None or self.generation_tokenizer is None:
            return self._generate_template_answer(query, context, specialty)
        
        # Build prompt following Vistral format
        system_prompt = f"""Báº¡n lÃ  trá»£ lÃ½ y táº¿ AI chuyÃªn vá» {specialty}. 
Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  cung cáº¥p lá»i khuyÃªn y táº¿ dá»±a trÃªn thÃ´ng tin tham kháº£o Ä‘Æ°á»£c cung cáº¥p.

Quy táº¯c:
1. Tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng báº±ng tiáº¿ng Viá»‡t
2. Dá»±a vÃ o thÃ´ng tin tham kháº£o Ä‘á»ƒ Ä‘Æ°a ra cÃ¢u tráº£ lá»i
3. Káº¿t thÃºc báº±ng lá»i khuyÃªn nÃªn Ä‘i khÃ¡m bÃ¡c sÄ© chuyÃªn khoa náº¿u cáº§n
4. KhÃ´ng Ä‘Æ°a ra cháº©n Ä‘oÃ¡n cháº¯c cháº¯n, chá»‰ tÆ° váº¥n tham kháº£o"""

        user_message = f"""ThÃ´ng tin tham kháº£o:
{context}

CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {query}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin tham kháº£o trÃªn."""

        # Format prompt (Vistral uses ChatML format)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            # Apply chat template
            prompt = self.generation_tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = self.generation_tokenizer(
                prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.generation_tokenizer.eos_token_id
                )
            
            # Decode
            response = self.generation_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"[RAG Engine] LLM generation failed: {e}")
            return self._generate_template_answer(query, context, specialty)
    
    def _generate_template_answer(self, query: str, context: str, specialty: str) -> str:
        """Generate answer using template (fallback if no LLM)"""
        answer = f"ChÃ o báº¡n, dá»±a trÃªn thÃ´ng tin y táº¿ liÃªn quan Ä‘áº¿n {specialty}:\n\n"
        answer += context[:400] + "\n\n"
        answer += f"CÃ¢u tráº£ lá»i chá»‰ mang tÃ­nh cháº¥t tham kháº£o. Báº¡n nÃªn Ä‘i khÃ¡m trá»±c tiáº¿p táº¡i chuyÃªn khoa {specialty} Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c hÆ¡n."
        return answer
    
    def get_specialties(self) -> List[Dict]:
        """Get list of available specialties"""
        if self.df_qa is None or 'topic' not in self.df_qa.columns:
            return []
        
        specialty_counts = self.df_qa['topic'].value_counts().to_dict()
        return [
            {"name": name, "count": count}
            for name, count in sorted(specialty_counts.items(), key=lambda x: x[1], reverse=True)
        ]

# Global instance
_rag_engine: Optional[HealthcareRAGEngine] = None

def get_rag_engine() -> HealthcareRAGEngine:
    """Get or create RAG engine singleton"""
    global _rag_engine
    if _rag_engine is None:
        _rag_engine = HealthcareRAGEngine()
        _rag_engine.load_models()
        _rag_engine.load_data()
        _rag_engine.build_indices()
    return _rag_engine

"""Core RAG Engine - Healthcare Consultation System"""
import os
import re
import numpy as np
import pandas as pd
import torch
import pickle
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
from underthesea import word_tokenize
import faiss
from html import unescape
import logging

logger = logging.getLogger(__name__)

from api.config import settings

class HealthcareRAGEngine:
    """Main RAG Engine for Healthcare Consultation"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() and settings.USE_GPU else "cpu"
        print(f"[RAG Engine] Initializing on device: {self.device}")
        
        if self.device == "cuda":
            print(f"[RAG Engine] GPU: {torch.cuda.get_device_name(0)}")
            print(f"[RAG Engine] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            # Set default device to GPU
            torch.cuda.set_device(0)
        
        # Models
        self.tokenizer_phobert = None
        self.model_phobert = None
        self.gen_tokenizer = None
        self.gen_model = None
        self.gen_pipe = None
        
        # Indices
        self.qa_index = None
        self.article_index = None
        
        # Data
        self.df_qa = None
        self.df_articles = None
        self.article_texts = []
        
        # Action verbs for filtering
        self.action_verbs = self._load_action_verbs()
        
        # Text cleaning patterns
        self._init_text_patterns()
        
    def _init_text_patterns(self):
        """Initialize regex patterns for text cleaning"""
        self.re_at_prefix = re.compile(r'^@[^:]{0,60}:\s*', flags=re.IGNORECASE)
        self.name_pattern = re.compile(r'\b([A-Z√Ä-·ª∏][a-z√†-·ªπ]+(?:_[A-Z√Ä-·ª∏][a-z√†-·ªπ]+)+)\b')
        self.doctor_pattern = re.compile(r'\b(BS|B√°c sƒ©|L∆∞∆°ng y|Dr)\.?\s+([A-Z√Ä-·ª∏][a-z√†-·ªπ_]+(\s+[A-Z√Ä-·ª∏][a-z√†-·ªπ_]+)*)', flags=re.IGNORECASE)
        self.pronoun_pattern = re.compile(r'\b(ch√°u|em|t·ªõ|m√¨nh|con|anh|ch·ªã)\b', flags=re.IGNORECASE)
        
        self.connectives = [
            r'v√¨ v·∫≠y', r'v√¨ th·∫ø', r'v·∫≠y n√™n', r'do v·∫≠y', r'v√¨ v·∫≠y n√™n', 
            r'v√¨ th·∫ø n√™n', r'cho n√™n', r't√≥m l·∫°i', r't√≥m t·∫Øt', r'nh∆∞ng', r'tuy nhi√™n'
        ]
        self.connective_pattern = re.compile("|".join([re.escape(x) for x in self.connectives]), flags=re.IGNORECASE)
        
    def _load_action_verbs(self) -> set:
        """Load action verbs for filtering actionable sentences"""
        return set([
            # Nh√≥m d√πng thu·ªëc / ƒëi·ªÅu tr·ªã
            "u·ªëng", "u·ªëng thu·ªëc", "d√πng", "d√πng thu·ªëc", "x·ªãt", "b√¥i", "thoa", "nh·ªè", "ng·∫≠m", 
            "ti√™m", "ch√≠ch", "truy·ªÅn", "ph·∫´u thu·∫≠t", "m·ªï", "ti·ªÉu ph·∫´u", "k√™ ƒë∆°n", "ƒëi·ªÅu tr·ªã",
            "ch∆∞·ªùm", "ch∆∞·ªùm n√≥ng", "ch∆∞·ªùm l·∫°nh", "bƒÉng b√≥", "s√°t tr√πng", "r·ª≠a v·∫øt th∆∞∆°ng",
            "h√∫t r·ª≠a", "x√¥ng", "kh√≠ dung", "ch√¢m c·ª©u", "b·∫•m huy·ªát", "massage", "xoa b√≥p",
            
            # Nh√≥m kh√°m / x√©t nghi·ªám
            "kh√°m", "ƒëi kh√°m", "t√°i kh√°m", "thƒÉm kh√°m", "ki·ªÉm tra", "x√©t nghi·ªám", "l·∫•y m·∫´u",
            "si√™u √¢m", "ch·ª•p", "ch·ª•p x-quang", "ch·ª•p ct", "ch·ª•p mri", "n·ªôi soi", "ƒëo huy·∫øt √°p",
            "ƒëo ƒë∆∞·ªùng huy·∫øt", "theo d√µi", "ƒë√°nh gi√°", "t·∫ßm so√°t",
            
            # Nh√≥m sinh ho·∫°t / dinh d∆∞·ª°ng
            "ƒÉn", "ƒÉn ki√™ng", "ki√™ng", "tr√°nh", "h·∫°n ch·∫ø", "b·ªï sung", "tƒÉng c∆∞·ªùng", "gi·∫£m",
            "u·ªëng n∆∞·ªõc", "ng·ªß", "ngh·ªâ ng∆°i", "k√™ g·ªëi", "n·∫±m nghi√™ng", "t·∫≠p", "t·∫≠p luy·ªán", 
            "v·∫≠n ƒë·ªông", "t·∫≠p v·∫≠t l√Ω tr·ªã li·ªáu", "th·ªÉ d·ª•c", "v·ªá sinh", "s√∫c mi·ªáng", "s√∫c h·ªçng",
            "r·ª≠a tay", "r·ª≠a m≈©i", "ƒëeo kh·∫©u trang", "c√°ch ly", "nh·∫≠p vi·ªán", "c·∫•p c·ª©u",

            # B·ªï sung t·ª´ dataset
            'ƒëi', 'si√™u', 'n·ªôi', 'ƒë·∫∑t', 'nh·ªè', 'b·ªï', 'kh√°m v√†', 'tr√°nh thai', 'kh√°m b·ªánh', 
            'ƒÉn u·ªëng', 'kh√°m b√°c sƒ©', 'kh√°m s·ª©c', 'kh√°m s·ª©c kh·ªèe', 'ƒëi ngo√†i', 'k√™', 
            'ƒëi si√™u √¢m', 'kh√°m thai', 'ƒë·∫∑t l·ªãch', 'kh√°m l·∫°i', 'ƒë·∫∑t l·ªãch kh√°m', 'ti√™m ch·ªßng',
            'ƒëi kh√°m b√°c', 'kh√°m ph·ª• khoa', 'ƒëi kh√°m ƒë·ªÉ', 'kh√°m chuy√™n khoa', 'kh√°m ph·ª•',
            'ƒëi kh√°m v√†', 'kh√°m chuy√™n', 'ti√™m ng·ª´a', 'ƒëi l·∫°i', 'r·ª≠a', 'ƒëi ti·ªÉu', 'ki√™ng',
            'ti√™m m≈©i', 'kh√°m tr·ª±c', 'ch·ª•p x quang', 'ti√™m vacxin', 'kh√°m v√† ƒëi·ªÅu',
            'ƒëi x√©t nghi·ªám', 'ƒëi kh√°m thai', 'ƒëi kh√°m chuy√™n', 'ti√™m ph√≤ng', 'ch·ª•p x',
            'cho b√© ƒëi', 'ƒëi t√°i', 'ƒë∆∞a b√© ƒëi', 'ƒëi x√©t', 'ƒëi ph√¢n', 'kh√°m t∆∞',
            'ƒëi c·∫ßu', 'ƒëi ngo√†i ph√¢n', 'ƒëi t√°i kh√°m', 'ƒë·∫øn b·ªánh vi·ªán', 'kh√°m thai ƒë·ªãnh',
            'kh√°m v·ªõi', 'ti√™m vaccine', 'ƒëi ki·ªÉm tra', 'ƒëi ti√™m', 'ƒëi ti√™u', 'ƒë∆∞a b√© ƒë·∫øn',
            'kh√°m tr·ª±c ti·∫øp', 'ƒë·∫∑t kh√°m', 'ƒëi kh√°m ngay', 'ti√™m v·∫Øc xin', 'ƒë·∫∑t t∆∞ v·∫•n',
            'kh√°m em', 'ƒë·∫∑t thu·ªëc', 'kh√°m v·ªõi b√°c', 'ti√™m v·∫Øc', 'kh√°m ƒë·ªÉ ƒë∆∞·ª£c', 'ƒëi l√†m',
            'h√∫t thai', 'si√™u √¢m tim', 'ƒëi kh√°m t·∫°i', 'ti√™m ƒë∆∞·ª£c', 'kh√°m kh√¥ng', 'ƒÉn v√†',
            'h√∫t thu·ªëc', 'l√†m g√¨', 'ƒëi kh√°m b·ªánh', 'ƒëi ki·ªÉm', 'si√™u √¢m l·∫°i', 'kh√°m ƒë·ªãnh',
            'ƒë·∫øn kh√°m t·∫°i', 'ƒëi v·ªá', 'ƒëi v·ªá sinh', 'ti√™m thu·ªëc', 'd√πng bi·ªán ph√°p',
            'ti√™m ch·ªßng chuy√™n', 'si√™u √¢m thai', 'kh√°m ƒë·ªãnh k·ª≥', 'k√™ ƒë∆°n thu·ªëc', 'ƒÉn ƒë∆∞·ª£c',
            'ƒÉn kh√¥ng', 'li√™n h·ªá v·ªõi', 'kh√°m ƒë·ªÉ b√°c', 'k√™ thu·ªëc', 'u·ªëng ƒë·ªß n∆∞·ªõc',
            'kh√°m t·∫°i khoa', 'ƒë·∫∑t v√≤ng', 'kh√°m b·ªánh chuy√™n', 'kh√°m hi·∫øm mu·ªôn', 'thai',
            'd√πng bi·ªán', 'ƒë·∫∑t c√¢u', 'ƒë·∫∑t c√¢u h·ªèi', 'u·ªëng v√†', 'ch·ª•p mri', 'kh√°m s·ªõm',
            'kh√°m cho', 'u·ªëng c√≥', 'l√†m g√¨ ƒë·ªÉ', 'u·ªëng b·ªï sung', 'k√™ toa', 'si√™u √¢m ·ªü',
            'n·ªôi soi b√≥c', 'si√™u √¢m th·∫•y', 'kh√°m t∆∞ v·∫•n', 'kh√°m hi·∫øm', 'd√πng cho',
            'ƒëi k√®m', 'ƒÉn ƒë·ªß', 'ƒÉn c·ªßa', 'kh√°m t·ªïng', 'kh√°m t·ªïng qu√°t', 'kh√°m v√† si√™u'
        ])
    
    def load_models(self):
        """Load PhoBERT and Generation models"""
        print("[RAG Engine] Loading PhoBERT...")
        self.tokenizer_phobert = AutoTokenizer.from_pretrained(settings.RETRIEVAL_MODEL, use_fast=True)
        
        try:
            if self.device == "cuda":
                self.model_phobert = AutoModel.from_pretrained(
                    settings.RETRIEVAL_MODEL,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True
                )
            else:
                self.model_phobert = AutoModel.from_pretrained(settings.RETRIEVAL_MODEL)
            self.model_phobert.eval()
            print("[RAG Engine] PhoBERT loaded successfully")
        except Exception as e:
            print(f"[RAG Engine] Error loading PhoBERT: {e}")
            raise
        
        # Generation model (optional, load on demand)
        self.generation_model = None
        self.generation_tokenizer = None
        print("[RAG Engine] Generation model will be loaded on first use")
    
    def _load_generation_model(self):
        """Load Vistral-7B-Chat generation model"""
        if self.generation_model is not None:
            return  # Already loaded
        
        try:
            from transformers import AutoModelForCausalLM
            
            model_name = "Viet-Mistral/Vistral-7B-Chat"
            print(f"[RAG Engine] Loading generation model: {model_name}")

            hf_token = (
                (getattr(settings, "HUGGINGFACE_HUB_TOKEN", "") or "").strip()
                or (getattr(settings, "HUGGINGFACE_TOKEN", "") or "").strip()
                or os.getenv("HUGGINGFACE_HUB_TOKEN", "").strip()
                or os.getenv("HUGGINGFACE_TOKEN", "").strip()
                or None
            )
            
            # Prefer `token=...` (newer transformers). Do not pass both.
            tok_kwargs = {"use_fast": True}
            if hf_token:
                tok_kwargs.update({"token": hf_token})
            self.generation_tokenizer = AutoTokenizer.from_pretrained(model_name, **tok_kwargs)
            
            if self.device == "cuda" and torch.cuda.is_available():
                print(f"[RAG Engine] Loading on GPU: {torch.cuda.get_device_name(0)}")
                model_kwargs = {
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "low_cpu_mem_usage": True,
                }
                if hf_token:
                    model_kwargs.update({"token": hf_token})
                self.generation_model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
            else:
                print("[RAG Engine] Loading on CPU")
                model_kwargs = {"dtype": torch.float32}
                if hf_token:
                    model_kwargs.update({"token": hf_token})
                self.generation_model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
                self.generation_model.to(self.device)
            
            self.generation_model.eval()
            print("[RAG Engine] Generation model loaded successfully")
        except Exception as e:
            print(f"[RAG Engine] Failed to load generation model: {e}")
            print("[RAG Engine] Will use template-based generation")
            self.generation_model = None
            self.generation_tokenizer = None
    
    def load_data(self):
        """Load Q&A and Articles data"""
        import sys
        print(f"[RAG Engine] Loading data from {settings.DATA_DIR}")
        sys.stdout.flush()

        # If FAISS cache exists, avoid expensive full-corpus preprocessing at startup.
        # This keeps API boot fast while still loading the full dataset for display/snippets.
        cache_present = (
            settings.ENABLE_CACHE
            and settings.QA_INDEX_CACHE.exists()
            and settings.METADATA_CACHE.exists()
        )
        preprocess_corpus = os.getenv("PREPROCESS_CORPUS", "0") == "1"
        
        # Load QA
        print("[RAG Engine] Reading QA CSV...")
        sys.stdout.flush()
        qa_usecols = None
        if settings.QA_CSV_PATH.exists():
            # Load only the columns we actually use.
            qa_usecols = ["question", "answer", "topic", "topic_original", "advice"]
        self.df_qa = pd.read_csv(settings.QA_CSV_PATH, usecols=lambda c: (qa_usecols is None or c in qa_usecols))
        # Sample if SAMPLE_SIZE > 0, otherwise use all data
        if settings.SAMPLE_SIZE > 0 and len(self.df_qa) > settings.SAMPLE_SIZE:
            self.df_qa = self.df_qa.sample(n=settings.SAMPLE_SIZE, random_state=42).reset_index(drop=True)
        print(f"[RAG Engine] Loaded {len(self.df_qa)} Q&A records")
        sys.stdout.flush()
        
        # Always normalize dtypes; optional preprocessing is VERY expensive for full datasets.
        for col in ["question", "answer", "advice", "topic", "topic_original"]:
            if col in self.df_qa.columns:
                self.df_qa[col] = self.df_qa[col].fillna("").astype(str)

        # Only preprocess if explicitly requested AND cache isn't present.
        if preprocess_corpus and not cache_present:
            for col in ["question", "answer", "advice"]:
                if col in self.df_qa.columns:
                    self.df_qa[col] = self.df_qa[col].apply(self._preprocess_text)
        
        # Load Articles
        if settings.ARTICLES_CSV_PATH.exists():
            article_usecols = ["id", "title", "text"]
            self.df_articles = pd.read_csv(settings.ARTICLES_CSV_PATH, usecols=lambda c: c in article_usecols)
            # Sample if SAMPLE_SIZE > 0, otherwise use all data
            if settings.SAMPLE_SIZE > 0 and len(self.df_articles) > settings.SAMPLE_SIZE:
                self.df_articles = self.df_articles.sample(n=settings.SAMPLE_SIZE, random_state=42).reset_index(drop=True)
            print(f"[RAG Engine] Loaded {len(self.df_articles)} articles")
            
            # Prepare article texts (use itertuples for speed)
            self.article_texts = []
            for row in self.df_articles.itertuples(index=False, name="ArticleRow"):
                title = str(getattr(row, "title", "")).strip()
                text = str(getattr(row, "text", "")).strip()
                self.article_texts.append((
                    str(getattr(row, "id", "")),
                    title,
                    f"{title}\n\n{text}"
                ))
        else:
            print("[RAG Engine] Articles file not found, skipping")
            self.df_articles = pd.DataFrame()
    
    def build_indices(self):
        """Build HNSW indices for fast retrieval with caching support"""
        import sys
        print("[RAG Engine] build_indices() started")
        sys.stdout.flush()
        
        # Try to load from cache first (fix issue #6: avoid rebuild every time)
        if settings.ENABLE_CACHE and self._try_load_indices():
            logger.info("[RAG Engine] Successfully loaded indices from cache")
            print("[RAG Engine] Successfully loaded indices from cache")
            sys.stdout.flush()
            return
        
        logger.info("[RAG Engine] Building HNSW indices from scratch...")
        print("[RAG Engine] Building HNSW indices from scratch...")
        sys.stdout.flush()
        
        # Build QA index
        qa_texts = (self.df_qa["question"].fillna("") + " " + self.df_qa["answer"].fillna("")).tolist()
        qa_embeddings = []
        
        print(f"[RAG Engine] Encoding {len(qa_texts)} Q&A pairs...")
        for i, text in enumerate(qa_texts):
            if i % 500 == 0:
                print(f"  Progress: {i}/{len(qa_texts)}")
            qa_embeddings.append(self._sentence_embedding(text))
        
        qa_embeddings = np.vstack(qa_embeddings).astype("float32")
        self.qa_index = self._build_hnsw_index(qa_embeddings)
        print(f"[RAG Engine] QA index built with {self.qa_index.ntotal} elements")
        
        # Build Article index
        article_embeddings = None
        if self.article_texts:
            print(f"[RAG Engine] Encoding {len(self.article_texts)} articles...")
            article_embeddings = []
            for i, (_, _, content) in enumerate(self.article_texts):
                if i % 500 == 0:
                    print(f"  Progress: {i}/{len(self.article_texts)}")
                article_embeddings.append(self._sentence_embedding(content))
            
            article_embeddings = np.vstack(article_embeddings).astype("float32")
            self.article_index = self._build_hnsw_index(article_embeddings)
            print(f"[RAG Engine] Article index built with {self.article_index.ntotal} elements")
        
        # Save to cache
        if settings.ENABLE_CACHE:
            self._save_indices(qa_embeddings, article_embeddings)
    
    def _save_indices(self, qa_embeddings: np.ndarray, article_embeddings: Optional[np.ndarray] = None):
        """Save HNSW indices and embeddings to cache"""
        try:
            settings.CACHE_DIR.mkdir(parents=True, exist_ok=True)
            
            # Save HNSW indices
            if self.qa_index:
                faiss.write_index(self.qa_index, str(settings.QA_INDEX_CACHE))
                logger.info(f"Saved QA index to {settings.QA_INDEX_CACHE}")
            
            if self.article_index:
                faiss.write_index(self.article_index, str(settings.ARTICLE_INDEX_CACHE))
                logger.info(f"Saved article index to {settings.ARTICLE_INDEX_CACHE}")
            
            # Save embeddings
            if qa_embeddings is not None:
                np.save(settings.QA_EMBEDDINGS_CACHE, qa_embeddings)
                logger.info(f"Saved QA embeddings to {settings.QA_EMBEDDINGS_CACHE}")
            
            if article_embeddings is not None:
                np.save(settings.ARTICLE_EMBEDDINGS_CACHE, article_embeddings)
                logger.info(f"Saved article embeddings to {settings.ARTICLE_EMBEDDINGS_CACHE}")
            
            # Save metadata (data hash for validation)
            metadata = {
                "qa_count": len(self.df_qa),
                "article_count": len(self.article_texts),
                "sample_size": settings.SAMPLE_SIZE,
                "data_hash": self._compute_data_hash()
            }
            with open(settings.METADATA_CACHE, "wb") as f:
                pickle.dump(metadata, f)
            
            logger.info("[RAG Engine] All indices saved to cache successfully")
        except Exception as e:
            logger.warning(f"Failed to save indices to cache: {e}")
    
    def _try_load_indices(self) -> bool:
        """Try to load indices from cache"""
        try:
            # Check if cache files exist
            if not all([
                settings.QA_INDEX_CACHE.exists(),
                settings.METADATA_CACHE.exists()
            ]):
                logger.info("Cache files not found, will build from scratch")
                return False
            
            # Load and validate metadata
            with open(settings.METADATA_CACHE, "rb") as f:
                metadata = pickle.load(f)
            
            # Validate data hasn't changed (older metadata may not include data_hash/sample_size)
            current_hash = self._compute_data_hash()
            meta_sample_size = metadata.get("sample_size")
            if meta_sample_size is not None and meta_sample_size != settings.SAMPLE_SIZE:
                logger.info("Sample size has changed, invalidating cache")
                return False

            meta_hash = metadata.get("data_hash")
            if meta_hash is not None and meta_hash != current_hash:
                logger.info("Data has changed, invalidating cache")
                return False

            # Backfill missing fields for older cache metadata
            if meta_hash is None:
                metadata["data_hash"] = current_hash
            if meta_sample_size is None:
                metadata["sample_size"] = settings.SAMPLE_SIZE
            try:
                with open(settings.METADATA_CACHE, "wb") as f:
                    pickle.dump(metadata, f)
            except Exception:
                pass
            
            # Load QA index
            self.qa_index = faiss.read_index(str(settings.QA_INDEX_CACHE))
            logger.info(f"Loaded QA index with {self.qa_index.ntotal} elements")
            
            # Load article index if exists
            if settings.ARTICLE_INDEX_CACHE.exists() and len(self.article_texts) > 0:
                self.article_index = faiss.read_index(str(settings.ARTICLE_INDEX_CACHE))
                logger.info(f"Loaded article index with {self.article_index.ntotal} elements")
            
            return True
            
        except Exception as e:
            logger.warning(f"Failed to load indices from cache: {e}")
            return False
    
    def _compute_data_hash(self) -> str:
        """Compute hash of data files to detect changes"""
        try:
            hash_obj = hashlib.md5()
            
            # Hash QA file
            if settings.QA_CSV_PATH.exists():
                hash_obj.update(str(settings.QA_CSV_PATH.stat().st_mtime).encode())
                hash_obj.update(str(settings.QA_CSV_PATH.stat().st_size).encode())
            
            # Hash articles file
            if settings.ARTICLES_CSV_PATH.exists():
                hash_obj.update(str(settings.ARTICLES_CSV_PATH.stat().st_mtime).encode())
                hash_obj.update(str(settings.ARTICLES_CSV_PATH.stat().st_size).encode())
            
            # Include sample size in hash
            hash_obj.update(str(settings.SAMPLE_SIZE).encode())
            
            return hash_obj.hexdigest()
        except Exception as e:
            logger.warning(f"Failed to compute data hash: {e}")
            return "unknown"
    
    def _preprocess_text(self, s: str) -> str:
        """Preprocess Vietnamese text"""
        if not isinstance(s, str):
            return ""
        s = re.sub(r'\s+', ' ', s.strip())
        # IMPORTANT: keep preprocessing consistent with how embeddings in cache were built.
        # Our full-corpus GPU rebuild embeds raw text without word segmentation.
        use_word_tokenize = os.getenv("USE_WORD_TOKENIZE", "0") == "1"
        if not use_word_tokenize:
            return s
        try:
            return word_tokenize(s, format="text")
        except Exception:
            return s
    
    def _preprocess_reference_sentence(self, s: str) -> str:
        """Preprocess reference sentence for embedding (remove names, pronouns, etc.)"""
        if not s:
            return ""
        
        s = s.strip()
        
        # Skip questions
        if s.endswith('?'):
            return ""
        
        # Remove @ prefix
        s = self.re_at_prefix.sub("", s)
        
        # Remove "tr·∫£ l·ªùi"
        s = re.sub(r'^tr·∫£[_\s]l·ªùi\s*[:.]?\s*', '', s, flags=re.IGNORECASE)
        
        # Remove names with underscore
        s = self.name_pattern.sub("", s)
        
        # Remove doctor names
        s = self.doctor_pattern.sub("", s)
        
        # Replace pronouns with "b·∫°n"
        s = self.pronoun_pattern.sub("b·∫°n", s)
        
        # Remove connectives
        s = self.connective_pattern.sub("", s)
        
        # Clean whitespace
        s = re.sub(r'\s+', ' ', s).strip()
        
        return s
    
    def _clean_text(self, t: str) -> str:
        """Clean text - unescape HTML and normalize whitespace"""
        return re.sub(r'\s+', ' ', unescape(t.strip())).strip()
    
    def _sentence_has_action(self, s: str) -> bool:
        """Check if sentence contains action verbs"""
        if not s:
            return False
        
        sl = s.lower()
        for act in self.action_verbs:
            act_norm = act.replace("_", " ").lower()
            if re.search(r'\b' + re.escape(act_norm) + r'\b', sl):
                return True
        return False
    
    def _sentence_embedding(self, text: str) -> np.ndarray:
        """Generate sentence embedding using PhoBERT"""
        if not text:
            return np.zeros(768, dtype=np.float32)
        
        try:
            inputs = self.tokenizer_phobert(text, return_tensors="pt", truncation=True, max_length=256)
            model_device = next(self.model_phobert.parameters()).device
            inputs = {k: v.to(model_device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model_phobert(**inputs)

                # Mean pooling over last hidden state (sentence embedding)
                last_hidden = outputs.last_hidden_state
                attention_mask = inputs.get("attention_mask")

                if attention_mask is not None:
                    attention_mask = attention_mask.unsqueeze(-1)
                    masked = last_hidden * attention_mask
                    summed = masked.sum(dim=1)
                    counts = attention_mask.sum(dim=1).clamp(min=1e-9)
                    mean_pooled = (summed / counts).squeeze().cpu().numpy().astype("float32")
                else:
                    mean_pooled = last_hidden.mean(dim=1).squeeze().cpu().numpy().astype("float32")

                return mean_pooled
        except Exception as e:
            print(f"[Embedding Error] {e}")
            return np.zeros(768, dtype=np.float32)
    
    def _build_hnsw_index(self, vectors: np.ndarray) -> Optional[faiss.Index]:
        """Build HNSW index from vectors using faiss"""
        if vectors is None or vectors.size == 0:
            return None
        
        try:
            dim = vectors.shape[1]
            # Use IndexFlatIP for inner product (similar to cosine similarity)
            index = faiss.IndexFlatIP(dim)
            # Normalize vectors for cosine similarity
            faiss.normalize_L2(vectors)
            index.add(vectors)
            return index
        except Exception as e:
            print(f"[HNSW Error] {e}")
            return None
    
    def retrieve_qa(self, query: str, k: int = 5) -> List[Dict]:
        """Retrieve top-k Q&A pairs"""
        if self.qa_index is None:
            return []
        
        query_processed = self._preprocess_text(query)
        user_emb = self._sentence_embedding(query_processed).reshape(1, -1)
        
        # Normalize query embedding for cosine similarity
        faiss.normalize_L2(user_emb)
        
        raw_k = min(max(k * 10, 30), self.qa_index.ntotal)
        distances, labels = self.qa_index.search(user_emb, k=raw_k)

        q_tokens = set([t.lower() for t in re.findall(r'\w+', query) if len(t) >= 2])

        candidates = []
        for dist, idx in zip(distances[0], labels[0]):
            if idx < 0 or idx >= len(self.df_qa):
                continue

            row = self.df_qa.iloc[int(idx)]
            sim = float(dist)

            # lexical overlap helps avoid off-topic high-embedding matches
            qa_text = f"{row.get('question', '')} {row.get('answer', '')}".lower()
            qa_tokens = set([t.lower() for t in re.findall(r'\w+', qa_text) if len(t) >= 2])
            lex = 0.0
            if q_tokens:
                lex = float(len(q_tokens & qa_tokens)) / max(1, len(q_tokens))

            combined = 0.85 * sim + 0.15 * lex
            candidates.append((combined, sim, lex, int(idx), row))

        # Sort by combined score
        candidates.sort(key=lambda x: x[0], reverse=True)

        results = []
        for combined, sim, lex, idx, row in candidates:
            # Keep a lower gate because combined scoring already filters noise
            if sim < settings.QUESTION_SIM_THRESHOLD and combined < (settings.QUESTION_SIM_THRESHOLD + 0.05):
                continue

            results.append({
                "score": float(combined),
                "sim": float(sim),
                "lex": float(lex),
                "index": idx,
                "question": row.get("question", ""),
                "answer": row.get("answer", ""),
                "topic": row.get("topic", "Kh√°c")
            })
            if len(results) >= k:
                break

        return results
    
    def retrieve_articles(self, query: str, k: int = 1) -> List[Dict]:
        """Retrieve top-k articles with re-ranking"""
        if self.article_index is None or not self.article_texts:
            return []
        
        query_processed = self._preprocess_text(query)
        user_emb = self._sentence_embedding(query_processed).reshape(1, -1)
        q_tokens_set = set([t.lower() for t in re.findall(r'\w+', query) if len(t) >= 2])
        
        # Normalize query embedding for cosine similarity
        faiss.normalize_L2(user_emb)
        
        raw_k = min(k * 3, self.article_index.ntotal)
        distances, labels = self.article_index.search(user_emb, k=raw_k)
        
        # Get raw candidates
        raw_candidates = []
        for dist, idx in zip(distances[0], labels[0]):
            if idx < 0 or idx >= len(self.article_texts):
                continue
            
            raw_candidates.append({
                "index": int(idx),
                # With normalized vectors, inner product == cosine similarity
                "score": float(dist)
            })
        
        # Re-rank with lexical overlap and title boost
        reranked = []
        w_sim = 0.75
        w_lex = 0.20
        w_title_boost = 0.05
        
        for c in raw_candidates:
            idx = int(c['index'])
            link, title, content = self.article_texts[idx]
            baseline_sim = float(c.get('score', 0.0))
            
            # Extract actual URL from content if present
            url_match = re.search(r'https?://[^\s]+', content)
            actual_link = url_match.group(0) if url_match else link
            
            # Lexical overlap with title + snippet
            article_snippet = title + " " + (content[:1000] if content else "")
            art_tokens = set([t.lower() for t in re.findall(r'\w+', article_snippet) if len(t) >= 2])
            
            lex_overlap = 0.0
            if q_tokens_set:
                common = len(q_tokens_set & art_tokens)
                lex_overlap = common / max(1, len(q_tokens_set))
            
            # Title boost
            title_tokens = set([t.lower() for t in re.findall(r'\w+', title) if len(t) >= 2])
            title_boost_flag = 1.0 if (q_tokens_set & title_tokens) else 0.0
            
            # Combined score
            combined = w_sim * baseline_sim + w_lex * lex_overlap + w_title_boost * title_boost_flag
            
            # Find best passage
            passages = self._chunk_text(content, max_chars=600)
            best_passage = ""
            best_passage_sim = -1.0
            
            for p in passages[:6]:
                p_proc = self._preprocess_text(p)
                p_emb = self._sentence_embedding(p_proc).astype("float32")
                sim_p = self._cosine_sim(user_emb.reshape(-1), p_emb)
                
                if sim_p > best_passage_sim:
                    best_passage_sim = sim_p
                    best_passage = p
            
            if not best_passage and content:
                best_passage = content[:600]
            
            reranked.append({
                "index": idx,
                "link": actual_link,
                "title": title,
                "txt": content,
                "baseline_sim": baseline_sim,
                "lex_overlap": lex_overlap,
                "title_boost": title_boost_flag,
                "combined_score": combined,
                "best_passage": best_passage,
                "best_passage_sim": best_passage_sim,
                "score": combined  # For compatibility
            })
        
        # Sort by combined score
        reranked_sorted = sorted(reranked, key=lambda x: x["combined_score"], reverse=True)
        
        # Filter by minimum score
        final = [r for r in reranked_sorted if r["combined_score"] >= settings.COMBINED_SCORE_THRESHOLD]
        
        if not final:
            return []
        
        # Return top-k
        results = []
        for r in final[:k]:
            results.append({
                "score": r["combined_score"],
                "index": r["index"],
                "title": r["title"],
                "link": r["link"],
                "snippet": r["best_passage"]
            })
        
        return results
    
    def _chunk_text(self, text: str, max_chars: int = 500, overlap_ratio: float = 0.15) -> List[str]:
        """Chunk text into overlapping passages"""
        if not text:
            return []
        
        passages = []
        start = 0
        L = len(text)
        overlap_chars = int(max_chars * overlap_ratio)
        
        while start < L:
            end = start + max_chars
            if end >= L:
                passages.append(text[start:L].strip())
                break
            
            # Find sentence boundary
            cut = text.rfind('.', start, end)
            if cut <= start:
                cut = end
            
            passages.append(text[start:cut].strip())
            
            # Move with overlap
            if overlap_chars > 0:
                start = max(cut - overlap_chars, start + 1)
            else:
                start = cut
        
        return [p for p in passages if p]
    
    def _cosine_sim(self, a: np.ndarray, b: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors"""
        if a is None or b is None:
            return 0.0
        na = np.linalg.norm(a)
        nb = np.linalg.norm(b)
        if na == 0 or nb == 0:
            return 0.0
        return float(np.dot(a, b) / (na * nb))
    
    def find_best_action_sentence(
        self, 
        user_text: str,
        topk_rows: List[Dict],
        sent_sim_thresh: float = 0.6,
        combined_thresh: float = 0.68,
        alpha: float = 0.75,
        beta: float = 0.2,
        gamma: float = 0.05
    ) -> Tuple[Optional[str], Optional[int], float]:
        """
        Find best action sentence from top-k Q&A results
        
        Args:
            user_text: User query
            topk_rows: List of top-k Q&A results
            sent_sim_thresh: Sentence similarity threshold
            combined_thresh: Combined score threshold
            alpha: Weight for sentence similarity
            beta: Weight for question similarity
            gamma: Weight for lexical overlap
            
        Returns:
            (action_text, ref_index, score)
        """
        if not topk_rows:
            return None, None, 0.0
        
        # Extract and preprocess all sentences
        all_sents = []
        for ref_pos, r in enumerate(topk_rows, start=1):
            question_text = r.get("question", "") or ""
            raw_answer = r.get("answer", "") or ""
            
            # Split answer into sentences
            sents = re.split(r'(?<=[.!?])\s+', raw_answer.strip()) if raw_answer else []
            
            for s in sents:
                s_orig = self._clean_text(s)
                s_proc = self._preprocess_reference_sentence(s_orig)
                
                if len(s_proc) >= 6:
                    all_sents.append((ref_pos, question_text, s_orig, s_proc))
        
        if not all_sents:
            return None, None, 0.0
        
        # Get user embedding
        user_q = self._preprocess_text(user_text)
        user_emb = self._sentence_embedding(user_q).astype("float32")
        user_tokens_set = set([t.lower() for t in re.findall(r'\w+', user_text) if len(t) >= 2])
        
        # Cache question embeddings
        question_emb_cache = {}
        scored = []
        
        for ref_pos, question_text, sent_orig, sent_proc in all_sents:
            # Get question embedding
            if ref_pos not in question_emb_cache:
                q_text_proc = self._preprocess_text(question_text) if question_text else ""
                if q_text_proc:
                    question_emb_cache[ref_pos] = self._sentence_embedding(q_text_proc).astype("float32")
                else:
                    question_emb_cache[ref_pos] = np.zeros(user_emb.shape, dtype=np.float32)
            
            # Get sentence embedding
            s_emb = self._sentence_embedding(sent_proc).astype("float32")
            
            # Calculate similarities
            sim_sent = self._cosine_sim(user_emb, s_emb)
            sim_q = self._cosine_sim(user_emb, question_emb_cache[ref_pos])
            
            # Lexical overlap
            sent_tokens_set = set([t.lower() for t in re.findall(r'\w+', sent_proc) if len(t) >= 2])
            lex_overlap = float(len(user_tokens_set & sent_tokens_set)) / max(1, len(user_tokens_set)) if user_tokens_set else 0.0
            
            # Combined score
            combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap
            
            scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))
        
        # Sort by combined score
        scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)
        
        # Find best action sentence
        best_ref_pos = None
        best_combined_score = 0.0
        
        for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:
            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:
                if self._sentence_has_action(sent_proc) or self._sentence_has_action(sent_orig):
                    best_ref_pos = ref_pos
                    best_combined_score = combined
                    break
        
        if best_ref_pos is None:
            return None, None, 0.0
        
        # Collect all action sentences from best reference
        final_sentences_list = []
        for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:
            if ref_pos == best_ref_pos:
                if sim_sent >= sent_sim_thresh and combined >= combined_thresh:
                    if self._sentence_has_action(sent_proc) or self._sentence_has_action(sent_orig):
                        # Replace pronouns
                        s_final = self.pronoun_pattern.sub("b·∫°n", sent_orig)
                        s_final = re.sub(r'\s+', ' ', s_final).strip()
                        
                        if s_final not in final_sentences_list:
                            final_sentences_list.append(s_final)
        
        if not final_sentences_list:
            return None, None, 0.0
        
        final_paragraph = " ".join(final_sentences_list)
        return final_paragraph, best_ref_pos, best_combined_score

    def _generate_generic_medical_advice(self, query: str, specialty: str) -> str:
        """Safe fallback when retrieval is low-quality (no LLM required)."""
        q = (query or "").strip()
        lines = []
        lines.append("M√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c th√¥ng tin tham kh·∫£o ƒë·ªß s√°t v·ªõi c√¢u h·ªèi ƒë·ªÉ ƒë∆∞a ra h∆∞·ªõng d·∫´n c·ª• th·ªÉ.")
        if q:
            lines.append(f"C√¢u h·ªèi c·ªßa b·∫°n: {q}")
        lines.append("")
        lines.append("B·∫°n c√≥ th·ªÉ b·ªï sung th√™m gi√∫p m√¨nh:")
        lines.append("- Tri·ªáu ch·ª©ng ch√≠nh l√† g√¨, xu·∫•t hi·ªán bao l√¢u, m·ª©c ƒë·ªô tƒÉng/gi·∫£m?")
        lines.append("- C√≥ s·ªët, n√¥n, ti√™u ch·∫£y/t√°o b√≥n, ch√≥ng m·∫∑t, kh√≥ th·ªü, ƒëau ng·ª±c, ng·∫•t kh√¥ng?")
        lines.append("- Tu·ªïi, b·ªánh n·ªÅn, thu·ªëc ƒëang d√πng (n·∫øu c√≥).")
        lines.append("")
        lines.append("N·∫øu c√≥ d·∫•u hi·ªáu n·∫∑ng (ƒëau d·ªØ d·ªôi, n√¥n ra m√°u, ƒëi ngo√†i ph√¢n ƒëen/ra m√°u, s·ªët cao li√™n t·ª•c, ng·∫•t, kh√≥ th·ªü, ƒëau ng·ª±c...), b·∫°n c·∫ßn ƒëi c·∫•p c·ª©u ngay.")
        return "\n".join(lines)
    
    def generate_answer(self, query: str, qa_results: List[Dict], article_results: List[Dict]) -> Tuple[str, str, float]:
        """Generate natural answer using retrieved context with medical safety checks"""
        # Log retrieval results
        logger.info(f"Query: {query}")
        logger.info(f"QA results: {len(qa_results)} found, best score: {qa_results[0]['score'] if qa_results else 0}")
        logger.info(f"Article results: {len(article_results)} found, best score: {article_results[0]['score'] if article_results else 0}")
        
        # Determine specialty
        specialty = "Y t·∫ø t·ªïng qu√°t"
        if qa_results:
            specialty = (qa_results[0].get("topic") or "").strip() or "Y t·∫ø t·ªïng qu√°t"
            # Avoid low-signal buckets like "Kh√°c" when possible
            if specialty.lower() in {"kh√°c", "khac", "other"}:
                for qa in qa_results[:5]:
                    cand = (qa.get("topic") or "").strip()
                    if cand and cand.lower() not in {"kh√°c", "khac", "other"}:
                        specialty = cand
                        break
                else:
                    specialty = "Y t·∫ø t·ªïng qu√°t"
        
        # Check if this is a non-medical query (greetings, thanks, etc.)
        is_non_medical, non_medical_response = self._check_non_medical_query(query)
        if is_non_medical:
            logger.info(f"Non-medical query detected: {query[:50]}...")
            return non_medical_response, "Tr·ª£ l√Ω AI", 0.95
        
        # Medical safety check (fix issue #9: detect emergencies)
        emergency_detected, emergency_type = self._check_emergency_keywords(query)
        
        # Retrieval quality
        best_qa_score = qa_results[0]['score'] if qa_results else 0.0
        best_article_score = article_results[0]['score'] if article_results else 0.0
        best_score = max(best_qa_score, best_article_score)

        # If nothing retrieved, fall back to generic medical advice.
        if not qa_results and not article_results:
            logger.warning("No retrieval results. Using generic response.")
            answer = self._generate_generic_medical_advice(query, specialty)
            confidence = 0.3
            return answer, specialty, confidence
        
        # Build context for LLM only (UI will show sources separately)
        retrieved_content = ""
        if qa_results or article_results:
            context_parts = []
            for qa in qa_results[:3]:
                q = (qa.get("question") or "").strip()
                a = (qa.get("answer") or "").strip()
                if q or a:
                    context_parts.append(f"Q: {self._truncate_text(q, 180)}\nA: {self._truncate_text(a, 300)}")
            if article_results:
                title = (article_results[0].get("title") or "").strip()
                snip = (article_results[0].get("snippet") or "").strip()
                if title or snip:
                    context_parts.append(f"Article: {self._truncate_text(title, 180)}\n{self._truncate_text(snip, 300)}")
            retrieved_content = "\n\n".join([p for p in context_parts if p])
        
        # Calculate confidence based on the best available source
        confidence = 0.35
        if best_score > 0:
            confidence = float(min(0.95, best_score + 0.1))
        
        # If emergency detected, return urgent warning
        if emergency_detected:
            logger.warning(f"Emergency detected: {emergency_type} in query: {query[:50]}...")
            answer = self._generate_emergency_response(emergency_type, specialty)
            return answer, specialty, 0.95  # High confidence for emergency warnings
        
        # Use template-based answer (faster, more reliable)
        # LLM generation disabled by default to avoid memory issues
        # Set ENABLE_LLM_GENERATION=1 in env to enable
        use_llm = os.getenv('ENABLE_LLM_GENERATION', '0') == '1'
        
        if use_llm and self.generation_model is None:
            try:
                self._load_generation_model()
            except Exception as e:
                logger.warning(f"Failed to load generation model: {e}. Using template.")
        
        if use_llm and self.generation_model is not None:
            print(f"[RAG] ‚úÖ Using LLM generation for query: {query[:50]}...")
            answer = self._generate_with_llm(query, retrieved_content, specialty)
        else:
            print(f"[RAG] ‚ö†Ô∏è Using template-based generation (no LLM) for query: {query[:50]}...")
            answer = self._generate_rag_answer_no_llm(query, qa_results, article_results, specialty)

        return answer, specialty, confidence

    def build_disclaimer(self, specialty: str, confidence: float) -> str:
        """Return medical disclaimer text (kept separate so UI can format consistently)."""
        base = (
            "Th√¥ng tin ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o. "
            "B·∫°n n√™n ƒëi kh√°m tr·ª±c ti·∫øp ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c h∆°n."
        )

        if confidence < 0.6:
            base = (
                "Th√¥ng tin ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o v·ªõi ƒë·ªô tin c·∫≠y th·∫•p. "
                "B·∫°n n√™n ƒëi kh√°m tr·ª±c ti·∫øp ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c h∆°n."
            )

        if specialty and specialty.strip():
            base += f" (G·ª£i √Ω chuy√™n khoa: {specialty.strip()})."

        base += "\n\nH·ªá th·ªëng n√†y KH√îNG thay th·∫ø cho √Ω ki·∫øn c·ªßa b√°c sƒ©. Trong tr∆∞·ªùng h·ª£p kh·∫©n c·∫•p, h√£y g·ªçi 115 ho·∫∑c ƒë·∫øn b·ªánh vi·ªán ngay."
        return base
    
    def _check_emergency_keywords(self, query: str) -> Tuple[bool, str]:
        """Detect emergency medical situations (fix issue #9: safety guardrails)"""
        query_lower = query.lower()
        
        # Critical emergency keywords
        critical_keywords = {
            "nguy_k·ªãch": ["nguy k·ªãch", "h√¥n m√™", "b·∫•t t·ªânh", "ng·∫•t x·ªâu", "th·ªü g·∫•p", "kh√≥ th·ªü n·∫∑ng", "co gi·∫≠t"],
            "ch·∫£y_m√°u": ["ch·∫£y m√°u nhi·ªÅu", "xu·∫•t huy·∫øt", "m√°u ch·∫£y kh√¥ng ng·ª´ng", "m√°u ƒë·ªè t∆∞∆°i"],
            "ƒëau_ng·ª±c": ["ƒëau ng·ª±c d·ªØ d·ªôi", "ƒëau th·∫Øt ng·ª±c", "ƒëau tim", "ngh·∫πt ng·ª±c"],
            "ƒë·ªôt_qu·ªµ": ["li·ªát n·ª≠a ng∆∞·ªùi", "m√©o mi·ªáng", "n√≥i l·∫Øp", "y·∫øu m·ªôt b√™n", "ƒë·ªôt qu·ªµ"],
            "tai_n·∫°n": ["tai n·∫°n nghi√™m tr·ªçng", "g√£y x∆∞∆°ng", "ch·∫•n th∆∞∆°ng n·∫∑ng", "xe ƒë√¢m"],
            "ng·ªô_ƒë·ªôc": ["ng·ªô ƒë·ªôc", "u·ªëng nh·∫ßm", "ƒÉn ph·∫£i", "ng·∫•t sau khi ƒÉn"]
        }
        
        for emergency_type, keywords in critical_keywords.items():
            if any(kw in query_lower for kw in keywords):
                return True, emergency_type
        
        return False, ""
    
    def _check_non_medical_query(self, query: str) -> Tuple[bool, str]:
        """Detect and handle non-medical queries (greetings, thanks, small talk)"""
        query_lower = query.lower().strip()
        
        # Greetings
        greetings = ["xin ch√†o", "ch√†o", "hello", "hi", "hey", "ch√†o b·∫°n", "ch√†o b√°c sƒ©", "ch√†o anh", "ch√†o ch·ªã"]
        for greeting in greetings:
            if query_lower == greeting or query_lower.startswith(greeting + " ") or query_lower.startswith(greeting + ","):
                return True, "Xin ch√†o! T√¥i l√† Healthcare AI Assistant - tr·ª£ l√Ω y t·∫ø th√¥ng minh. T√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n t∆∞ v·∫•n v·ªÅ c√°c v·∫•n ƒë·ªÅ s·ª©c kh·ªèe, tri·ªáu ch·ª©ng b·ªánh, dinh d∆∞·ª°ng v√† chƒÉm s√≥c s·ª©c kh·ªèe t·ªïng qu√°t.\n\nB·∫°n c√≥ th·ªÉ h·ªèi t√¥i v·ªÅ:\n‚Ä¢ Tri·ªáu ch·ª©ng v√† d·∫•u hi·ªáu b·ªánh\n‚Ä¢ C√°ch ph√≤ng ng·ª´a v√† chƒÉm s√≥c s·ª©c kh·ªèe\n‚Ä¢ Ch·∫ø ƒë·ªô dinh d∆∞·ª°ng ph√π h·ª£p\n‚Ä¢ T∆∞ v·∫•n ban ƒë·∫ßu v·ªÅ c√°c v·∫•n ƒë·ªÅ y t·∫ø\n\nB·∫°n c·∫ßn t∆∞ v·∫•n v·ªÅ v·∫•n ƒë·ªÅ g√¨?"
        
        # Thanks
        thanks = ["c·∫£m ∆°n", "c√°m ∆°n", "thank", "thanks", "c·∫£m ∆°n b·∫°n", "c·∫£m ∆°n nhi·ªÅu", "thanks b·∫°n"]
        for thank in thanks:
            if query_lower == thank or query_lower.startswith(thank + " ") or query_lower.endswith(" " + thank):
                return True, "R·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ b·∫°n! üòä\n\nN·∫øu b·∫°n c√≥ th√™m c√¢u h·ªèi v·ªÅ s·ª©c kh·ªèe ho·∫∑c c·∫ßn t∆∞ v·∫•n th√™m, ƒë·ª´ng ng·∫ßn ng·∫°i h·ªèi t√¥i b·∫•t c·ª© l√∫c n√†o.\n\nCh√∫c b·∫°n v√† gia ƒë√¨nh lu√¥n kh·ªèe m·∫°nh! üè•üíö"
        
        # Apologies
        apologies = ["xin l·ªói", "sorry", "xin l·ªói nh√©", "m√¨nh xin l·ªói", "t√¥i xin l·ªói"]
        for apology in apologies:
            if query_lower == apology or query_lower.startswith(apology + " ") or query_lower.startswith(apology + ","):
                return True, "Kh√¥ng sao c·∫£, b·∫°n kh√¥ng c·∫ßn xin l·ªói! üòä\n\nT√¥i ·ªü ƒë√¢y ƒë·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ c√°c v·∫•n ƒë·ªÅ s·ª©c kh·ªèe. N·∫øu b·∫°n c√≥ c√¢u h·ªèi ho·∫∑c lo l·∫Øng v·ªÅ s·ª©c kh·ªèe, c·ª© tho·∫£i m√°i chia s·∫ª v·ªõi t√¥i.\n\nB·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ g√¨ c·∫ßn t∆∞ v·∫•n kh√¥ng?"
        
        # Can you help me
        help_requests = [
            "b·∫°n c√≥ th·ªÉ gi√∫p t√¥i", "b·∫°n c√≥ th·ªÉ gi√∫p", "gi√∫p t√¥i v·ªõi", "gi√∫p t√¥i", 
            "gi√∫p m√¨nh v·ªõi", "b·∫°n gi√∫p t√¥i", "c√≥ th·ªÉ gi√∫p kh√¥ng", "gi√∫p ƒë∆∞·ª£c kh√¥ng"
        ]
        for help_req in help_requests:
            if help_req in query_lower:
                return True, "T·∫•t nhi√™n r·ªìi! T√¥i r·∫•t s·∫µn l√≤ng gi√∫p b·∫°n! ü§ó\n\nT√¥i c√≥ th·ªÉ h·ªó tr·ª£ b·∫°n v·ªÅ:\n\nüè• **T∆∞ v·∫•n s·ª©c kh·ªèe:**\n‚Ä¢ Ph√¢n t√≠ch tri·ªáu ch·ª©ng b·ªánh\n‚Ä¢ Gi·∫£i ƒë√°p th·∫Øc m·∫Øc y t·∫ø\n‚Ä¢ G·ª£i √Ω chuy√™n khoa ph√π h·ª£p\n\nüíä **ChƒÉm s√≥c s·ª©c kh·ªèe:**\n‚Ä¢ Ph√≤ng ng·ª´a b·ªánh t·∫≠t\n‚Ä¢ Ch·∫ø ƒë·ªô ƒÉn u·ªëng l√†nh m·∫°nh\n‚Ä¢ L·ªùi khuy√™n s·ª©c kh·ªèe h√†ng ng√†y\n\nH√£y cho t√¥i bi·∫øt c·ª• th·ªÉ b·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ g√¨ ho·∫∑c tri·ªáu ch·ª©ng n√†o nh√©!"
        
        # Goodbye
        goodbyes = ["t·∫°m bi·ªát", "bye", "goodbye", "h·∫πn g·∫∑p l·∫°i", "t·∫°m bi·ªát nh√©", "bye bye"]
        for goodbye in goodbyes:
            if query_lower == goodbye or query_lower.startswith(goodbye + " "):
                return True, "T·∫°m bi·ªát! Ch√∫c b·∫°n m·ªôt ng√†y t·ªët l√†nh v√† lu√¥n kh·ªèe m·∫°nh! üåü\n\nN·∫øu c·∫ßn t∆∞ v·∫•n y t·∫ø, t√¥i lu√¥n s·∫µn s√†ng h·ªó tr·ª£ b·∫°n 24/7. H·∫πn g·∫∑p l·∫°i!"
        
        # How are you / About the bot
        how_are_you = ["b·∫°n kh·ªèe kh√¥ng", "kh·ªèe kh√¥ng", "how are you", "b·∫°n th·∫ø n√†o", "b·∫°n l√† ai", "b·∫°n l√† g√¨", "ai t·∫°o ra b·∫°n"]
        for phrase in how_are_you:
            if phrase in query_lower:
                return True, "T√¥i l√† Healthcare AI Assistant - tr·ª£ l√Ω ·∫£o t∆∞ v·∫•n y t·∫ø ƒë∆∞·ª£c ph√°t tri·ªÉn d·ª±a tr√™n c∆° s·ªü d·ªØ li·ªáu 148,000+ t√†i li·ªáu y t·∫ø ti·∫øng Vi·ªát! ü§ñüíô\n\nT√¥i ho·∫°t ƒë·ªông 24/7 ƒë·ªÉ:\n‚úÖ T∆∞ v·∫•n v·ªÅ tri·ªáu ch·ª©ng v√† s·ª©c kh·ªèe\n‚úÖ Cung c·∫•p th√¥ng tin y t·∫ø ƒë√°ng tin c·∫≠y\n‚úÖ H·ªó tr·ª£ ƒë·ªãnh h∆∞·ªõng chuy√™n khoa ph√π h·ª£p\n\nTuy nhi√™n, t√¥i KH√îNG thay th·∫ø b√°c sƒ©. V·ªõi c√°c tr∆∞·ªùng h·ª£p nghi√™m tr·ªçng, b·∫°n n√™n ƒë·∫øn c∆° s·ªü y t·∫ø ƒë·ªÉ ƒë∆∞·ª£c kh√°m tr·ª±c ti·∫øp.\n\nB·∫°n mu·ªën h·ªèi v·ªÅ v·∫•n ƒë·ªÅ s·ª©c kh·ªèe n√†o?"
        
        # Help / What can you do
        help_queries = ["b·∫°n c√≥ th·ªÉ l√†m g√¨", "b·∫°n gi√∫p g√¨ ƒë∆∞·ª£c", "help", "gi√∫p t√¥i", "h∆∞·ªõng d·∫´n"]
        for help_q in help_queries:
            if help_q in query_lower:
                return True, "T√¥i c√≥ th·ªÉ gi√∫p b·∫°n v·ªõi c√°c v·∫•n ƒë·ªÅ sau:\n\nüè• **T∆∞ v·∫•n y t·∫ø:**\n‚Ä¢ Ph√¢n t√≠ch tri·ªáu ch·ª©ng v√† d·∫•u hi·ªáu b·ªánh\n‚Ä¢ T∆∞ v·∫•n v·ªÅ c√°c b·ªánh th∆∞·ªùng g·∫∑p\n‚Ä¢ G·ª£i √Ω chuy√™n khoa ph√π h·ª£p\n\nüíä **S·ª©c kh·ªèe t·ªïng qu√°t:**\n‚Ä¢ Ph√≤ng ng·ª´a b·ªánh t·∫≠t\n‚Ä¢ ChƒÉm s√≥c s·ª©c kh·ªèe h√†ng ng√†y\n‚Ä¢ Ch·∫ø ƒë·ªô dinh d∆∞·ª°ng l√†nh m·∫°nh\n\nüë®‚Äç‚öïÔ∏è **L∆∞u √Ω quan tr·ªçng:**\n‚Ä¢ Th√¥ng tin ch·ªâ mang t√≠nh tham kh·∫£o\n‚Ä¢ KH√îNG thay th·∫ø kh√°m b√°c sƒ© tr·ª±c ti·∫øp\n‚Ä¢ Kh·∫©n c·∫•p: G·ªçi 115 ho·∫∑c ƒë·∫øn b·ªánh vi·ªán\n\nH√£y cho t√¥i bi·∫øt tri·ªáu ch·ª©ng ho·∫∑c v·∫•n ƒë·ªÅ s·ª©c kh·ªèe b·∫°n ƒëang g·∫∑p ph·∫£i!"
        
        # Very short queries (less than 10 chars, likely gibberish or test)
        if len(query_lower) < 3 and query_lower not in ["hi", "ok"]:
            return False, ""  # Let it go through normal flow
        
        return False, ""
    
    def _generate_emergency_response(self, emergency_type: str, specialty: str) -> str:
        """Generate urgent response for emergency situations"""
        responses = {
            "nguy_k·ªãch": "‚ö†Ô∏è T√åNH HU·ªêNG KH·∫®N C·∫§P: Tri·ªáu ch·ª©ng b·∫°n m√¥ t·∫£ C√ì TH·ªÇ nghi√™m tr·ªçng. NGAY L·∫¨P T·ª®C:\n1. G·ªåI 115 (c·∫•p c·ª©u) ho·∫∑c ƒë∆∞a ng∆∞·ªùi b·ªánh ƒë·∫øn b·ªánh vi·ªán G·∫¶N NH·∫§T\n2. Gi·ªØ b√¨nh tƒ©nh, theo d√µi √Ω th·ª©c v√† nh·ªãp th·ªü\n3. KH√îNG t·ª± √Ω cho u·ªëng thu·ªëc\n\nƒê√¢y KH√îNG ph·∫£i l·ªùi khuy√™n thay th·∫ø c·∫•p c·ª©u y t·∫ø chuy√™n nghi·ªáp.",
            
            "ch·∫£y_m√°u": "‚ö†Ô∏è C·∫¢NH B√ÅO: Ch·∫£y m√°u nhi·ªÅu c·∫ßn X·ª¨ TR√ç NGAY:\n1. ·∫§n tr·ª±c ti·∫øp v√†o v·∫øt th∆∞∆°ng b·∫±ng v·∫£i s·∫°ch\n2. N√¢ng cao v·ªã tr√≠ b·ªã th∆∞∆°ng (n·∫øu c√≥ th·ªÉ)\n3. G·ªåI 115 ho·∫∑c ƒë·∫øn c·∫•p c·ª©u NGAY n·∫øu m√°u kh√¥ng c·∫ßm\n4. KH√îNG b·ªè bƒÉng ra khi m√°u ƒë√£ ƒë√¥ng\n\nC·∫ßn ƒë√°nh gi√° y t·∫ø KH·∫®N C·∫§P.",
            
            "ƒëau_ng·ª±c": "‚ö†Ô∏è KH·∫®N C·∫§P TIM M·∫†CH: ƒêau ng·ª±c c√≥ th·ªÉ l√† d·∫•u hi·ªáu nh·ªìi m√°u c∆° tim.\nH√ÄNH ƒê·ªòNG NGAY:\n1. G·ªåI 115 ho·∫∑c ƒë·∫øn c·∫•p c·ª©u NGAY L·∫¨P T·ª®C\n2. Ng·ªìi ngh·ªâ, KH√îNG v·∫≠n ƒë·ªông\n3. N·∫øu c√≥ s·∫µn: nhai 1 vi√™n aspirin 300mg (tr·ª´ khi d·ªã ·ª©ng)\n4. Theo d√µi nh·ªãp tim, h√¥ h·∫•p\n\nTh·ªùi gian l√† v√†ng - M·ªñI PH√öT tr√¨ ho√£n l√†m tƒÉng nguy c∆°.",
            
            "ƒë·ªôt_qu·ªµ": "‚ö†Ô∏è D·∫§U HI·ªÜU ƒê·ªòT QU·ª¥ - H√ÄNH ƒê·ªòNG NGAY:\nTEST NHANH (FAST):\n- Face (m·∫∑t): c∆∞·ªùi c√≥ m√©o mi·ªáng?\n- Arms (tay): gi∆° 2 tay c√≥ tay n√†o y·∫øu?\n- Speech (n√≥i): n√≥i c√≥ l·∫Øp?\n- Time (th·ªùi gian): G·ªåI 115 NGAY!\n\n‚úÖ ƒê∆ØA B·ªÜNH NH√ÇN ƒê·∫æN B·ªÜNH VI·ªÜN TRONG 4.5 GI·ªú ƒê·∫¶U\n‚ùå KH√îNG cho ƒÉn, u·ªëng (nguy c∆° s·∫∑c)\n\nƒê·ªôt qu·ªµ l√† KH·∫®N C·∫§P Y T·∫æ!",
            
            "tai_n·∫°n": "‚ö†Ô∏è TAI N·∫†N - C·∫¶N H·ªñ TR·ª¢ Y T·∫æ:\n1. ƒê·∫£m b·∫£o an to√†n hi·ªán tr∆∞·ªùng\n2. G·ªåI 115 n·∫øu: ch·∫•n th∆∞∆°ng ƒë·∫ßu/c·ªôt s·ªëng, g√£y x∆∞∆°ng, ch·∫£y m√°u nhi·ªÅu\n3. KH√îNG di chuy·ªÉn ng∆∞·ªùi b·ªã th∆∞∆°ng (tr·ª´ khi nguy hi·ªÉm)\n4. Gi·ªØ ·∫•m, theo d√µi √Ω th·ª©c\n\nCh·∫•n th∆∞∆°ng c·∫ßn ƒë∆∞·ª£c b√°c sƒ© ƒê√ÅNH GI√Å CHUY√äN M√îN.",
            
            "ng·ªô_ƒë·ªôc": "‚ö†Ô∏è NG·ªò ƒê·ªòC - X·ª¨ TR√ç KH·∫®N:\n1. G·ªåI 115 ho·∫∑c Trung t√¢m Ch·ªëng ƒë·ªôc: (028) 3829 2345\n2. Mang theo bao b√¨/m·∫´u ch·∫•t nghi ng·ªù\n3. KH√îNG t·ª± √Ω g√¢y n√¥n (tr·ª´ khi b√°c sƒ© ch·ªâ d·∫´n)\n4. N·∫øu h√≥a ch·∫•t d√≠nh da: r·ª≠a s·∫°ch b·∫±ng n∆∞·ªõc 15-20 ph√∫t\n\nNg·ªô ƒë·ªôc C·∫¶N ƒëi·ªÅu tr·ªã chuy√™n khoa NGAY."
        }
        
        return responses.get(emergency_type, 
            "‚ö†Ô∏è Tri·ªáu ch·ª©ng b·∫°n m√¥ t·∫£ c√≥ th·ªÉ nghi√™m tr·ªçng. Vui l√≤ng G·ªåI 115 ho·∫∑c ƒë·∫øn c∆° s·ªü y t·∫ø G·∫¶N NH·∫§T ƒë·ªÉ ƒë∆∞·ª£c kh√°m v√† t∆∞ v·∫•n chuy√™n m√¥n.")
    
    def _add_medical_disclaimer(self, answer: str, confidence: float) -> str:
        """Add appropriate medical disclaimer based on confidence (fix issue #9)"""
        if confidence < 0.6:
            disclaimer = "\n\n‚ö†Ô∏è L∆ØU √ù QUAN TR·ªåNG: Th√¥ng tin tr√™n ch·ªâ mang t√≠nh tham kh·∫£o v·ªõi ƒë·ªô tin c·∫≠y TH·∫§P. B·∫°n N√äN ƒëi kh√°m tr·ª±c ti·∫øp t·∫°i c∆° s·ªü y t·∫ø ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c."
        elif confidence < 0.8:
            disclaimer = "\n\nüìã L∆∞u √Ω: Th√¥ng tin tr√™n mang t√≠nh tham kh·∫£o. N·∫øu tri·ªáu ch·ª©ng k√©o d√†i ho·∫∑c n·∫∑ng h∆°n, vui l√≤ng ƒë·∫øn g·∫∑p b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c kh√°m v√† ƒëi·ªÅu tr·ªã ph√π h·ª£p."
        else:
            disclaimer = "\n\nüí° L·ªùi khuy√™n t·ª´ h·ªá th·ªëng AI ch·ªâ mang t√≠nh ch·∫•t tham kh·∫£o. ƒê·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c v√† ƒëi·ªÅu tr·ªã an to√†n, b·∫°n n√™n ƒëi kh√°m tr·ª±c ti·∫øp t·∫°i c∆° s·ªü y t·∫ø."
        
        # Add general disclaimer
        disclaimer += "\n\nüè• H·ªá th·ªëng n√†y KH√îNG thay th·∫ø cho √Ω ki·∫øn c·ªßa b√°c sƒ©. Trong tr∆∞·ªùng h·ª£p kh·∫©n c·∫•p, h√£y g·ªçi 115 ho·∫∑c ƒë·∫øn b·ªánh vi·ªán ngay."
        
        return answer + disclaimer
    
    def _truncate_text(self, text: str, max_chars: int) -> str:
        """Truncate text to complete sentences"""
        if len(text) <= max_chars:
            return text
        
        # Find last complete sentence
        truncated = text[:max_chars]
        for sep in ['. ', '! ', '? ', '; ']:
            idx = truncated.rfind(sep)
            if idx > max_chars * 0.5:  # At least 50% of max_chars
                return truncated[:idx + 1].strip()
        
        # No sentence boundary, cut at word
        idx = truncated.rfind(' ')
        if idx > 0:
            return truncated[:idx] + "..."
        
        return truncated + "..."
    
    def _generate_with_llm(self, query: str, context: str, specialty: str) -> str:
        """Generate answer using Vistral-7B-Chat"""
        if self.generation_model is None or self.generation_tokenizer is None:
            return self._generate_rag_answer_no_llm(query, [], specialty)
        
        # Build prompt following Vistral format
        # NOTE: UI/API will display sources + disclaimer separately.
        # The model should ONLY output the main guidance text.
        system_prompt = f"""B·∫°n l√† b√°c sƒ© t∆∞ v·∫•n AI chuy√™n khoa {specialty}, t∆∞ v·∫•n b·∫±ng ng√¥n ng·ªØ T·ª∞ NHI√äN, G·∫¶N G≈®I.

    QUAN TR·ªåNG - c√°ch vi·∫øt c√¢u tr·∫£ l·ªùi:
    - KH√îNG li·ªát k√™ ki·ªÉu "1. 2. 3." hay "- g·∫°ch ƒë·∫ßu d√≤ng".
    - KH√îNG copy nguy√™n vƒÉn th√¥ng tin RAG.
    - H√£y VI·∫æT L·∫†I th√†nh ƒëo·∫°n vƒÉn t·ª± nhi√™n, m∆∞·ª£t m√†, nh∆∞ ƒëang n√≥i chuy·ªán tr·ª±c ti·∫øp v·ªõi ng∆∞·ªùi h·ªèi.
    - Gi·ªçng vƒÉn th√¢n thi·ªán, d·ªÖ hi·ªÉu, nh∆∞ng chuy√™n nghi·ªáp.

    N·ªôi dung c·∫ßn c√≥ (nh∆∞ng vi·∫øt d·∫°ng vƒÉn xu√¥i, kh√¥ng ƒë√°nh s·ªë):
    - M·ªü ƒë·∫ßu: t√≥m t·∫Øt t√¨nh hu·ªëng (1-2 c√¢u).
    - Ph·∫ßn ch√≠nh: khuy·∫øn ngh·ªã c·ª• th·ªÉ v·ªÅ theo d√µi, x·ª≠ tr√≠, ƒëi·ªÅu c·∫ßn/kh√¥ng n√™n l√†m. Vi·∫øt th√†nh c√°c c√¢u li·ªÅn m·∫°ch, kh√¥ng t√°ch th√†nh list.
    - D·∫•u hi·ªáu c·∫£nh b√°o: nh·∫Øc nh·ªü nh·ªØng tri·ªáu ch·ª©ng n·∫∑ng c·∫ßn kh√°m ngay. T√≠ch h·ª£p v√†o ƒëo·∫°n vƒÉn t·ª± nhi√™n.
    - K·∫øt: 1-2 c√¢u h·ªèi l√†m r√µ n·∫øu c·∫ßn th√™m th√¥ng tin.

    R√ÄNG BU·ªòC:
    - B·∫Øt bu·ªôc d√πng √≠t nh·∫•t 2 chi ti·∫øt t·ª´ "TH√îNG TIN RAG" (s·ªë li·ªáu, ng∆∞·ª°ng, bi·ªán ph√°p c·ª• th·ªÉ).
    - Kh√¥ng b·ªãa th√¥ng tin kh√¥ng c√≥ trong RAG. N·∫øu thi·∫øu, ghi "th√¥ng tin ch∆∞a n√™u r√µ".
    - Kh√¥ng ch√†o h·ªèi, kh√¥ng disclaimer (h·ªá th·ªëng t·ª± th√™m).
    - Ng√¥n ng·ªØ th·∫≠n tr·ªçng: "c√≥ th·ªÉ", "th∆∞·ªùng", "n√™n".

    V√ç D·ª§ C√ÇU TR·∫¢ L·ªúI T·ªêT:
    "Khi tr·∫ª 3 tu·ªïi s·ªët 39 ƒë·ªô, ƒëi·ªÅu ƒë·∫ßu ti√™n l√† theo d√µi th√¢n nhi·ªát ƒë·ªÅu ƒë·∫∑n m·ªói 4 gi·ªù v√† cho b√© u·ªëng nhi·ªÅu n∆∞·ªõc ƒë·ªÉ tr√°nh m·∫•t n∆∞·ªõc. C√≥ th·ªÉ d√πng paracetamol ho·∫∑c ibuprofen theo ch·ªâ ƒë·ªãnh ƒë·ªÉ h·∫° s·ªët, k·∫øt h·ª£p ch∆∞·ªùm ·∫•m l√™n tr√°n. N·∫øu s·ªët k√©o d√†i qu√° 3 ng√†y, tr·∫ª li b√¨ kh√≥ ƒë√°nh th·ª©c, ho·∫∑c th·ªü nhanh co r√∫t l·ªìng ng·ª±c, c·∫ßn ƒë∆∞a b√© ƒë·∫øn c∆° s·ªü y t·∫ø ngay. B·∫°n c√≥ th·ªÉ cho bi·∫øt b√© ƒë√£ u·ªëng thu·ªëc h·∫° s·ªët ch∆∞a v√† c√≥ d·∫•u hi·ªáu n√†o b·∫•t th∆∞·ªùng kh√°c kh√¥ng?"

    V√ç D·ª§ C√ÇU TR·∫¢ L·ªúI X·∫§U (TR√ÅNH):
    "1. Theo d√µi th√¢n nhi·ªát. 2. Cho u·ªëng n∆∞·ªõc. 3. D√πng thu·ªëc h·∫° s·ªët..."

    ƒê·ªãnh d·∫°ng: ti·∫øng Vi·ªát, vƒÉn xu√¥i t·ª± nhi√™n, 2‚Äì4 ƒëo·∫°n ng·∫Øn."""

        user_message = f"""C√ÇU H·ªéI: {query}

    TH√îNG TIN TRUY XU·∫§T (RAG) - ch·ªâ d√πng l√†m ng·ªØ c·∫£nh:
    {context}

    H√£y tr·∫£ l·ªùi ƒë√∫ng theo y√™u c·∫ßu h·ªá th·ªëng."""

        # Format prompt (Vistral uses ChatML format)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            # Apply chat template
            prompt = self.generation_tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = self.generation_tokenizer(
                prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    **inputs,
                    max_new_tokens=settings.MAX_NEW_TOKENS,
                    temperature=settings.TEMPERATURE,
                    top_p=settings.TOP_P,
                    do_sample=True,
                    pad_token_id=self.generation_tokenizer.eos_token_id
                )
            
            # Decode
            response = self.generation_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"[RAG Engine] LLM generation failed: {e}")
            return self._generate_rag_answer_no_llm(query, [], specialty)
    
    def _strip_greeting_noise(self, text: str) -> str:
        if not text:
            return ""
        t = text.strip()
        t = re.sub(r'^\s*(tr·∫£\s*l·ªùi\s*[:\-]?\s*)', '', t, flags=re.IGNORECASE)
        t = re.sub(r'^\s*(xin\s+ch√†o|ch√†o)\s+(b·∫°n|anh|ch·ªã|em|b√°c)\s*[,!.:;-]*\s*', '', t, flags=re.IGNORECASE)
        t = re.sub(r'^\s*(th∆∞a\s+b√°c\s+sƒ©|th∆∞a\s+bs)\s*[,!.:;-]*\s*', '', t, flags=re.IGNORECASE)
        return t.strip()

    def _extract_action_sentences(self, text: str, max_sentences: int = 5) -> List[str]:
        """Extract short actionable sentences from a passage."""
        if not text:
            return []
        raw = self._clean_text(text)
        sents = re.split(r'(?<=[.!?])\s+', raw)
        out: List[str] = []
        for s in sents:
            s = self._strip_greeting_noise(s)
            if len(s) < 12:
                continue
            s_proc = self._preprocess_reference_sentence(s)
            if not s_proc:
                continue
            if self._sentence_has_action(s_proc) or self._sentence_has_action(s):
                out.append(s)
            if len(out) >= max_sentences:
                break
        return out

    def _generate_rag_answer_no_llm(
        self,
        query: str,
        qa_results: List[Dict],
        article_results: List[Dict],
        specialty: str,
    ) -> str:
        """Generate a readable answer using retrieved QAs + Articles (no LLM)."""
        bullets: List[str] = []

        # 1) From QAs: try extracting actionable sentences
        action_text, _, _ = self.find_best_action_sentence(
            user_text=query,
            topk_rows=qa_results[:8],
            sent_sim_thresh=0.55,
            combined_thresh=0.62,
        )
        if action_text:
            cleaned = self._strip_greeting_noise(action_text)
            if cleaned:
                bullets.extend(self._extract_action_sentences(cleaned, max_sentences=4) or [self._truncate_text(cleaned, 280)])

        # 2) From Articles: use best passage to extract actionable sentences
        if article_results:
            passage = (article_results[0].get("snippet") or "").strip()
            passage = self._strip_greeting_noise(passage)
            if passage:
                bullets.extend(self._extract_action_sentences(passage, max_sentences=4))

        # 3) Fallbacks if still empty: use top QA answer / article passage snippet
        if not bullets and qa_results:
            top_answer = self._strip_greeting_noise((qa_results[0].get("answer") or "").strip())
            if top_answer:
                bullets.append(self._truncate_text(top_answer, 300))
        if not bullets and article_results:
            passage = self._strip_greeting_noise((article_results[0].get("snippet") or "").strip())
            if passage:
                bullets.append(self._truncate_text(passage, 300))

        if not bullets:
            return self._generate_generic_medical_advice(query, specialty)

        # Deduplicate + format
        uniq: List[str] = []
        seen = set()
        for b in bullets:
            b2 = re.sub(r'\s+', ' ', (b or '').strip())
            if len(b2) < 12:
                continue
            key = b2.lower()
            if key in seen:
                continue
            seen.add(key)
            uniq.append(b2)
            if len(uniq) >= 6:
                break

        if not uniq:
            return self._generate_generic_medical_advice(query, specialty)

        lines = ["G·ª£i √Ω d·ª±a tr√™n th√¥ng tin tham kh·∫£o:"]
        for b in uniq:
            lines.append(f"- {b}")

        # Add 1‚Äì2 clarifying questions when query is too short
        if len((query or "").strip()) <= 25:
            lines.append("")
            lines.append("B·∫°n c√≥ th·ªÉ cho m√¨nh bi·∫øt th√™m:")
            lines.append("- B·∫°n b·ªã ti√™u ch·∫£y bao l√¢u r·ªìi, s·ªë l·∫ßn/ng√†y, c√≥ s·ªët ho·∫∑c ƒëau b·ª•ng kh√¥ng?")
            lines.append("- C√≥ d·∫•u hi·ªáu m·∫•t n∆∞·ªõc (kh√°t nhi·ªÅu, ti·ªÉu √≠t, ch√≥ng m·∫∑t) ho·∫∑c ƒëi ngo√†i ra m√°u kh√¥ng?")

        return "\n".join(lines)
    
    def get_specialties(self) -> List[Dict]:
        """Get list of available specialties"""
        if self.df_qa is None or 'topic' not in self.df_qa.columns:
            return []
        
        specialty_counts = self.df_qa['topic'].value_counts().to_dict()
        return [
            {"name": name, "count": count}
            for name, count in sorted(specialty_counts.items(), key=lambda x: x[1], reverse=True)
        ]

# Global instance
_rag_engine: Optional[HealthcareRAGEngine] = None

def get_rag_engine() -> HealthcareRAGEngine:
    """Get or create RAG engine singleton"""
    global _rag_engine
    if _rag_engine is None:
        print("[RAG] Step 1: Creating HealthcareRAGEngine instance...")
        _rag_engine = HealthcareRAGEngine()
        print("[RAG] Step 2: Loading models...")
        _rag_engine.load_models()
        print("[RAG] Step 3: Loading data...")
        _rag_engine.load_data()
        print("[RAG] Step 4: Building/loading indices...")
        _rag_engine.build_indices()
        print("[RAG] ‚úÖ All steps completed!")
    return _rag_engine

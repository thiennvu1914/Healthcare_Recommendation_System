"""Core RAG Engine - Healthcare Consultation System"""
import os
import re
import numpy as np
import pandas as pd
import torch
import pickle
import hashlib
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
from underthesea import word_tokenize
import faiss
from html import unescape
import logging

logger = logging.getLogger(__name__)

from api.config import settings

class HealthcareRAGEngine:
    """Main RAG Engine for Healthcare Consultation"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() and settings.USE_GPU else "cpu"
        print(f"[RAG Engine] Initializing on device: {self.device}")
        
        if self.device == "cuda":
            print(f"[RAG Engine] GPU: {torch.cuda.get_device_name(0)}")
            print(f"[RAG Engine] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            # Set default device to GPU
            torch.cuda.set_device(0)
        
        # Models
        self.tokenizer_phobert = None
        self.model_phobert = None
        self.gen_tokenizer = None
        self.gen_model = None
        self.gen_pipe = None
        
        # Indices
        self.qa_index = None
        self.article_index = None
        
        # Data
        self.df_qa = None
        self.df_articles = None
        self.article_texts = []
        
        # Action verbs for filtering
        self.action_verbs = self._load_action_verbs()
        
        # Text cleaning patterns
        self._init_text_patterns()
        
    def _init_text_patterns(self):
        """Initialize regex patterns for text cleaning"""
        self.re_at_prefix = re.compile(r'^@[^:]{0,60}:\s*', flags=re.IGNORECASE)
        self.name_pattern = re.compile(r'\b([A-ZÃ€-á»¸][a-zÃ -á»¹]+(?:_[A-ZÃ€-á»¸][a-zÃ -á»¹]+)+)\b')
        self.doctor_pattern = re.compile(r'\b(BS|BÃ¡c sÄ©|LÆ°Æ¡ng y|Dr)\.?\s+([A-ZÃ€-á»¸][a-zÃ -á»¹_]+(\s+[A-ZÃ€-á»¸][a-zÃ -á»¹_]+)*)', flags=re.IGNORECASE)
        self.pronoun_pattern = re.compile(r'\b(chÃ¡u|em|tá»›|mÃ¬nh|con|anh|chá»‹)\b', flags=re.IGNORECASE)
        
        self.connectives = [
            r'vÃ¬ váº­y', r'vÃ¬ tháº¿', r'váº­y nÃªn', r'do váº­y', r'vÃ¬ váº­y nÃªn', 
            r'vÃ¬ tháº¿ nÃªn', r'cho nÃªn', r'tÃ³m láº¡i', r'tÃ³m táº¯t', r'nhÆ°ng', r'tuy nhiÃªn'
        ]
        self.connective_pattern = re.compile("|".join([re.escape(x) for x in self.connectives]), flags=re.IGNORECASE)
        
    def _load_action_verbs(self) -> set:
        """Load action verbs for filtering actionable sentences"""
        return set([
            # NhÃ³m dÃ¹ng thuá»‘c / Ä‘iá»u trá»‹
            "uá»‘ng", "uá»‘ng thuá»‘c", "dÃ¹ng", "dÃ¹ng thuá»‘c", "xá»‹t", "bÃ´i", "thoa", "nhá»", "ngáº­m", 
            "tiÃªm", "chÃ­ch", "truyá»n", "pháº«u thuáº­t", "má»•", "tiá»ƒu pháº«u", "kÃª Ä‘Æ¡n", "Ä‘iá»u trá»‹",
            "chÆ°á»m", "chÆ°á»m nÃ³ng", "chÆ°á»m láº¡nh", "bÄƒng bÃ³", "sÃ¡t trÃ¹ng", "rá»­a váº¿t thÆ°Æ¡ng",
            "hÃºt rá»­a", "xÃ´ng", "khÃ­ dung", "chÃ¢m cá»©u", "báº¥m huyá»‡t", "massage", "xoa bÃ³p",
            
            # NhÃ³m khÃ¡m / xÃ©t nghiá»‡m
            "khÃ¡m", "Ä‘i khÃ¡m", "tÃ¡i khÃ¡m", "thÄƒm khÃ¡m", "kiá»ƒm tra", "xÃ©t nghiá»‡m", "láº¥y máº«u",
            "siÃªu Ã¢m", "chá»¥p", "chá»¥p x-quang", "chá»¥p ct", "chá»¥p mri", "ná»™i soi", "Ä‘o huyáº¿t Ã¡p",
            "Ä‘o Ä‘Æ°á»ng huyáº¿t", "theo dÃµi", "Ä‘Ã¡nh giÃ¡", "táº§m soÃ¡t",
            
            # NhÃ³m sinh hoáº¡t / dinh dÆ°á»¡ng
            "Äƒn", "Äƒn kiÃªng", "kiÃªng", "trÃ¡nh", "háº¡n cháº¿", "bá»• sung", "tÄƒng cÆ°á»ng", "giáº£m",
            "uá»‘ng nÆ°á»›c", "ngá»§", "nghá»‰ ngÆ¡i", "kÃª gá»‘i", "náº±m nghiÃªng", "táº­p", "táº­p luyá»‡n", 
            "váº­n Ä‘á»™ng", "táº­p váº­t lÃ½ trá»‹ liá»‡u", "thá»ƒ dá»¥c", "vá»‡ sinh", "sÃºc miá»‡ng", "sÃºc há»ng",
            "rá»­a tay", "rá»­a mÅ©i", "Ä‘eo kháº©u trang", "cÃ¡ch ly", "nháº­p viá»‡n", "cáº¥p cá»©u",

            # Bá»• sung tá»« dataset
            'Ä‘i', 'siÃªu', 'ná»™i', 'Ä‘áº·t', 'nhá»', 'bá»•', 'khÃ¡m vÃ ', 'trÃ¡nh thai', 'khÃ¡m bá»‡nh', 
            'Äƒn uá»‘ng', 'khÃ¡m bÃ¡c sÄ©', 'khÃ¡m sá»©c', 'khÃ¡m sá»©c khá»e', 'Ä‘i ngoÃ i', 'kÃª', 
            'Ä‘i siÃªu Ã¢m', 'khÃ¡m thai', 'Ä‘áº·t lá»‹ch', 'khÃ¡m láº¡i', 'Ä‘áº·t lá»‹ch khÃ¡m', 'tiÃªm chá»§ng',
            'Ä‘i khÃ¡m bÃ¡c', 'khÃ¡m phá»¥ khoa', 'Ä‘i khÃ¡m Ä‘á»ƒ', 'khÃ¡m chuyÃªn khoa', 'khÃ¡m phá»¥',
            'Ä‘i khÃ¡m vÃ ', 'khÃ¡m chuyÃªn', 'tiÃªm ngá»«a', 'Ä‘i láº¡i', 'rá»­a', 'Ä‘i tiá»ƒu', 'kiÃªng',
            'tiÃªm mÅ©i', 'khÃ¡m trá»±c', 'chá»¥p x quang', 'tiÃªm vacxin', 'khÃ¡m vÃ  Ä‘iá»u',
            'Ä‘i xÃ©t nghiá»‡m', 'Ä‘i khÃ¡m thai', 'Ä‘i khÃ¡m chuyÃªn', 'tiÃªm phÃ²ng', 'chá»¥p x',
            'cho bÃ© Ä‘i', 'Ä‘i tÃ¡i', 'Ä‘Æ°a bÃ© Ä‘i', 'Ä‘i xÃ©t', 'Ä‘i phÃ¢n', 'khÃ¡m tÆ°',
            'Ä‘i cáº§u', 'Ä‘i ngoÃ i phÃ¢n', 'Ä‘i tÃ¡i khÃ¡m', 'Ä‘áº¿n bá»‡nh viá»‡n', 'khÃ¡m thai Ä‘á»‹nh',
            'khÃ¡m vá»›i', 'tiÃªm vaccine', 'Ä‘i kiá»ƒm tra', 'Ä‘i tiÃªm', 'Ä‘i tiÃªu', 'Ä‘Æ°a bÃ© Ä‘áº¿n',
            'khÃ¡m trá»±c tiáº¿p', 'Ä‘áº·t khÃ¡m', 'Ä‘i khÃ¡m ngay', 'tiÃªm váº¯c xin', 'Ä‘áº·t tÆ° váº¥n',
            'khÃ¡m em', 'Ä‘áº·t thuá»‘c', 'khÃ¡m vá»›i bÃ¡c', 'tiÃªm váº¯c', 'khÃ¡m Ä‘á»ƒ Ä‘Æ°á»£c', 'Ä‘i lÃ m',
            'hÃºt thai', 'siÃªu Ã¢m tim', 'Ä‘i khÃ¡m táº¡i', 'tiÃªm Ä‘Æ°á»£c', 'khÃ¡m khÃ´ng', 'Äƒn vÃ ',
            'hÃºt thuá»‘c', 'lÃ m gÃ¬', 'Ä‘i khÃ¡m bá»‡nh', 'Ä‘i kiá»ƒm', 'siÃªu Ã¢m láº¡i', 'khÃ¡m Ä‘á»‹nh',
            'Ä‘áº¿n khÃ¡m táº¡i', 'Ä‘i vá»‡', 'Ä‘i vá»‡ sinh', 'tiÃªm thuá»‘c', 'dÃ¹ng biá»‡n phÃ¡p',
            'tiÃªm chá»§ng chuyÃªn', 'siÃªu Ã¢m thai', 'khÃ¡m Ä‘á»‹nh ká»³', 'kÃª Ä‘Æ¡n thuá»‘c', 'Äƒn Ä‘Æ°á»£c',
            'Äƒn khÃ´ng', 'liÃªn há»‡ vá»›i', 'khÃ¡m Ä‘á»ƒ bÃ¡c', 'kÃª thuá»‘c', 'uá»‘ng Ä‘á»§ nÆ°á»›c',
            'khÃ¡m táº¡i khoa', 'Ä‘áº·t vÃ²ng', 'khÃ¡m bá»‡nh chuyÃªn', 'khÃ¡m hiáº¿m muá»™n', 'thai',
            'dÃ¹ng biá»‡n', 'Ä‘áº·t cÃ¢u', 'Ä‘áº·t cÃ¢u há»i', 'uá»‘ng vÃ ', 'chá»¥p mri', 'khÃ¡m sá»›m',
            'khÃ¡m cho', 'uá»‘ng cÃ³', 'lÃ m gÃ¬ Ä‘á»ƒ', 'uá»‘ng bá»• sung', 'kÃª toa', 'siÃªu Ã¢m á»Ÿ',
            'ná»™i soi bÃ³c', 'siÃªu Ã¢m tháº¥y', 'khÃ¡m tÆ° váº¥n', 'khÃ¡m hiáº¿m', 'dÃ¹ng cho',
            'Ä‘i kÃ¨m', 'Äƒn Ä‘á»§', 'Äƒn cá»§a', 'khÃ¡m tá»•ng', 'khÃ¡m tá»•ng quÃ¡t', 'khÃ¡m vÃ  siÃªu'
        ])
    
    def load_models(self):
        """Load PhoBERT and Generation models"""
        print("[RAG Engine] Loading PhoBERT...")
        self.tokenizer_phobert = AutoTokenizer.from_pretrained(settings.RETRIEVAL_MODEL, use_fast=True)
        
        try:
            if self.device == "cuda":
                self.model_phobert = AutoModel.from_pretrained(
                    settings.RETRIEVAL_MODEL,
                    device_map="auto",
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True
                )
            else:
                self.model_phobert = AutoModel.from_pretrained(settings.RETRIEVAL_MODEL)
            self.model_phobert.eval()
            print("[RAG Engine] PhoBERT loaded successfully")
        except Exception as e:
            print(f"[RAG Engine] Error loading PhoBERT: {e}")
            raise
        
        # Generation model (optional, load on demand)
        self.generation_model = None
        self.generation_tokenizer = None
        print("[RAG Engine] Generation model will be loaded on first use")
    
    def _load_generation_model(self):
        """Load Vistral-7B-Chat generation model"""
        if self.generation_model is not None:
            return  # Already loaded
        
        try:
            from transformers import AutoModelForCausalLM
            
            model_name = "Viet-Mistral/Vistral-7B-Chat"
            print(f"[RAG Engine] Loading generation model: {model_name}")

            hf_token = (
                (getattr(settings, "HUGGINGFACE_HUB_TOKEN", "") or "").strip()
                or (getattr(settings, "HUGGINGFACE_TOKEN", "") or "").strip()
                or os.getenv("HUGGINGFACE_HUB_TOKEN", "").strip()
                or os.getenv("HUGGINGFACE_TOKEN", "").strip()
                or None
            )
            
            # Prefer `token=...` (newer transformers). Do not pass both.
            tok_kwargs = {"use_fast": True}
            if hf_token:
                tok_kwargs.update({"token": hf_token})
            self.generation_tokenizer = AutoTokenizer.from_pretrained(model_name, **tok_kwargs)
            
            if self.device == "cuda" and torch.cuda.is_available():
                print(f"[RAG Engine] Loading on GPU: {torch.cuda.get_device_name(0)}")
                model_kwargs = {
                    "device_map": "auto",
                    "dtype": torch.float16,
                    "low_cpu_mem_usage": True,
                }
                if hf_token:
                    model_kwargs.update({"token": hf_token})
                self.generation_model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
            else:
                print("[RAG Engine] Loading on CPU")
                model_kwargs = {"dtype": torch.float32}
                if hf_token:
                    model_kwargs.update({"token": hf_token})
                self.generation_model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
                self.generation_model.to(self.device)
            
            self.generation_model.eval()
            print("[RAG Engine] Generation model loaded successfully")
        except Exception as e:
            print(f"[RAG Engine] Failed to load generation model: {e}")
            print("[RAG Engine] Will use template-based generation")
            self.generation_model = None
            self.generation_tokenizer = None
    
    def load_data(self):
        """Load Q&A and Articles data"""
        import sys
        print(f"[RAG Engine] Loading data from {settings.DATA_DIR}")
        sys.stdout.flush()

        # If FAISS cache exists, avoid expensive full-corpus preprocessing at startup.
        # This keeps API boot fast while still loading the full dataset for display/snippets.
        cache_present = (
            settings.ENABLE_CACHE
            and settings.QA_INDEX_CACHE.exists()
            and settings.METADATA_CACHE.exists()
        )
        preprocess_corpus = os.getenv("PREPROCESS_CORPUS", "0") == "1"
        
        # Load QA
        print("[RAG Engine] Reading QA CSV...")
        sys.stdout.flush()
        qa_usecols = None
        if settings.QA_CSV_PATH.exists():
            # Load only the columns we actually use.
            qa_usecols = ["question", "answer", "topic", "topic_original", "advice"]
        self.df_qa = pd.read_csv(settings.QA_CSV_PATH, usecols=lambda c: (qa_usecols is None or c in qa_usecols))
        # Sample if SAMPLE_SIZE > 0, otherwise use all data
        if settings.SAMPLE_SIZE > 0 and len(self.df_qa) > settings.SAMPLE_SIZE:
            self.df_qa = self.df_qa.sample(n=settings.SAMPLE_SIZE, random_state=42).reset_index(drop=True)
        print(f"[RAG Engine] Loaded {len(self.df_qa)} Q&A records")
        sys.stdout.flush()
        
        # Always normalize dtypes; optional preprocessing is VERY expensive for full datasets.
        for col in ["question", "answer", "advice", "topic", "topic_original"]:
            if col in self.df_qa.columns:
                self.df_qa[col] = self.df_qa[col].fillna("").astype(str)

        # Only preprocess if explicitly requested AND cache isn't present.
        if preprocess_corpus and not cache_present:
            for col in ["question", "answer", "advice"]:
                if col in self.df_qa.columns:
                    self.df_qa[col] = self.df_qa[col].apply(self._preprocess_text)
        
        # Load Articles
        if settings.ARTICLES_CSV_PATH.exists():
            article_usecols = ["id", "title", "text"]
            self.df_articles = pd.read_csv(settings.ARTICLES_CSV_PATH, usecols=lambda c: c in article_usecols)
            # Sample if SAMPLE_SIZE > 0, otherwise use all data
            if settings.SAMPLE_SIZE > 0 and len(self.df_articles) > settings.SAMPLE_SIZE:
                self.df_articles = self.df_articles.sample(n=settings.SAMPLE_SIZE, random_state=42).reset_index(drop=True)
            print(f"[RAG Engine] Loaded {len(self.df_articles)} articles")
            
            # Prepare article texts (use itertuples for speed)
            self.article_texts = []
            for row in self.df_articles.itertuples(index=False, name="ArticleRow"):
                title = str(getattr(row, "title", "")).strip()
                text = str(getattr(row, "text", "")).strip()
                self.article_texts.append((
                    str(getattr(row, "id", "")),
                    title,
                    f"{title}\n\n{text}"
                ))
        else:
            print("[RAG Engine] Articles file not found, skipping")
            self.df_articles = pd.DataFrame()
    
    def build_indices(self):
        """Build HNSW indices for fast retrieval with caching support"""
        import sys
        print("[RAG Engine] build_indices() started")
        sys.stdout.flush()
        
        # Try to load from cache first (fix issue #6: avoid rebuild every time)
        if settings.ENABLE_CACHE and self._try_load_indices():
            logger.info("[RAG Engine] Successfully loaded indices from cache")
            print("[RAG Engine] Successfully loaded indices from cache")
            sys.stdout.flush()
            return
        
        logger.info("[RAG Engine] Building HNSW indices from scratch...")
        print("[RAG Engine] Building HNSW indices from scratch...")
        sys.stdout.flush()
        
        # Build QA index
        qa_texts = (self.df_qa["question"].fillna("") + " " + self.df_qa["answer"].fillna("")).tolist()
        qa_embeddings = []
        
        print(f"[RAG Engine] Encoding {len(qa_texts)} Q&A pairs...")
        for i, text in enumerate(qa_texts):
            if i % 500 == 0:
                print(f"  Progress: {i}/{len(qa_texts)}")
            qa_embeddings.append(self._sentence_embedding(text))
        
        qa_embeddings = np.vstack(qa_embeddings).astype("float32")
        self.qa_index = self._build_hnsw_index(qa_embeddings)
        print(f"[RAG Engine] QA index built with {self.qa_index.ntotal} elements")
        
        # Build Article index
        article_embeddings = None
        if self.article_texts:
            print(f"[RAG Engine] Encoding {len(self.article_texts)} articles...")
            article_embeddings = []
            for i, (_, _, content) in enumerate(self.article_texts):
                if i % 500 == 0:
                    print(f"  Progress: {i}/{len(self.article_texts)}")
                article_embeddings.append(self._sentence_embedding(content))
            
            article_embeddings = np.vstack(article_embeddings).astype("float32")
            self.article_index = self._build_hnsw_index(article_embeddings)
            print(f"[RAG Engine] Article index built with {self.article_index.ntotal} elements")
        
        # Save to cache
        if settings.ENABLE_CACHE:
            self._save_indices(qa_embeddings, article_embeddings)
    
    def _save_indices(self, qa_embeddings: np.ndarray, article_embeddings: Optional[np.ndarray] = None):
        """Save HNSW indices and embeddings to cache"""
        try:
            settings.CACHE_DIR.mkdir(parents=True, exist_ok=True)
            
            # Save HNSW indices
            if self.qa_index:
                faiss.write_index(self.qa_index, str(settings.QA_INDEX_CACHE))
                logger.info(f"Saved QA index to {settings.QA_INDEX_CACHE}")
            
            if self.article_index:
                faiss.write_index(self.article_index, str(settings.ARTICLE_INDEX_CACHE))
                logger.info(f"Saved article index to {settings.ARTICLE_INDEX_CACHE}")
            
            # Save embeddings
            if qa_embeddings is not None:
                np.save(settings.QA_EMBEDDINGS_CACHE, qa_embeddings)
                logger.info(f"Saved QA embeddings to {settings.QA_EMBEDDINGS_CACHE}")
            
            if article_embeddings is not None:
                np.save(settings.ARTICLE_EMBEDDINGS_CACHE, article_embeddings)
                logger.info(f"Saved article embeddings to {settings.ARTICLE_EMBEDDINGS_CACHE}")
            
            # Save metadata (data hash for validation)
            metadata = {
                "qa_count": len(self.df_qa),
                "article_count": len(self.article_texts),
                "sample_size": settings.SAMPLE_SIZE,
                "data_hash": self._compute_data_hash()
            }
            with open(settings.METADATA_CACHE, "wb") as f:
                pickle.dump(metadata, f)
            
            logger.info("[RAG Engine] All indices saved to cache successfully")
        except Exception as e:
            logger.warning(f"Failed to save indices to cache: {e}")
    
    def _try_load_indices(self) -> bool:
        """Try to load indices from cache"""
        try:
            # Check if cache files exist
            if not all([
                settings.QA_INDEX_CACHE.exists(),
                settings.METADATA_CACHE.exists()
            ]):
                logger.info("Cache files not found, will build from scratch")
                return False
            
            # Load and validate metadata
            with open(settings.METADATA_CACHE, "rb") as f:
                metadata = pickle.load(f)
            
            # Validate data hasn't changed (older metadata may not include data_hash/sample_size)
            current_hash = self._compute_data_hash()
            meta_sample_size = metadata.get("sample_size")
            if meta_sample_size is not None and meta_sample_size != settings.SAMPLE_SIZE:
                logger.info("Sample size has changed, invalidating cache")
                return False

            meta_hash = metadata.get("data_hash")
            if meta_hash is not None and meta_hash != current_hash:
                logger.info("Data has changed, invalidating cache")
                return False

            # Backfill missing fields for older cache metadata
            if meta_hash is None:
                metadata["data_hash"] = current_hash
            if meta_sample_size is None:
                metadata["sample_size"] = settings.SAMPLE_SIZE
            try:
                with open(settings.METADATA_CACHE, "wb") as f:
                    pickle.dump(metadata, f)
            except Exception:
                pass
            
            # Load QA index
            self.qa_index = faiss.read_index(str(settings.QA_INDEX_CACHE))
            logger.info(f"Loaded QA index with {self.qa_index.ntotal} elements")
            
            # Load article index if exists
            if settings.ARTICLE_INDEX_CACHE.exists() and len(self.article_texts) > 0:
                self.article_index = faiss.read_index(str(settings.ARTICLE_INDEX_CACHE))
                logger.info(f"Loaded article index with {self.article_index.ntotal} elements")
            
            return True
            
        except Exception as e:
            logger.warning(f"Failed to load indices from cache: {e}")
            return False
    
    def _compute_data_hash(self) -> str:
        """Compute hash of data files to detect changes"""
        try:
            hash_obj = hashlib.md5()
            
            # Hash QA file
            if settings.QA_CSV_PATH.exists():
                hash_obj.update(str(settings.QA_CSV_PATH.stat().st_mtime).encode())
                hash_obj.update(str(settings.QA_CSV_PATH.stat().st_size).encode())
            
            # Hash articles file
            if settings.ARTICLES_CSV_PATH.exists():
                hash_obj.update(str(settings.ARTICLES_CSV_PATH.stat().st_mtime).encode())
                hash_obj.update(str(settings.ARTICLES_CSV_PATH.stat().st_size).encode())
            
            # Include sample size in hash
            hash_obj.update(str(settings.SAMPLE_SIZE).encode())
            
            return hash_obj.hexdigest()
        except Exception as e:
            logger.warning(f"Failed to compute data hash: {e}")
            return "unknown"
    
    def _preprocess_text(self, s: str) -> str:
        """Preprocess Vietnamese text"""
        if not isinstance(s, str):
            return ""
        s = re.sub(r'\s+', ' ', s.strip())
        # IMPORTANT: keep preprocessing consistent with how embeddings in cache were built.
        # Our full-corpus GPU rebuild embeds raw text without word segmentation.
        use_word_tokenize = os.getenv("USE_WORD_TOKENIZE", "0") == "1"
        if not use_word_tokenize:
            return s
        try:
            return word_tokenize(s, format="text")
        except Exception:
            return s
    
    def _preprocess_reference_sentence(self, s: str) -> str:
        """Preprocess reference sentence for embedding (remove names, pronouns, etc.)"""
        if not s:
            return ""
        
        s = s.strip()
        
        # Skip questions
        if s.endswith('?'):
            return ""
        
        # Remove @ prefix
        s = self.re_at_prefix.sub("", s)
        
        # Remove "tráº£ lá»i"
        s = re.sub(r'^tráº£[_\s]lá»i\s*[:.]?\s*', '', s, flags=re.IGNORECASE)
        
        # Remove names with underscore
        s = self.name_pattern.sub("", s)
        
        # Remove doctor names
        s = self.doctor_pattern.sub("", s)
        
        # Replace pronouns with "báº¡n"
        s = self.pronoun_pattern.sub("báº¡n", s)
        
        # Remove connectives
        s = self.connective_pattern.sub("", s)
        
        # Clean whitespace
        s = re.sub(r'\s+', ' ', s).strip()
        
        return s
    
    def _clean_text(self, t: str) -> str:
        """Clean text - unescape HTML and normalize whitespace"""
        return re.sub(r'\s+', ' ', unescape(t.strip())).strip()
    
    def _sentence_has_action(self, s: str) -> bool:
        """Check if sentence contains action verbs"""
        if not s:
            return False
        
        sl = s.lower()
        for act in self.action_verbs:
            act_norm = act.replace("_", " ").lower()
            if re.search(r'\b' + re.escape(act_norm) + r'\b', sl):
                return True
        return False
    
    def _sentence_embedding(self, text: str) -> np.ndarray:
        """Generate sentence embedding using PhoBERT"""
        if not text:
            return np.zeros(768, dtype=np.float32)
        
        try:
            inputs = self.tokenizer_phobert(text, return_tensors="pt", truncation=True, max_length=256)
            model_device = next(self.model_phobert.parameters()).device
            inputs = {k: v.to(model_device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model_phobert(**inputs)

                # Mean pooling over last hidden state (sentence embedding)
                last_hidden = outputs.last_hidden_state
                attention_mask = inputs.get("attention_mask")

                if attention_mask is not None:
                    attention_mask = attention_mask.unsqueeze(-1)
                    masked = last_hidden * attention_mask
                    summed = masked.sum(dim=1)
                    counts = attention_mask.sum(dim=1).clamp(min=1e-9)
                    mean_pooled = (summed / counts).squeeze().cpu().numpy().astype("float32")
                else:
                    mean_pooled = last_hidden.mean(dim=1).squeeze().cpu().numpy().astype("float32")

                return mean_pooled
        except Exception as e:
            print(f"[Embedding Error] {e}")
            return np.zeros(768, dtype=np.float32)
    
    def _build_hnsw_index(self, vectors: np.ndarray) -> Optional[faiss.Index]:
        """Build HNSW index from vectors using faiss"""
        if vectors is None or vectors.size == 0:
            return None
        
        try:
            dim = vectors.shape[1]
            # Use IndexFlatIP for inner product (similar to cosine similarity)
            index = faiss.IndexFlatIP(dim)
            # Normalize vectors for cosine similarity
            faiss.normalize_L2(vectors)
            index.add(vectors)
            return index
        except Exception as e:
            print(f"[HNSW Error] {e}")
            return None
    
    def retrieve_qa(self, query: str, k: int = 5) -> List[Dict]:
        """Retrieve top-k Q&A pairs"""
        if self.qa_index is None:
            return []
        
        query_processed = self._preprocess_text(query)
        user_emb = self._sentence_embedding(query_processed).reshape(1, -1)
        
        # Normalize query embedding for cosine similarity
        faiss.normalize_L2(user_emb)
        
        raw_k = min(max(k * 10, 30), self.qa_index.ntotal)
        distances, labels = self.qa_index.search(user_emb, k=raw_k)

        q_tokens = set([t.lower() for t in re.findall(r'\w+', query) if len(t) >= 2])

        candidates = []
        for dist, idx in zip(distances[0], labels[0]):
            if idx < 0 or idx >= len(self.df_qa):
                continue

            row = self.df_qa.iloc[int(idx)]
            sim = float(dist)

            # lexical overlap helps avoid off-topic high-embedding matches
            qa_text = f"{row.get('question', '')} {row.get('answer', '')}".lower()
            qa_tokens = set([t.lower() for t in re.findall(r'\w+', qa_text) if len(t) >= 2])
            lex = 0.0
            if q_tokens:
                lex = float(len(q_tokens & qa_tokens)) / max(1, len(q_tokens))

            combined = 0.85 * sim + 0.15 * lex
            candidates.append((combined, sim, lex, int(idx), row))

        # Sort by combined score
        candidates.sort(key=lambda x: x[0], reverse=True)

        results = []
        for combined, sim, lex, idx, row in candidates:
            # Keep a lower gate because combined scoring already filters noise
            if sim < settings.QUESTION_SIM_THRESHOLD and combined < (settings.QUESTION_SIM_THRESHOLD + 0.05):
                continue

            results.append({
                "score": float(combined),
                "sim": float(sim),
                "lex": float(lex),
                "index": idx,
                "question": row.get("question", ""),
                "answer": row.get("answer", ""),
                "topic": row.get("topic", "KhÃ¡c")
            })
            if len(results) >= k:
                break

        return results
    
    def retrieve_articles(self, query: str, k: int = 1) -> List[Dict]:
        """Retrieve top-k articles with re-ranking"""
        if self.article_index is None or not self.article_texts:
            return []
        
        query_processed = self._preprocess_text(query)
        user_emb = self._sentence_embedding(query_processed).reshape(1, -1)
        q_tokens_set = set([t.lower() for t in re.findall(r'\w+', query) if len(t) >= 2])
        
        # Normalize query embedding for cosine similarity
        faiss.normalize_L2(user_emb)
        
        raw_k = min(k * 3, self.article_index.ntotal)
        distances, labels = self.article_index.search(user_emb, k=raw_k)
        
        # Get raw candidates
        raw_candidates = []
        for dist, idx in zip(distances[0], labels[0]):
            if idx < 0 or idx >= len(self.article_texts):
                continue
            
            raw_candidates.append({
                "index": int(idx),
                # With normalized vectors, inner product == cosine similarity
                "score": float(dist)
            })
        
        # Re-rank with lexical overlap and title boost
        reranked = []
        w_sim = 0.75
        w_lex = 0.20
        w_title_boost = 0.05
        
        for c in raw_candidates:
            idx = int(c['index'])
            link, title, content = self.article_texts[idx]
            baseline_sim = float(c.get('score', 0.0))
            
            # Extract actual URL from content if present
            url_match = re.search(r'https?://[^\s]+', content)
            actual_link = url_match.group(0) if url_match else link
            
            # Lexical overlap with title + snippet
            article_snippet = title + " " + (content[:1000] if content else "")
            art_tokens = set([t.lower() for t in re.findall(r'\w+', article_snippet) if len(t) >= 2])
            
            lex_overlap = 0.0
            if q_tokens_set:
                common = len(q_tokens_set & art_tokens)
                lex_overlap = common / max(1, len(q_tokens_set))
            
            # Title boost
            title_tokens = set([t.lower() for t in re.findall(r'\w+', title) if len(t) >= 2])
            title_boost_flag = 1.0 if (q_tokens_set & title_tokens) else 0.0
            
            # Combined score
            combined = w_sim * baseline_sim + w_lex * lex_overlap + w_title_boost * title_boost_flag
            
            # Find best passage
            passages = self._chunk_text(content, max_chars=600)
            best_passage = ""
            best_passage_sim = -1.0
            
            for p in passages[:6]:
                p_proc = self._preprocess_text(p)
                p_emb = self._sentence_embedding(p_proc).astype("float32")
                sim_p = self._cosine_sim(user_emb.reshape(-1), p_emb)
                
                if sim_p > best_passage_sim:
                    best_passage_sim = sim_p
                    best_passage = p
            
            if not best_passage and content:
                best_passage = content[:600]
            
            reranked.append({
                "index": idx,
                "link": actual_link,
                "title": title,
                "txt": content,
                "baseline_sim": baseline_sim,
                "lex_overlap": lex_overlap,
                "title_boost": title_boost_flag,
                "combined_score": combined,
                "best_passage": best_passage,
                "best_passage_sim": best_passage_sim,
                "score": combined  # For compatibility
            })
        
        # Sort by combined score
        reranked_sorted = sorted(reranked, key=lambda x: x["combined_score"], reverse=True)
        
        # Filter by minimum score
        final = [r for r in reranked_sorted if r["combined_score"] >= settings.COMBINED_SCORE_THRESHOLD]
        
        if not final:
            return []
        
        # Return top-k
        results = []
        for r in final[:k]:
            results.append({
                "score": r["combined_score"],
                "index": r["index"],
                "title": r["title"],
                "link": r["link"],
                "snippet": r["best_passage"]
            })
        
        return results
    
    def _chunk_text(self, text: str, max_chars: int = 500, overlap_ratio: float = 0.15) -> List[str]:
        """Chunk text into overlapping passages"""
        if not text:
            return []
        
        passages = []
        start = 0
        L = len(text)
        overlap_chars = int(max_chars * overlap_ratio)
        
        while start < L:
            end = start + max_chars
            if end >= L:
                passages.append(text[start:L].strip())
                break
            
            # Find sentence boundary
            cut = text.rfind('.', start, end)
            if cut <= start:
                cut = end
            
            passages.append(text[start:cut].strip())
            
            # Move with overlap
            if overlap_chars > 0:
                start = max(cut - overlap_chars, start + 1)
            else:
                start = cut
        
        return [p for p in passages if p]
    
    def _cosine_sim(self, a: np.ndarray, b: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors"""
        if a is None or b is None:
            return 0.0
        na = np.linalg.norm(a)
        nb = np.linalg.norm(b)
        if na == 0 or nb == 0:
            return 0.0
        return float(np.dot(a, b) / (na * nb))
    
    def find_best_action_sentence(
        self, 
        user_text: str,
        topk_rows: List[Dict],
        sent_sim_thresh: float = 0.6,
        combined_thresh: float = 0.68,
        alpha: float = 0.75,
        beta: float = 0.2,
        gamma: float = 0.05
    ) -> Tuple[Optional[str], Optional[int], float]:
        """
        Find best action sentence from top-k Q&A results
        
        Args:
            user_text: User query
            topk_rows: List of top-k Q&A results
            sent_sim_thresh: Sentence similarity threshold
            combined_thresh: Combined score threshold
            alpha: Weight for sentence similarity
            beta: Weight for question similarity
            gamma: Weight for lexical overlap
            
        Returns:
            (action_text, ref_index, score)
        """
        if not topk_rows:
            return None, None, 0.0
        
        # Extract and preprocess all sentences
        all_sents = []
        for ref_pos, r in enumerate(topk_rows, start=1):
            question_text = r.get("question", "") or ""
            raw_answer = r.get("answer", "") or ""
            
            # Split answer into sentences
            sents = re.split(r'(?<=[.!?])\s+', raw_answer.strip()) if raw_answer else []
            
            for s in sents:
                s_orig = self._clean_text(s)
                s_proc = self._preprocess_reference_sentence(s_orig)
                
                if len(s_proc) >= 6:
                    all_sents.append((ref_pos, question_text, s_orig, s_proc))
        
        if not all_sents:
            return None, None, 0.0
        
        # Get user embedding
        user_q = self._preprocess_text(user_text)
        user_emb = self._sentence_embedding(user_q).astype("float32")
        user_tokens_set = set([t.lower() for t in re.findall(r'\w+', user_text) if len(t) >= 2])
        
        # Cache question embeddings
        question_emb_cache = {}
        scored = []
        
        for ref_pos, question_text, sent_orig, sent_proc in all_sents:
            # Get question embedding
            if ref_pos not in question_emb_cache:
                q_text_proc = self._preprocess_text(question_text) if question_text else ""
                if q_text_proc:
                    question_emb_cache[ref_pos] = self._sentence_embedding(q_text_proc).astype("float32")
                else:
                    question_emb_cache[ref_pos] = np.zeros(user_emb.shape, dtype=np.float32)
            
            # Get sentence embedding
            s_emb = self._sentence_embedding(sent_proc).astype("float32")
            
            # Calculate similarities
            sim_sent = self._cosine_sim(user_emb, s_emb)
            sim_q = self._cosine_sim(user_emb, question_emb_cache[ref_pos])
            
            # Lexical overlap
            sent_tokens_set = set([t.lower() for t in re.findall(r'\w+', sent_proc) if len(t) >= 2])
            lex_overlap = float(len(user_tokens_set & sent_tokens_set)) / max(1, len(user_tokens_set)) if user_tokens_set else 0.0
            
            # Combined score
            combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap
            
            scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))
        
        # Sort by combined score
        scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)
        
        # Find best action sentence
        best_ref_pos = None
        best_combined_score = 0.0
        
        for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:
            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:
                if self._sentence_has_action(sent_proc) or self._sentence_has_action(sent_orig):
                    best_ref_pos = ref_pos
                    best_combined_score = combined
                    break
        
        if best_ref_pos is None:
            return None, None, 0.0
        
        # Collect all action sentences from best reference
        final_sentences_list = []
        for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:
            if ref_pos == best_ref_pos:
                if sim_sent >= sent_sim_thresh and combined >= combined_thresh:
                    if self._sentence_has_action(sent_proc) or self._sentence_has_action(sent_orig):
                        # Replace pronouns
                        s_final = self.pronoun_pattern.sub("báº¡n", sent_orig)
                        s_final = re.sub(r'\s+', ' ', s_final).strip()
                        
                        if s_final not in final_sentences_list:
                            final_sentences_list.append(s_final)
        
        if not final_sentences_list:
            return None, None, 0.0
        
        final_paragraph = " ".join(final_sentences_list)
        return final_paragraph, best_ref_pos, best_combined_score

    def _generate_generic_medical_advice(self, query: str, specialty: str) -> str:
        """Safe fallback when retrieval is low-quality (no LLM required)."""
        q = (query or "").strip()
        lines = []
        lines.append("MÃ¬nh chÆ°a tÃ¬m Ä‘Æ°á»£c thÃ´ng tin tham kháº£o Ä‘á»§ sÃ¡t vá»›i cÃ¢u há»i Ä‘á»ƒ Ä‘Æ°a ra hÆ°á»›ng dáº«n cá»¥ thá»ƒ.")
        if q:
            lines.append(f"CÃ¢u há»i cá»§a báº¡n: {q}")
        lines.append("")
        lines.append("Báº¡n cÃ³ thá»ƒ bá»• sung thÃªm giÃºp mÃ¬nh:")
        lines.append("- Triá»‡u chá»©ng chÃ­nh lÃ  gÃ¬, xuáº¥t hiá»‡n bao lÃ¢u, má»©c Ä‘á»™ tÄƒng/giáº£m?")
        lines.append("- CÃ³ sá»‘t, nÃ´n, tiÃªu cháº£y/tÃ¡o bÃ³n, chÃ³ng máº·t, khÃ³ thá»Ÿ, Ä‘au ngá»±c, ngáº¥t khÃ´ng?")
        lines.append("- Tuá»•i, bá»‡nh ná»n, thuá»‘c Ä‘ang dÃ¹ng (náº¿u cÃ³).")
        lines.append("")
        lines.append("Náº¿u cÃ³ dáº¥u hiá»‡u náº·ng (Ä‘au dá»¯ dá»™i, nÃ´n ra mÃ¡u, Ä‘i ngoÃ i phÃ¢n Ä‘en/ra mÃ¡u, sá»‘t cao liÃªn tá»¥c, ngáº¥t, khÃ³ thá»Ÿ, Ä‘au ngá»±c...), báº¡n cáº§n Ä‘i cáº¥p cá»©u ngay.")
        return "\n".join(lines)
    
    def generate_answer(self, query: str, qa_results: List[Dict], article_results: List[Dict]) -> Tuple[str, str, float]:
        """Generate natural answer using retrieved context with medical safety checks"""
        # Log retrieval results
        logger.info(f"Query: {query}")
        logger.info(f"QA results: {len(qa_results)} found, best score: {qa_results[0]['score'] if qa_results else 0}")
        logger.info(f"Article results: {len(article_results)} found, best score: {article_results[0]['score'] if article_results else 0}")
        
        # Determine specialty
        specialty = "Y táº¿ tá»•ng quÃ¡t"
        if qa_results:
            specialty = (qa_results[0].get("topic") or "").strip() or "Y táº¿ tá»•ng quÃ¡t"
            # Avoid low-signal buckets like "KhÃ¡c" when possible
            if specialty.lower() in {"khÃ¡c", "khac", "other"}:
                for qa in qa_results[:5]:
                    cand = (qa.get("topic") or "").strip()
                    if cand and cand.lower() not in {"khÃ¡c", "khac", "other"}:
                        specialty = cand
                        break
                else:
                    specialty = "Y táº¿ tá»•ng quÃ¡t"
        
        # Medical safety check (fix issue #9: detect emergencies)
        emergency_detected, emergency_type = self._check_emergency_keywords(query)
        
        # Retrieval quality
        best_qa_score = qa_results[0]['score'] if qa_results else 0.0
        best_article_score = article_results[0]['score'] if article_results else 0.0
        best_score = max(best_qa_score, best_article_score)

        # If nothing retrieved, fall back to generic medical advice.
        if not qa_results and not article_results:
            logger.warning("No retrieval results. Using generic response.")
            answer = self._generate_generic_medical_advice(query, specialty)
            confidence = 0.3
            return answer, specialty, confidence
        
        # Build context for LLM only (UI will show sources separately)
        retrieved_content = ""
        if qa_results or article_results:
            context_parts = []
            for qa in qa_results[:3]:
                q = (qa.get("question") or "").strip()
                a = (qa.get("answer") or "").strip()
                if q or a:
                    context_parts.append(f"Q: {self._truncate_text(q, 180)}\nA: {self._truncate_text(a, 300)}")
            if article_results:
                title = (article_results[0].get("title") or "").strip()
                snip = (article_results[0].get("snippet") or "").strip()
                if title or snip:
                    context_parts.append(f"Article: {self._truncate_text(title, 180)}\n{self._truncate_text(snip, 300)}")
            retrieved_content = "\n\n".join([p for p in context_parts if p])
        
        # Calculate confidence based on the best available source
        confidence = 0.35
        if best_score > 0:
            confidence = float(min(0.95, best_score + 0.1))
        
        # If emergency detected, return urgent warning
        if emergency_detected:
            logger.warning(f"Emergency detected: {emergency_type} in query: {query[:50]}...")
            answer = self._generate_emergency_response(emergency_type, specialty)
            return answer, specialty, 0.95  # High confidence for emergency warnings
        
        # Use template-based answer (faster, more reliable)
        # LLM generation disabled by default to avoid memory issues
        # Set ENABLE_LLM_GENERATION=1 in env to enable
        use_llm = os.getenv('ENABLE_LLM_GENERATION', '0') == '1'
        
        if use_llm and self.generation_model is None:
            try:
                self._load_generation_model()
            except Exception as e:
                logger.warning(f"Failed to load generation model: {e}. Using template.")
        
        if use_llm and self.generation_model is not None:
            print(f"[RAG] âœ… Using LLM generation for query: {query[:50]}...")
            answer = self._generate_with_llm(query, retrieved_content, specialty)
        else:
            print(f"[RAG] âš ï¸ Using template-based generation (no LLM) for query: {query[:50]}...")
            answer = self._generate_rag_answer_no_llm(query, qa_results, article_results, specialty)

        return answer, specialty, confidence

    def build_disclaimer(self, specialty: str, confidence: float) -> str:
        """Return medical disclaimer text (kept separate so UI can format consistently)."""
        base = (
            "ThÃ´ng tin chá»‰ mang tÃ­nh cháº¥t tham kháº£o. "
            "Báº¡n nÃªn Ä‘i khÃ¡m trá»±c tiáº¿p Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c hÆ¡n."
        )

        if confidence < 0.6:
            base = (
                "ThÃ´ng tin chá»‰ mang tÃ­nh cháº¥t tham kháº£o vá»›i Ä‘á»™ tin cáº­y tháº¥p. "
                "Báº¡n nÃªn Ä‘i khÃ¡m trá»±c tiáº¿p Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c hÆ¡n."
            )

        if specialty and specialty.strip():
            base += f" (Gá»£i Ã½ chuyÃªn khoa: {specialty.strip()})."

        base += "\n\nHá»‡ thá»‘ng nÃ y KHÃ”NG thay tháº¿ cho Ã½ kiáº¿n cá»§a bÃ¡c sÄ©. Trong trÆ°á»ng há»£p kháº©n cáº¥p, hÃ£y gá»i 115 hoáº·c Ä‘áº¿n bá»‡nh viá»‡n ngay."
        return base
    
    def _check_emergency_keywords(self, query: str) -> Tuple[bool, str]:
        """Detect emergency medical situations (fix issue #9: safety guardrails)"""
        query_lower = query.lower()
        
        # Critical emergency keywords
        critical_keywords = {
            "nguy_ká»‹ch": ["nguy ká»‹ch", "hÃ´n mÃª", "báº¥t tá»‰nh", "ngáº¥t xá»‰u", "thá»Ÿ gáº¥p", "khÃ³ thá»Ÿ náº·ng", "co giáº­t"],
            "cháº£y_mÃ¡u": ["cháº£y mÃ¡u nhiá»u", "xuáº¥t huyáº¿t", "mÃ¡u cháº£y khÃ´ng ngá»«ng", "mÃ¡u Ä‘á» tÆ°Æ¡i"],
            "Ä‘au_ngá»±c": ["Ä‘au ngá»±c dá»¯ dá»™i", "Ä‘au tháº¯t ngá»±c", "Ä‘au tim", "ngháº¹t ngá»±c"],
            "Ä‘á»™t_quá»µ": ["liá»‡t ná»­a ngÆ°á»i", "mÃ©o miá»‡ng", "nÃ³i láº¯p", "yáº¿u má»™t bÃªn", "Ä‘á»™t quá»µ"],
            "tai_náº¡n": ["tai náº¡n nghiÃªm trá»ng", "gÃ£y xÆ°Æ¡ng", "cháº¥n thÆ°Æ¡ng náº·ng", "xe Ä‘Ã¢m"],
            "ngá»™_Ä‘á»™c": ["ngá»™ Ä‘á»™c", "uá»‘ng nháº§m", "Äƒn pháº£i", "ngáº¥t sau khi Äƒn"]
        }
        
        for emergency_type, keywords in critical_keywords.items():
            if any(kw in query_lower for kw in keywords):
                return True, emergency_type
        
        return False, ""
    
    def _generate_emergency_response(self, emergency_type: str, specialty: str) -> str:
        """Generate urgent response for emergency situations"""
        responses = {
            "nguy_ká»‹ch": "âš ï¸ TÃŒNH HUá»NG KHáº¨N Cáº¤P: Triá»‡u chá»©ng báº¡n mÃ´ táº£ CÃ“ THá»‚ nghiÃªm trá»ng. NGAY Láº¬P Tá»¨C:\n1. Gá»ŒI 115 (cáº¥p cá»©u) hoáº·c Ä‘Æ°a ngÆ°á»i bá»‡nh Ä‘áº¿n bá»‡nh viá»‡n Gáº¦N NHáº¤T\n2. Giá»¯ bÃ¬nh tÄ©nh, theo dÃµi Ã½ thá»©c vÃ  nhá»‹p thá»Ÿ\n3. KHÃ”NG tá»± Ã½ cho uá»‘ng thuá»‘c\n\nÄÃ¢y KHÃ”NG pháº£i lá»i khuyÃªn thay tháº¿ cáº¥p cá»©u y táº¿ chuyÃªn nghiá»‡p.",
            
            "cháº£y_mÃ¡u": "âš ï¸ Cáº¢NH BÃO: Cháº£y mÃ¡u nhiá»u cáº§n Xá»¬ TRÃ NGAY:\n1. áº¤n trá»±c tiáº¿p vÃ o váº¿t thÆ°Æ¡ng báº±ng váº£i sáº¡ch\n2. NÃ¢ng cao vá»‹ trÃ­ bá»‹ thÆ°Æ¡ng (náº¿u cÃ³ thá»ƒ)\n3. Gá»ŒI 115 hoáº·c Ä‘áº¿n cáº¥p cá»©u NGAY náº¿u mÃ¡u khÃ´ng cáº§m\n4. KHÃ”NG bá» bÄƒng ra khi mÃ¡u Ä‘Ã£ Ä‘Ã´ng\n\nCáº§n Ä‘Ã¡nh giÃ¡ y táº¿ KHáº¨N Cáº¤P.",
            
            "Ä‘au_ngá»±c": "âš ï¸ KHáº¨N Cáº¤P TIM Máº CH: Äau ngá»±c cÃ³ thá»ƒ lÃ  dáº¥u hiá»‡u nhá»“i mÃ¡u cÆ¡ tim.\nHÃ€NH Äá»˜NG NGAY:\n1. Gá»ŒI 115 hoáº·c Ä‘áº¿n cáº¥p cá»©u NGAY Láº¬P Tá»¨C\n2. Ngá»“i nghá»‰, KHÃ”NG váº­n Ä‘á»™ng\n3. Náº¿u cÃ³ sáºµn: nhai 1 viÃªn aspirin 300mg (trá»« khi dá»‹ á»©ng)\n4. Theo dÃµi nhá»‹p tim, hÃ´ háº¥p\n\nThá»i gian lÃ  vÃ ng - Má»–I PHÃšT trÃ¬ hoÃ£n lÃ m tÄƒng nguy cÆ¡.",
            
            "Ä‘á»™t_quá»µ": "âš ï¸ Dáº¤U HIá»†U Äá»˜T QUá»´ - HÃ€NH Äá»˜NG NGAY:\nTEST NHANH (FAST):\n- Face (máº·t): cÆ°á»i cÃ³ mÃ©o miá»‡ng?\n- Arms (tay): giÆ¡ 2 tay cÃ³ tay nÃ o yáº¿u?\n- Speech (nÃ³i): nÃ³i cÃ³ láº¯p?\n- Time (thá»i gian): Gá»ŒI 115 NGAY!\n\nâœ… ÄÆ¯A Bá»†NH NHÃ‚N Äáº¾N Bá»†NH VIá»†N TRONG 4.5 GIá»œ Äáº¦U\nâŒ KHÃ”NG cho Äƒn, uá»‘ng (nguy cÆ¡ sáº·c)\n\nÄá»™t quá»µ lÃ  KHáº¨N Cáº¤P Y Táº¾!",
            
            "tai_náº¡n": "âš ï¸ TAI Náº N - Cáº¦N Há»– TRá»¢ Y Táº¾:\n1. Äáº£m báº£o an toÃ n hiá»‡n trÆ°á»ng\n2. Gá»ŒI 115 náº¿u: cháº¥n thÆ°Æ¡ng Ä‘áº§u/cá»™t sá»‘ng, gÃ£y xÆ°Æ¡ng, cháº£y mÃ¡u nhiá»u\n3. KHÃ”NG di chuyá»ƒn ngÆ°á»i bá»‹ thÆ°Æ¡ng (trá»« khi nguy hiá»ƒm)\n4. Giá»¯ áº¥m, theo dÃµi Ã½ thá»©c\n\nCháº¥n thÆ°Æ¡ng cáº§n Ä‘Æ°á»£c bÃ¡c sÄ© ÄÃNH GIÃ CHUYÃŠN MÃ”N.",
            
            "ngá»™_Ä‘á»™c": "âš ï¸ NGá»˜ Äá»˜C - Xá»¬ TRÃ KHáº¨N:\n1. Gá»ŒI 115 hoáº·c Trung tÃ¢m Chá»‘ng Ä‘á»™c: (028) 3829 2345\n2. Mang theo bao bÃ¬/máº«u cháº¥t nghi ngá»\n3. KHÃ”NG tá»± Ã½ gÃ¢y nÃ´n (trá»« khi bÃ¡c sÄ© chá»‰ dáº«n)\n4. Náº¿u hÃ³a cháº¥t dÃ­nh da: rá»­a sáº¡ch báº±ng nÆ°á»›c 15-20 phÃºt\n\nNgá»™ Ä‘á»™c Cáº¦N Ä‘iá»u trá»‹ chuyÃªn khoa NGAY."
        }
        
        return responses.get(emergency_type, 
            "âš ï¸ Triá»‡u chá»©ng báº¡n mÃ´ táº£ cÃ³ thá»ƒ nghiÃªm trá»ng. Vui lÃ²ng Gá»ŒI 115 hoáº·c Ä‘áº¿n cÆ¡ sá»Ÿ y táº¿ Gáº¦N NHáº¤T Ä‘á»ƒ Ä‘Æ°á»£c khÃ¡m vÃ  tÆ° váº¥n chuyÃªn mÃ´n.")
    
    def _add_medical_disclaimer(self, answer: str, confidence: float) -> str:
        """Add appropriate medical disclaimer based on confidence (fix issue #9)"""
        if confidence < 0.6:
            disclaimer = "\n\nâš ï¸ LÆ¯U Ã QUAN TRá»ŒNG: ThÃ´ng tin trÃªn chá»‰ mang tÃ­nh tham kháº£o vá»›i Ä‘á»™ tin cáº­y THáº¤P. Báº¡n NÃŠN Ä‘i khÃ¡m trá»±c tiáº¿p táº¡i cÆ¡ sá»Ÿ y táº¿ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c."
        elif confidence < 0.8:
            disclaimer = "\n\nðŸ“‹ LÆ°u Ã½: ThÃ´ng tin trÃªn mang tÃ­nh tham kháº£o. Náº¿u triá»‡u chá»©ng kÃ©o dÃ i hoáº·c náº·ng hÆ¡n, vui lÃ²ng Ä‘áº¿n gáº·p bÃ¡c sÄ© Ä‘á»ƒ Ä‘Æ°á»£c khÃ¡m vÃ  Ä‘iá»u trá»‹ phÃ¹ há»£p."
        else:
            disclaimer = "\n\nðŸ’¡ Lá»i khuyÃªn tá»« há»‡ thá»‘ng AI chá»‰ mang tÃ­nh cháº¥t tham kháº£o. Äá»ƒ Ä‘Æ°á»£c cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c vÃ  Ä‘iá»u trá»‹ an toÃ n, báº¡n nÃªn Ä‘i khÃ¡m trá»±c tiáº¿p táº¡i cÆ¡ sá»Ÿ y táº¿."
        
        # Add general disclaimer
        disclaimer += "\n\nðŸ¥ Há»‡ thá»‘ng nÃ y KHÃ”NG thay tháº¿ cho Ã½ kiáº¿n cá»§a bÃ¡c sÄ©. Trong trÆ°á»ng há»£p kháº©n cáº¥p, hÃ£y gá»i 115 hoáº·c Ä‘áº¿n bá»‡nh viá»‡n ngay."
        
        return answer + disclaimer
    
    def _truncate_text(self, text: str, max_chars: int) -> str:
        """Truncate text to complete sentences"""
        if len(text) <= max_chars:
            return text
        
        # Find last complete sentence
        truncated = text[:max_chars]
        for sep in ['. ', '! ', '? ', '; ']:
            idx = truncated.rfind(sep)
            if idx > max_chars * 0.5:  # At least 50% of max_chars
                return truncated[:idx + 1].strip()
        
        # No sentence boundary, cut at word
        idx = truncated.rfind(' ')
        if idx > 0:
            return truncated[:idx] + "..."
        
        return truncated + "..."
    
    def _generate_with_llm(self, query: str, context: str, specialty: str) -> str:
        """Generate answer using Vistral-7B-Chat"""
        if self.generation_model is None or self.generation_tokenizer is None:
            return self._generate_rag_answer_no_llm(query, [], specialty)
        
        # Build prompt following Vistral format
        # NOTE: UI/API will display sources + disclaimer separately.
        # The model should ONLY output the main guidance text.
        system_prompt = f"""Báº¡n lÃ  bÃ¡c sÄ© tÆ° váº¥n AI chuyÃªn khoa {specialty}, tÆ° váº¥n báº±ng ngÃ´n ngá»¯ Tá»° NHIÃŠN, Gáº¦N GÅ¨I.

    QUAN TRá»ŒNG - cÃ¡ch viáº¿t cÃ¢u tráº£ lá»i:
    - KHÃ”NG liá»‡t kÃª kiá»ƒu "1. 2. 3." hay "- gáº¡ch Ä‘áº§u dÃ²ng".
    - KHÃ”NG copy nguyÃªn vÄƒn thÃ´ng tin RAG.
    - HÃ£y VIáº¾T Láº I thÃ nh Ä‘oáº¡n vÄƒn tá»± nhiÃªn, mÆ°á»£t mÃ , nhÆ° Ä‘ang nÃ³i chuyá»‡n trá»±c tiáº¿p vá»›i ngÆ°á»i há»i.
    - Giá»ng vÄƒn thÃ¢n thiá»‡n, dá»… hiá»ƒu, nhÆ°ng chuyÃªn nghiá»‡p.

    Ná»™i dung cáº§n cÃ³ (nhÆ°ng viáº¿t dáº¡ng vÄƒn xuÃ´i, khÃ´ng Ä‘Ã¡nh sá»‘):
    - Má»Ÿ Ä‘áº§u: tÃ³m táº¯t tÃ¬nh huá»‘ng (1-2 cÃ¢u).
    - Pháº§n chÃ­nh: khuyáº¿n nghá»‹ cá»¥ thá»ƒ vá» theo dÃµi, xá»­ trÃ­, Ä‘iá»u cáº§n/khÃ´ng nÃªn lÃ m. Viáº¿t thÃ nh cÃ¡c cÃ¢u liá»n máº¡ch, khÃ´ng tÃ¡ch thÃ nh list.
    - Dáº¥u hiá»‡u cáº£nh bÃ¡o: nháº¯c nhá»Ÿ nhá»¯ng triá»‡u chá»©ng náº·ng cáº§n khÃ¡m ngay. TÃ­ch há»£p vÃ o Ä‘oáº¡n vÄƒn tá»± nhiÃªn.
    - Káº¿t: 1-2 cÃ¢u há»i lÃ m rÃµ náº¿u cáº§n thÃªm thÃ´ng tin.

    RÃ€NG BUá»˜C:
    - Báº¯t buá»™c dÃ¹ng Ã­t nháº¥t 2 chi tiáº¿t tá»« "THÃ”NG TIN RAG" (sá»‘ liá»‡u, ngÆ°á»¡ng, biá»‡n phÃ¡p cá»¥ thá»ƒ).
    - KhÃ´ng bá»‹a thÃ´ng tin khÃ´ng cÃ³ trong RAG. Náº¿u thiáº¿u, ghi "thÃ´ng tin chÆ°a nÃªu rÃµ".
    - KhÃ´ng chÃ o há»i, khÃ´ng disclaimer (há»‡ thá»‘ng tá»± thÃªm).
    - NgÃ´n ngá»¯ tháº­n trá»ng: "cÃ³ thá»ƒ", "thÆ°á»ng", "nÃªn".

    VÃ Dá»¤ CÃ‚U TRáº¢ Lá»œI Tá»T:
    "Khi tráº» 3 tuá»•i sá»‘t 39 Ä‘á»™, Ä‘iá»u Ä‘áº§u tiÃªn lÃ  theo dÃµi thÃ¢n nhiá»‡t Ä‘á»u Ä‘áº·n má»—i 4 giá» vÃ  cho bÃ© uá»‘ng nhiá»u nÆ°á»›c Ä‘á»ƒ trÃ¡nh máº¥t nÆ°á»›c. CÃ³ thá»ƒ dÃ¹ng paracetamol hoáº·c ibuprofen theo chá»‰ Ä‘á»‹nh Ä‘á»ƒ háº¡ sá»‘t, káº¿t há»£p chÆ°á»m áº¥m lÃªn trÃ¡n. Náº¿u sá»‘t kÃ©o dÃ i quÃ¡ 3 ngÃ y, tráº» li bÃ¬ khÃ³ Ä‘Ã¡nh thá»©c, hoáº·c thá»Ÿ nhanh co rÃºt lá»“ng ngá»±c, cáº§n Ä‘Æ°a bÃ© Ä‘áº¿n cÆ¡ sá»Ÿ y táº¿ ngay. Báº¡n cÃ³ thá»ƒ cho biáº¿t bÃ© Ä‘Ã£ uá»‘ng thuá»‘c háº¡ sá»‘t chÆ°a vÃ  cÃ³ dáº¥u hiá»‡u nÃ o báº¥t thÆ°á»ng khÃ¡c khÃ´ng?"

    VÃ Dá»¤ CÃ‚U TRáº¢ Lá»œI Xáº¤U (TRÃNH):
    "1. Theo dÃµi thÃ¢n nhiá»‡t. 2. Cho uá»‘ng nÆ°á»›c. 3. DÃ¹ng thuá»‘c háº¡ sá»‘t..."

    Äá»‹nh dáº¡ng: tiáº¿ng Viá»‡t, vÄƒn xuÃ´i tá»± nhiÃªn, 2â€“4 Ä‘oáº¡n ngáº¯n."""

        user_message = f"""CÃ‚U Há»ŽI: {query}

    THÃ”NG TIN TRUY XUáº¤T (RAG) - chá»‰ dÃ¹ng lÃ m ngá»¯ cáº£nh:
    {context}

    HÃ£y tráº£ lá»i Ä‘Ãºng theo yÃªu cáº§u há»‡ thá»‘ng."""

        # Format prompt (Vistral uses ChatML format)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            # Apply chat template
            prompt = self.generation_tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = self.generation_tokenizer(
                prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.generation_model.generate(
                    **inputs,
                    max_new_tokens=settings.MAX_NEW_TOKENS,
                    temperature=settings.TEMPERATURE,
                    top_p=settings.TOP_P,
                    do_sample=True,
                    pad_token_id=self.generation_tokenizer.eos_token_id
                )
            
            # Decode
            response = self.generation_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"[RAG Engine] LLM generation failed: {e}")
            return self._generate_rag_answer_no_llm(query, [], specialty)
    
    def _strip_greeting_noise(self, text: str) -> str:
        if not text:
            return ""
        t = text.strip()
        t = re.sub(r'^\s*(tráº£\s*lá»i\s*[:\-]?\s*)', '', t, flags=re.IGNORECASE)
        t = re.sub(r'^\s*(xin\s+chÃ o|chÃ o)\s+(báº¡n|anh|chá»‹|em|bÃ¡c)\s*[,!.:;-]*\s*', '', t, flags=re.IGNORECASE)
        t = re.sub(r'^\s*(thÆ°a\s+bÃ¡c\s+sÄ©|thÆ°a\s+bs)\s*[,!.:;-]*\s*', '', t, flags=re.IGNORECASE)
        return t.strip()

    def _extract_action_sentences(self, text: str, max_sentences: int = 5) -> List[str]:
        """Extract short actionable sentences from a passage."""
        if not text:
            return []
        raw = self._clean_text(text)
        sents = re.split(r'(?<=[.!?])\s+', raw)
        out: List[str] = []
        for s in sents:
            s = self._strip_greeting_noise(s)
            if len(s) < 12:
                continue
            s_proc = self._preprocess_reference_sentence(s)
            if not s_proc:
                continue
            if self._sentence_has_action(s_proc) or self._sentence_has_action(s):
                out.append(s)
            if len(out) >= max_sentences:
                break
        return out

    def _generate_rag_answer_no_llm(
        self,
        query: str,
        qa_results: List[Dict],
        article_results: List[Dict],
        specialty: str,
    ) -> str:
        """Generate a readable answer using retrieved QAs + Articles (no LLM)."""
        bullets: List[str] = []

        # 1) From QAs: try extracting actionable sentences
        action_text, _, _ = self.find_best_action_sentence(
            user_text=query,
            topk_rows=qa_results[:8],
            sent_sim_thresh=0.55,
            combined_thresh=0.62,
        )
        if action_text:
            cleaned = self._strip_greeting_noise(action_text)
            if cleaned:
                bullets.extend(self._extract_action_sentences(cleaned, max_sentences=4) or [self._truncate_text(cleaned, 280)])

        # 2) From Articles: use best passage to extract actionable sentences
        if article_results:
            passage = (article_results[0].get("snippet") or "").strip()
            passage = self._strip_greeting_noise(passage)
            if passage:
                bullets.extend(self._extract_action_sentences(passage, max_sentences=4))

        # 3) Fallbacks if still empty: use top QA answer / article passage snippet
        if not bullets and qa_results:
            top_answer = self._strip_greeting_noise((qa_results[0].get("answer") or "").strip())
            if top_answer:
                bullets.append(self._truncate_text(top_answer, 300))
        if not bullets and article_results:
            passage = self._strip_greeting_noise((article_results[0].get("snippet") or "").strip())
            if passage:
                bullets.append(self._truncate_text(passage, 300))

        if not bullets:
            return self._generate_generic_medical_advice(query, specialty)

        # Deduplicate + format
        uniq: List[str] = []
        seen = set()
        for b in bullets:
            b2 = re.sub(r'\s+', ' ', (b or '').strip())
            if len(b2) < 12:
                continue
            key = b2.lower()
            if key in seen:
                continue
            seen.add(key)
            uniq.append(b2)
            if len(uniq) >= 6:
                break

        if not uniq:
            return self._generate_generic_medical_advice(query, specialty)

        lines = ["Gá»£i Ã½ dá»±a trÃªn thÃ´ng tin tham kháº£o:"]
        for b in uniq:
            lines.append(f"- {b}")

        # Add 1â€“2 clarifying questions when query is too short
        if len((query or "").strip()) <= 25:
            lines.append("")
            lines.append("Báº¡n cÃ³ thá»ƒ cho mÃ¬nh biáº¿t thÃªm:")
            lines.append("- Báº¡n bá»‹ tiÃªu cháº£y bao lÃ¢u rá»“i, sá»‘ láº§n/ngÃ y, cÃ³ sá»‘t hoáº·c Ä‘au bá»¥ng khÃ´ng?")
            lines.append("- CÃ³ dáº¥u hiá»‡u máº¥t nÆ°á»›c (khÃ¡t nhiá»u, tiá»ƒu Ã­t, chÃ³ng máº·t) hoáº·c Ä‘i ngoÃ i ra mÃ¡u khÃ´ng?")

        return "\n".join(lines)
    
    def get_specialties(self) -> List[Dict]:
        """Get list of available specialties"""
        if self.df_qa is None or 'topic' not in self.df_qa.columns:
            return []
        
        specialty_counts = self.df_qa['topic'].value_counts().to_dict()
        return [
            {"name": name, "count": count}
            for name, count in sorted(specialty_counts.items(), key=lambda x: x[1], reverse=True)
        ]

# Global instance
_rag_engine: Optional[HealthcareRAGEngine] = None

def get_rag_engine() -> HealthcareRAGEngine:
    """Get or create RAG engine singleton"""
    global _rag_engine
    if _rag_engine is None:
        print("[RAG] Step 1: Creating HealthcareRAGEngine instance...")
        _rag_engine = HealthcareRAGEngine()
        print("[RAG] Step 2: Loading models...")
        _rag_engine.load_models()
        print("[RAG] Step 3: Loading data...")
        _rag_engine.load_data()
        print("[RAG] Step 4: Building/loading indices...")
        _rag_engine.build_indices()
        print("[RAG] âœ… All steps completed!")
    return _rag_engine

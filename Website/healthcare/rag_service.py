"""
RAG Service v·ªõi LLM (Gemini API) cho Healthcare System
Implement ƒë√∫ng theo pipeline trong notebook: PhoBERT Embedding + HNSWLIB + LLM Generation
"""

import os
import re
import json
from typing import List, Dict, Tuple, Optional
import numpy as np
from django.conf import settings

# Import AI libraries
try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False
    print("‚ö†Ô∏è google-generativeai not installed. Install with: pip install google-generativeai")

from .models import QuestionAnswer, Article


class HealthcareRAGService:
    """
    RAG Service s·ª≠ d·ª•ng:
    - TF-IDF/PhoBERT cho embedding (tu·ª≥ config)
    - HNSWLIB cho fast retrieval  
    - Gemini API cho LLM generation
    """
    
    def __init__(self, use_llm: bool = False):
        # M·∫∑c ƒë·ªãnh kh√¥ng d√πng LLM API (ƒë·ªÉ ch·∫°y offline)
        self.use_llm = False
        self.gemini_model = None
        print("‚ÑπÔ∏è Running in OFFLINE mode (no API needed)")
        
        # Fallback to TF-IDF if no LLM
        if not self.use_llm:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            self.vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
            self.qa_vectors = None
            self.qa_ids = []
        
        # Vietnamese stop words
        self.stop_words = {
            'v√†', 'ho·∫∑c', 'l√†', 'c√≥', 'ƒë∆∞·ª£c', 'c·ªßa', 'do', 't·ª´', 'trong', 
            'n√†y', 'ƒë√≥', 'th√¨', 'cho', 'ƒë√£', 'v·ªõi', 'ƒë·ªÉ', 'khi', 'n√™n', 
            'c·∫ßn', 'ph·∫£i', 'hay', 'r·∫±ng', 'v√¨', 'n√†o', 'n·∫øu'
        }
        
        # Specialty mapping
        self.specialty_keywords = {
            'ch·ªânh h√¨nh': ['x∆∞∆°ng', 'kh·ªõp', 'g√£y', 'ƒëau l∆∞ng', 'c·ªôt s·ªëng'],
            'nhi khoa': ['b√©', 'tr·∫ª', 'em', 'b√© s∆° sinh', 'tr·∫ª s∆° sinh'],
            'tim m·∫°ch': ['tim', 'nh·ªãp', 'huy·∫øt √°p', 'm·∫°ch', 'ƒëau ng·ª±c'],
            'ti√™u h√≥a': ['d·∫° d√†y', 'ƒÉn', 'ti√™u', 'ru·ªôt', 'gan', 't·ª•y'],
            'h√¥ h·∫•p': ['ph·ªïi', 'th·ªü', 'ho', 'c·∫£m l·∫°nh', 'hen'],
            'da li·ªÖu': ['da', 'n·ªïi m·∫©n', 'ng·ª©a', 'm·ª•n'],
            'tai m≈©i h·ªçng': ['tai', 'm≈©i', 'h·ªçng', 'vi√™m amidan'],
            'ph·ª• s·∫£n': ['mang thai', 'c√≥ thai', 'thai k·ª≥', 'sinh'],
            'y t·∫ø chung': []
        }
    
    def initialize_indices(self):
        """Initialize TF-IDF vectors"""
        print("üîß Initializing RAG indices...")
        qas = QuestionAnswer.objects.all().values('qa_id', 'question', 'answer', 'topic')
        
        qa_texts = []
        self.qa_ids = []
        
        for qa in qas:
            combined_text = f"{qa['question']} {qa['answer']}"
            qa_texts.append(combined_text)
            self.qa_ids.append(qa['qa_id'])
        
        if qa_texts and not self.use_llm:
            self.qa_vectors = self.vectorizer.fit_transform(qa_texts)
            print(f"‚úÖ Initialized {len(qa_texts)} Q&As")
    
    def retrieve_context(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Retrieve top-k most relevant Q&As
        """
        if not self.use_llm:
            # TF-IDF fallback
            from sklearn.metrics.pairwise import cosine_similarity
            query_vector = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.qa_vectors)[0]
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.05:
                    qa_id = self.qa_ids[idx]
                    qa = QuestionAnswer.objects.get(qa_id=qa_id)
                    results.append({
                        'qa_id': qa.qa_id,
                        'question': qa.question,
                        'answer': qa.answer,
                        'topic': qa.topic,
                        'similarity': float(similarities[idx])
                    })
            return results
        else:
            # Use database search for now (can upgrade to HNSWLIB later)
            from django.db.models import Q
            qas = QuestionAnswer.objects.filter(
                Q(question__icontains=query) | Q(answer__icontains=query)
            )[:top_k]
            
            return [{
                'qa_id': qa.qa_id,
                'question': qa.question,
                'answer': qa.answer,
                'topic': qa.topic,
                'similarity': 0.8  # Placeholder
            } for qa in qas]
    
    def suggest_specialty(self, query: str) -> Optional[str]:
        """Detect specialty from query"""
        query_lower = query.lower()
        for specialty, keywords in self.specialty_keywords.items():
            for keyword in keywords:
                if keyword in query_lower:
                    return specialty
        return None
    
    def generate_answer_with_llm(
        self, 
        query: str, 
        context_qas: List[Dict],
        specialty: Optional[str] = None
    ) -> str:
        """
        Generate natural answer using Gemini API (RAG pattern)
        ƒê√¢y l√† ph·∫ßn th·ª±c s·ª± d√πng LLM nh∆∞ trong notebook!
        """
        if not self.use_llm or not self.gemini_model:
            return self._fallback_answer(query, context_qas, specialty)
        
        # Build context from retrieved Q&As
        context_text = ""
        for i, qa in enumerate(context_qas[:3], 1):
            context_text += f"\n[Tham kh·∫£o {i}]\n"
            context_text += f"C√¢u h·ªèi: {qa['question']}\n"
            context_text += f"Tr·∫£ l·ªùi: {qa['answer'][:500]}\n"  # Limit length
            context_text += f"Chuy√™n khoa: {qa['topic']}\n"
        
        # Create prompt (similar to notebook's prompt)
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω y t·∫ø AI chuy√™n nghi·ªáp. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t∆∞ v·∫•n s·ª©c kh·ªèe d·ª±a tr√™n c√°c t√†i li·ªáu tham kh·∫£o t·ª´ b√°c sƒ©.

C√ÅC T√ÄI LI·ªÜU THAM KH·∫¢O:
{context_text}

QUAN TR·ªåNG:
- CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c t√†i li·ªáu tham kh·∫£o tr√™n
- KH√îNG th√™m th√¥ng tin ngo√†i nh·ªØng g√¨ c√≥ trong t√†i li·ªáu
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, d·ªÖ hi·ªÉu
- N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, n√≥i r√µ v√† khuy√™n n√™n g·∫∑p b√°c sƒ©

C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG:
"{query}"

H√£y tr·∫£ l·ªùi theo format sau:

**Chuy√™n khoa:** [T√™n chuy√™n khoa n·∫øu x√°c ƒë·ªãnh ƒë∆∞·ª£c]

**L·ªùi khuy√™n:**
[C√¢u tr·∫£ l·ªùi t·ªïng h·ª£p t·ª´ t√†i li·ªáu tham kh·∫£o, ng·∫Øn g·ªçn 2-4 c√¢u]

**C·∫ßn l∆∞u √Ω:**
- [C√°c ƒëi·ªÅu c·∫ßn ch√∫ √Ω, vi·ªác n√™n l√†m]

**Tham kh·∫£o:** [1], [2] (n·∫øu c√≥)
"""
        
        try:
            # Call Gemini API
            response = self.gemini_model.generate_content(prompt)
            answer = response.text.strip()
            
            # Add disclaimer
            answer += "\n\n‚ö†Ô∏è **L∆∞u √Ω:** Th√¥ng tin tr√™n ch·ªâ mang t√≠nh tham kh·∫£o. Vui l√≤ng g·∫∑p b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c."
            
            return answer
            
        except Exception as e:
            print(f"‚ùå Gemini API error: {e}")
            return self._fallback_answer(query, context_qas, specialty)
    
    def _fallback_answer(
        self, 
        query: str, 
        context_qas: List[Dict],
        specialty: Optional[str]
    ) -> str:
        """
        Intelligent template-based answer - T·ªïng h·ª£p t·ª´ nhi·ªÅu Q&As
        ƒê√¢y l√† RAG kh√¥ng d√πng LLM nh∆∞ng v·∫´n smart!
        """
        if not context_qas:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p. Vui l√≤ng li√™n h·ªá b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n."
        
        # L·∫•y Q&A t·ªët nh·∫•t
        best_qa = context_qas[0]
        answer_text = best_qa['answer']
        
        # Tr√≠ch xu·∫•t c√°c c√¢u quan tr·ªçng (ch·ª©a action keywords)
        action_keywords = ['n√™n', 'c·∫ßn', 'ph·∫£i', 'ƒëi kh√°m', 'x√©t nghi·ªám', 'u·ªëng thu·ªëc', 
                          'theo d√µi', 'tr√°nh', 'ki√™ng', 'ch∆∞·ªùm', 'ngh·ªâ ng∆°i', 'b·ªï sung']
        
        important_sentences = []
        for sentence in answer_text.split('.'):
            sentence = sentence.strip()
            if len(sentence) < 10:
                continue
            sentence_lower = sentence.lower()
            if any(kw in sentence_lower for kw in action_keywords):
                important_sentences.append(sentence)
        
        # N·∫øu kh√¥ng c√≥ c√¢u quan tr·ªçng, l·∫•y 3 c√¢u ƒë·∫ßu
        if not important_sentences:
            sentences = [s.strip() for s in answer_text.split('.') if len(s.strip()) > 10]
            important_sentences = sentences[:3]
        
        # T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi
        main_answer = '. '.join(important_sentences[:4]) + '.'
        
        # Th√™m th√¥ng tin t·ª´ Q&As kh√°c
        additional_info = []
        for qa in context_qas[1:3]:  # L·∫•y 2 Q&As ti·∫øp theo
            for sentence in qa['answer'].split('.'):
                sentence = sentence.strip()
                if len(sentence) < 15:
                    continue
                sentence_lower = sentence.lower()
                # Ch·ªâ l·∫•y c√¢u c√≥ th√¥ng tin m·ªõi
                if any(kw in sentence_lower for kw in ['c·∫ßn', 'n√™n', 'ph·∫£i']):
                    if sentence not in main_answer:  # Tr√°nh l·∫∑p
                        additional_info.append(sentence)
                        break
        
        # Build final answer
        result = f"**V·ªÅ c√¢u h·ªèi:** *{query}*\n\n"
        
        if specialty:
            specialty_names = {
                'ch·ªânh h√¨nh': 'Ch·ªânh H√¨nh',
                'nhi khoa': 'Nhi Khoa',
                'tim m·∫°ch': 'Tim M·∫°ch',
                'ti√™u h√≥a': 'Ti√™u H√≥a',
                'h√¥ h·∫•p': 'H√¥ H·∫•p',
                'da li·ªÖu': 'Da Li·ªÖu',
                'tai m≈©i h·ªçng': 'Tai M≈©i H·ªçng',
                'ph·ª• s·∫£n': 'Ph·ª• S·∫£n',
                'y t·∫ø chung': 'Y T·∫ø Chung'
            }
            specialty_display = specialty_names.get(specialty, specialty.title())
            result += f"üè• **Chuy√™n khoa:** {specialty_display}\n\n"
        
        result += f"**üí° L·ªùi khuy√™n t·ª´ b√°c sƒ©:**\n\n"
        result += f"{main_answer}\n\n"
        
        # Th√™m th√¥ng tin b·ªï sung n·∫øu c√≥
        if additional_info:
            result += f"**üìå Th√¥ng tin th√™m:**\n\n"
            for info in additional_info[:2]:
                result += f"‚Ä¢ {info}.\n"
            result += "\n"
        
        # Tr√≠ch xu·∫•t c√°c h√†nh ƒë·ªông c·ª• th·ªÉ
        actions = []
        for qa in context_qas[:2]:
            answer_lower = qa['answer'].lower()
            if 'ƒëi kh√°m' in answer_lower or 'kh√°m b√°c sƒ©' in answer_lower:
                actions.append('üè• ƒêi kh√°m b√°c sƒ© chuy√™n khoa')
            if 'x√©t nghi·ªám' in answer_lower:
                actions.append('üî¨ L√†m x√©t nghi·ªám theo ch·ªâ ƒë·ªãnh')
            if 'u·ªëng thu·ªëc' in answer_lower or 'd√πng thu·ªëc' in answer_lower:
                actions.append('üíä D√πng thu·ªëc theo ƒë∆°n c·ªßa b√°c sƒ©')
            if 'theo d√µi' in answer_lower:
                actions.append('üëÅÔ∏è Theo d√µi tri·ªáu ch·ª©ng')
        
        # Deduplicate actions
        actions = list(dict.fromkeys(actions))
        
        if actions:
            result += f"**‚úÖ C√°c vi·ªác c·∫ßn l√†m:**\n\n"
            for action in actions[:4]:
                result += f"{action}\n"
            result += "\n"
        
        result += f"üìö **Ngu·ªìn tham kh·∫£o:** {len(context_qas)} c√¢u tr·∫£ l·ªùi t·ª´ b√°c sƒ© chuy√™n khoa\n\n"
        result += "‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:** Th√¥ng tin tr√™n ch·ªâ mang t√≠nh tham kh·∫£o. Vui l√≤ng g·∫∑p b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c kh√°m v√† t∆∞ v·∫•n ch√≠nh x√°c."
        
        return result
    
    def generate_rag_response(
        self, 
        query: str, 
        top_k: int = 5
    ) -> Dict:
        """
        Main RAG pipeline: Retrieve + Generate
        ƒê√¢y l√† pipeline ch√≠nh nh∆∞ trong notebook!
        """
        # Step 1: Retrieve relevant context
        context_qas = self.retrieve_context(query, top_k=top_k)
        
        # Step 2: Detect specialty
        specialty = self.suggest_specialty(query)
        
        # Step 3: Generate answer with LLM (ho·∫∑c fallback)
        ai_answer = self.generate_answer_with_llm(query, context_qas, specialty)
        
        # Step 4: Return structured result
        return {
            'query': query,
            'ai_answer': ai_answer,
            'context_qas': context_qas,
            'suggested_specialty': specialty,
            'used_llm': self.use_llm,
            'model': 'Gemini-Pro' if self.use_llm else 'Template-based'
        }


# Singleton instance
_rag_service_instance = None

def get_rag_service() -> HealthcareRAGService:
    """Get singleton instance of RAG Service"""
    global _rag_service_instance
    if _rag_service_instance is None:
        # Lu√¥n ch·∫°y offline mode (kh√¥ng c·∫ßn API)
        _rag_service_instance = HealthcareRAGService(use_llm=False)
        _rag_service_instance.initialize_indices()
        print("‚úÖ RAG Service initialized (Offline mode)")
    return _rag_service_instance

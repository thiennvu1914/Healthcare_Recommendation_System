{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e4e35273bae7438fb63c95b6c7d15995":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_030f625fd8a743bca45a05df1154db81","IPY_MODEL_1355a4923b134c9d9ab4975cc525287b","IPY_MODEL_23315028e9734ee59b19be8f8c3fd9cc"],"layout":"IPY_MODEL_e458a104de614caaac7b0112d2cb28db"}},"030f625fd8a743bca45a05df1154db81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b64f4df25cd4207a8f461474d102841","placeholder":"​","style":"IPY_MODEL_e2124290f4434374868728d8bc06cdeb","value":"config.json: 100%"}},"1355a4923b134c9d9ab4975cc525287b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9387163e4ccd4edb93befc0ee876b85c","max":557,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f5e54857beb437ca76e1ebdf43b9e7c","value":557}},"23315028e9734ee59b19be8f8c3fd9cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8e060e482f04593823a063a3a234234","placeholder":"​","style":"IPY_MODEL_5f66025041014855880e23e65ea0fab2","value":" 557/557 [00:00&lt;00:00, 40.8kB/s]"}},"e458a104de614caaac7b0112d2cb28db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b64f4df25cd4207a8f461474d102841":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2124290f4434374868728d8bc06cdeb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9387163e4ccd4edb93befc0ee876b85c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f5e54857beb437ca76e1ebdf43b9e7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d8e060e482f04593823a063a3a234234":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f66025041014855880e23e65ea0fab2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1eede5e44dc54bc2a095c4f430d44bc1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf5e3546fff54141ac813158ebb62de4","IPY_MODEL_a7373db488044c34a5ce3915e6b7c7d3","IPY_MODEL_06702a6d3a1c48728ed601df975178db"],"layout":"IPY_MODEL_a3faa043c867455eb2792e9a7369cb06"}},"cf5e3546fff54141ac813158ebb62de4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb3dcc260d6a422b952448665a83933d","placeholder":"​","style":"IPY_MODEL_d0b804bc140446d390a81a384a7564e4","value":"vocab.txt: "}},"a7373db488044c34a5ce3915e6b7c7d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc128f4e1c9c4654913d8c5d501f4b76","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45221f6806aa4c4694c71adea03e63d2","value":1}},"06702a6d3a1c48728ed601df975178db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40c21eed8350480ab8964ab3f1b68a5e","placeholder":"​","style":"IPY_MODEL_1b964a38edd142a5a52cd9291d054402","value":" 895k/? [00:00&lt;00:00, 33.9MB/s]"}},"a3faa043c867455eb2792e9a7369cb06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb3dcc260d6a422b952448665a83933d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0b804bc140446d390a81a384a7564e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc128f4e1c9c4654913d8c5d501f4b76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"45221f6806aa4c4694c71adea03e63d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40c21eed8350480ab8964ab3f1b68a5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b964a38edd142a5a52cd9291d054402":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d5b61e9e9af4dcdb2d44e28258a432a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_510cf80d18d14ba5b0346b035fda2225","IPY_MODEL_6409023b3ebf4e1b95ff45c277715753","IPY_MODEL_ec4c464c47cc49d38baa1b9c7225be18"],"layout":"IPY_MODEL_fb631e6cc7a74570a0480add87610b3f"}},"510cf80d18d14ba5b0346b035fda2225":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f453fa7d47fb41ae937054e849a5a9b6","placeholder":"​","style":"IPY_MODEL_8a637a2373fd49ac9c9568b25eed18b8","value":"bpe.codes: "}},"6409023b3ebf4e1b95ff45c277715753":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0731d5cdb7a42269b8c5d6ab19bfbdd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fdc8138eaf7c4292a0b52811c5c5000b","value":1}},"ec4c464c47cc49d38baa1b9c7225be18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7e8370258854aa98f75d9721c5d32b6","placeholder":"​","style":"IPY_MODEL_41d5541f8406422695ea2f681d233672","value":" 1.14M/? [00:00&lt;00:00, 35.9MB/s]"}},"fb631e6cc7a74570a0480add87610b3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f453fa7d47fb41ae937054e849a5a9b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a637a2373fd49ac9c9568b25eed18b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0731d5cdb7a42269b8c5d6ab19bfbdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"fdc8138eaf7c4292a0b52811c5c5000b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7e8370258854aa98f75d9721c5d32b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41d5541f8406422695ea2f681d233672":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"687c73bfa045497da94a9b3d02607ad1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2aa3e374af94705a1d303c3d2e76519","IPY_MODEL_e59e49a274674b118b8044b96e6be168","IPY_MODEL_1ae1c95540644fa399572bddf32cb92d"],"layout":"IPY_MODEL_4e70e433616b424f8554ffe46027a2b0"}},"b2aa3e374af94705a1d303c3d2e76519":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f8488ed59ed45ac87e79d28cfb14475","placeholder":"​","style":"IPY_MODEL_c9bbc511b6834fe1b93d0df0f3c45066","value":"tokenizer.json: "}},"e59e49a274674b118b8044b96e6be168":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3aaaf65bb1404daca66434b08f57d899","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_566c151275cc4bc7886f250c5c035062","value":1}},"1ae1c95540644fa399572bddf32cb92d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_856a555207294c2db82aa4e57d38c84c","placeholder":"​","style":"IPY_MODEL_245110e4283c4f1e9545e0ef0ef055e2","value":" 3.13M/? [00:00&lt;00:00, 63.1MB/s]"}},"4e70e433616b424f8554ffe46027a2b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f8488ed59ed45ac87e79d28cfb14475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9bbc511b6834fe1b93d0df0f3c45066":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3aaaf65bb1404daca66434b08f57d899":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"566c151275cc4bc7886f250c5c035062":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"856a555207294c2db82aa4e57d38c84c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"245110e4283c4f1e9545e0ef0ef055e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"051d024e6aab4bf8b58ce5711e630245":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af31516bf13e4521b9f678160555f3c5","IPY_MODEL_af0b21258a5340eaae16bac497680705","IPY_MODEL_87eba20ef6db4d08bb116be979aad89a"],"layout":"IPY_MODEL_ce0ac42dfdae4127bd2e62cf1efb41f2"}},"af31516bf13e4521b9f678160555f3c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16c2fe3e5ae148bbac3d408f3e7a7c70","placeholder":"​","style":"IPY_MODEL_63307c3e9b7d4e6aac57e30ad6fb09b9","value":"pytorch_model.bin: 100%"}},"af0b21258a5340eaae16bac497680705":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_617ea10f951e40cdb360ea8dbc354337","max":542923308,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f71cffbdbf2b44749cf3ee12ce8d8ae5","value":542923308}},"87eba20ef6db4d08bb116be979aad89a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_324a9c24a779415db56dd6226ef8ff0c","placeholder":"​","style":"IPY_MODEL_038f84551b134c0b8417161d121f934f","value":" 543M/543M [00:06&lt;00:00, 138MB/s]"}},"ce0ac42dfdae4127bd2e62cf1efb41f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c2fe3e5ae148bbac3d408f3e7a7c70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63307c3e9b7d4e6aac57e30ad6fb09b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"617ea10f951e40cdb360ea8dbc354337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f71cffbdbf2b44749cf3ee12ce8d8ae5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"324a9c24a779415db56dd6226ef8ff0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"038f84551b134c0b8417161d121f934f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13825313,"sourceType":"datasetVersion","datasetId":8804510},{"sourceId":13924851,"sourceType":"datasetVersion","datasetId":8873482},{"sourceId":13993150,"sourceType":"datasetVersion","datasetId":8917936}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cài đặt môi trường","metadata":{"id":"SexHxwRgj4T3"}},{"cell_type":"code","source":"# 1 lệnh: nâng numpy lên 1.26.x và cài lại các package đã pin\n!pip install --upgrade --force-reinstall --no-cache-dir \\\n  numpy==1.26.2 \\\n  transformers==4.37.1 \\\n  huggingface_hub==0.23.0 \\\n  tokenizers==0.15.2 \\\n  accelerate==1.11.0 \\\n  sentence-transformers==2.2.2 \\\n  hnswlib==0.7.0 \\\n  underthesea==1.3.3 \\\n  pyarrow==11.0.0 \\\n  pandas==2.3.3\n\n# cài torch và triton/flash-attn\n!pip install torch --upgrade\n!pip install triton==2.0.0\n!pip install flash-attn --upgrade\n\n#Cài phụ thuộc cần thiết\n!pip install -q bitsandbytes accelerate safetensors transformers==4.41.2 --quiet\n\nimport accelerate\n!pip uninstall -y bitsandbytes\n!pip install -i https://pypi.org/simple/ bitsandbytes==0.42.0\n!pip install accelerate==0.30.1\n!pip install transformers==4.41.2 --upgrade\n\n# Gỡ tất cả các package xung đột\n!pip uninstall -y transformers accelerate bitsandbytes flash-attn triton\n\n# Cài lại phiên bản tương thích PhoGPT-4B\n!pip install --quiet transformers==4.41.2 accelerate==0.30.1 safetensors bitsandbytes==0.42.0\n!pip install --upgrade --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Cài xong -> restart session","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mek-8LfGj8cj","outputId":"e1c8cad5-1b52-4122-a8b8-b8b146433d5f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import importlib\nfor pkg in [\"numpy\",\"pyarrow\",\"tokenizers\",\"huggingface_hub\",\"transformers\",\"accelerate\",\"hnswlib\",\"underthesea\",\"sentence_transformers\",\"pandas\"]:\n    try:\n        m = importlib.import_module(pkg)\n        print(pkg, getattr(m, \"__version__\", \"unknown\"))\n    except Exception as e:\n        print(pkg, \"IMPORT ERROR:\", e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PhoGPT-4B","metadata":{"id":"ZSmf5Z4hfIR3"}},{"cell_type":"code","source":"# Cài Git LFS\n!apt-get install git-lfs -y\n!git lfs install\n\n# Clone lại repo\n!git clone https://huggingface.co/vinai/PhoGPT-4B\n%cd PhoGPT-4B\n\n# Tải đầy đủ weights\n!git lfs pull","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-7SIq-merf_d","outputId":"54c13ddb-201b-4ef9-d930-2ba1dabdf99c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pho_gpt4b_strict_cli_progress.py\n# =========================\n# PhoGPT-4B CLI chatbot - strict mode: JSON output + validation\n# + Progress bar with estimated response time for CLI\n# =========================\nimport os\nimport re\nimport gc\nimport json\nimport time\nimport threading\nimport unicodedata\nfrom html import unescape\nfrom difflib import SequenceMatcher\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# =========================\n# 0. Config\n# =========================\noffload_folder = \"/kaggle/working/llm_offload_phogpt\"\nos.makedirs(offload_folder, exist_ok=True)\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n\n# Estimated response time (seconds) used to drive progress bar UI.\n# Tweak this based on your machine/model to make ETA realistic.\nESTIMATED_RESPONSE_TIME = 10.0\n\n# =========================\n# 1. Load model + tokenizer\n# =========================\nrepo_path = \"/kaggle/working/PhoGPT-4B\"\n\nprint(\"Loading tokenizer and model... (có thể mất vài phút)\")\ntokenizer = AutoTokenizer.from_pretrained(repo_path, trust_remote_code=True)\n\n# ensure pad_token_id exists to avoid generation warnings/errors\nif tokenizer.pad_token_id is None:\n    try:\n        tokenizer.pad_token = tokenizer.eos_token\n    except Exception:\n        pass\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    repo_path,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    offload_folder=offload_folder,\n    offload_state_dict=True,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\ndevice = next(model.parameters()).device\nprint(f\"Model loaded on device: {device}\")\n\n# =========================\n# 2. References (DO NOT EDIT CONTENTS if you want strict behavior)\n# =========================\nREFERENCES = {\n    1: \"Ống tai rất gần nền sọ , nên khi bị viêm tại giữa có_thể gây viêm tai xương chũm , nguy_cơ gây viêm màng_não . Điều_trị viêm tai giữa nhiều khi mất rất nhiều thời_gian , nhiều trường_hợp điều_trị kéo_dài . Bệnh rất hay tái_phát . Nói_chung phải sử_dụng kháng_sinh liều cao , kéo_dài , kết_hợp với hút rửa tai hàng này . Khi có biến_chứng viêm não do viêm tai , thường có bệnh cảnh của viêm_não như sốt , đau_đầu , buồn_nôn , nôn , ... trường_hợp nặng có_thể ảnh_hưởng đến ý_thức như lơ_mơ , ngủ gà , thậm_chí hôn_mê . Nếu tình_trạng đau_đầu nhiều , kéo_dài , bạn phải đi khám , làm xét_nghiệm , chụp chiếu để chẩn_đoán xác_định ._Ngoài tình_trạng viêm tai giữa bạn đã điều_trị , biểu_hiện của bạn rất có_thể là tình_trạng viêm xoang mạn_tính . Với tình_trạng này , bạn nên đi khám để được chẩn_đoán xác_định và có biện_pháp điều_trị phù_hợp . Nếu chưa đi khám được , bạn có_thể mua lọ xịt nước muối_biển để rửa mũi họng hàng ngày . Có_thể mua các loại thuốc xịt điều_trị bệnh viêm xoang có chứa các thành_phần như xylometazoline , neomycin , dexamethason , có tác_dụng có mạch , giảm tình_trạng hắt_hơi , sổ_mũi , chảy nước_mũi ...\",\n    2: \"Khi bị cảm cúm , gây phù_nề , xung_huyết niêm_mạc vùng mũi họng , tắc mũi , ảnh_hưởng đến cả ống thông giữa xoang với tai . Vì_vậy mà cảm cúm_thường là ngửi kém , rối_loạn vị_giác , ù_tai , nghe kém . Nhiều người cảm_thấy đâu đầu , đau khắp người , đau_nhức cơ_bắp ( đây là hội_chứng nhiễm vi rut ) . Để cải_thiện các triệu_chứng này , khi hết các triệu_chứng cảm cúm , có_thể thự chiện các biện_pháp : - Mua nước muối_biển xịt , về xịt rửa mũi họng hàng ngày - Tăng_cường vận_động , tránh nằm nhiều , có_thể xông hơi , chườm nóng - Sử_dụng các sản_phẩm chứa vitamin nhóm B , Ginkgo_biloba , Blueberry có trong sản_phẩm Vindermen_Plus . Sản_phẩm có tác_dụng giam đau , giảm tê bì , tăng tuần_hoàn , chống não hóa . Chúc bạn mạnh_khỏe ! Nếu còn thắc_mắc nào cần giải_đáp vui_lòng gọi 19001259 !\",\n    3: \"Nghẹt_mũi kéo_dài ít khi là do nguyên_nhân cấp_tính như cảm_lạnh , nhiễm virus thông_thường , đó thường là biểu_hiện của một nguyên_nhân tồn_tại lâu_dài chưa được xử_trí như : - Viêm_nhiễm mạn_tính của đường hô_hấp trên : viêm xoang mạn_tính , viêm mũi dị_ứng , … - Khối_u , polyp nhỏ trong mũi , xoang làm cản_trở đường lưu_thông của dịch mũi . - Cấu_trúc bất_thường vùng mũi , xoang : vẹo vách ngăn mũi , … - Rối_loạn cảm_giác : khiến cho người_bệnh luôn thấy nghẹt_mũi dù thực_tế không có sự tắc_nghẽn đường thở . - Rối_loạn nội_tiết : thường gặp ở phụ_nữ mang thai . - Tiếp_xúc thường_xuyên , liên_tục với các tác_nhân : khói bụi , hóa_chất , khói thuốc_lá , … cũng có_thể khiến bạn nghẹt_mũi kéo_dài . Để khắc_phục nhanh tình_trạng nghẹt_mũi , em nên rửa mũi ngày 2 lần bằng nước muối 0,9 % , sau đó xịt mũi khoảng 3-4 lần bằng sản_phẩm chứa 3 thành_phần gồm Xylomethazolin ( giúp co mạch , hết ngạt_mũi ) , Dexathethazone ( giúp chống_viêm , hết ngạt_mũi , sổ_mũi ) và Neomycin_sulfat ( kháng_sinh có tác_dụng diệt khuẩn tại_chỗ ) như thuốc Hadocort D cho hiệu_quả khá cao trong điều_trị các triệu_chứng của bệnh viêm xoang khi kết_hợp với các thuốc điều_trị khác . Để điều_trị khỏi bệnh , cần phải xác_định được nguyên_nhân gây bệnh . Nếu do viêm_nhiễm , cần phải dùng thuốc kháng_sinh . Tốt nhất , em nên đi khám bác_sĩ tai_mũi họng để phát_hiện chính_xác và điều_trị triệt_để nguyên_nhân gây bệnh . Đồng_thời , thực_hiện các lưu_ý sau : - Đeo khẩu_trang trước khi ra đường hoặc làm công_việc gặp nhiều bụi , giữ môi_trường xung_quanh luôn sạch_sẽ , tránh xa khói bụi , chất_thải , khói thuốc_lá ... - Tránh hít luồng không_khí lạnh , khô : Không nên để mũi đối_diện trực_tiếp với luồng gió của máy_lạnh hoặc máy quạt khi nằm ngủ , hoặc khi ngồi làm_việc . Cần giữ ấm khi đi ngoài trời lạnh , trời mưa , đặc_biệt với những_ai phải làm_việc quá khuya hoặc dậy quá sớm . - Khi tắm hoặc đi bơi , nếu bị nước vào tai hoặc mũi cần biết cách để cho nước ra ngoài . - Vệ_sinh mũi thường_xuyên với dung_dịch nước muối_biển . - Tránh stress : Khi làm_việc quá_sức , lo_lắng nhiều , hệ miễn_dịch của cơ_thể suy_yếu rất dễ bị nhiễm_khuẩn , trong đó mũi xoang dễ bị nhiễm nhất vì là cơ_quan lọc không_khí trước khi đưa vào cơ_thể . Chúc em chóng khỏi bệnh ! Nếu còn thắc_mắc nào cần giải_đáp vui_lòng gọi 19001259 !\",\n    4: \"Theo như bạn mô_tả , khả_năng bạn bị trào ngược dạ_dày thực_quản . Do chế_độ ăn và sinh_hoạt không phù_hợp , lại do thể_trạng béo nữa , khiến cho dịch dạ_dày trào ngược lên thực_quản , có_thể tràn vào đường hô_hấp gây khó thở , nhiều trường_hợp trào ngược có_thể là khởi_phát của cơn hen phế_quản . Để khắc_phục tình_trạng này , bạn nên thực_hiện các biện_pháp sau : - Không ăn chua , cay , các chất kích_thích , như bưởi , chanh , cam , rượu , chè , cà_phê , bỏ hút thuốc , ... - Không ăn thức_ăn cứng - Không ăn muộn , nhất_là ăn xong đi ngủ luôn , ăn xong tối_thiểu 2 - 3 tiếng mới nên đi ngủ , khi ngủ kê gối cao từ vai lên đầu , tạo tư_thế ngủ với đầu và vai cao hơn . - Bạn áp_dụng trong vài tháng , nếu không kết_quả thì phải đi khám nội_soi dạ_dày , nội_soi tai_mũi họng để chấn đoán xác_định . Khi đã chẩn_đoán xác_định , thì có_thể phải uống thuốc theo chỉ_định của bác_sĩ chuyên_khoa ._Chúc bạn mạnh_khỏe ! Nếu còn thắc_mắc nào cần giải_đáp vui_lòng gọi 19001259 !\",\n    5: \"theo như cháu mô_tả , cháu bị đau có_thể do vận_động nhiều , do đi giày chật khiến máu kém lưu_thông gây đau . Ngoài_ra nguyên_nhân gây đau có_thể do bệnh_lý như viêm cân gan chân hoặc gai gót chân . Tốt nhất cháu nên đi khám tại các bệnh_viện chuyên_khoa xương khớp để xác_định nguyên_nhân và có hướng điều_trị phù_hợp . Đồng_thời để giảm những cơn đau chân , cháu nên dùng sản_phẩm Vindermen_Plus ngày 2 viên chia 2 lần , duy_trì 3 - 6 tháng , giúp tăng sự dẫn_truyền thần_kinh , phục_hồi hư tổn tại sụn khớp , giúp giảm đau nhức hiệu_quả . Kết_hợp với sản_phẩm Vipteen ngày 4 viên chia 2 lần cung_cấp các thành_phần : MK7 , Canxi nano , Zn nano , Magie , Vitamin_D3 , ... có tác_dụng bổ_sung vi_chất cần_thiết cho sự phát_triển của khung xương , giúp cháu có được chiều cao tối_ưu , và giúp xương chắc khỏe , ngăn_ngừa các bệnh về xương khớp . Bên_cạnh đó trong thời_gian này cháu nên nghỉ_ngơi . Tránh đứng ngồi ở một tư_thế quá lâu , hoặc vận_động , chạy_nhảy nhiều , đi giầy đúng kích_cỡ chân . Chườm đá 20 phút từ 3-4 lần mỗi ngày để giảm các cơn đau gót chân . Chúc cháu sức_khỏe !\"\n}\n\nREFERENCES_TEXT = \"\\n\".join([f\"[{k}] {v}\" for k, v in REFERENCES.items()])\n\n# =========================\n# 3. Utilities: cleaning, dedupe, similarity\n# =========================\ndef _clean_text(t: str) -> str:\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\ndef dedupe_consecutive_sentences(text: str) -> str:\n    pieces = re.split(r'(?<=[.!?])\\s+', text.strip())\n    new_pieces = []\n    for s in pieces:\n        if not s:\n            continue\n        if not new_pieces:\n            new_pieces.append(s)\n        else:\n            if s.strip().lower() != new_pieces[-1].strip().lower():\n                new_pieces.append(s)\n    return \" \".join(new_pieces).strip()\n\ndef remove_self_claims(text: str) -> str:\n    return re.sub(r'(?i)\\b(mình|tôi)\\b', 'trợ lý', text)\n\ndef _is_substring_or_similar(advice: str, ref_text: str, threshold=0.62) -> bool:\n    a = _clean_text(advice).lower()\n    r = _clean_text(ref_text).lower()\n    if not a or not r:\n        return False\n    if a in r:\n        return True\n    awords = [w for w in re.findall(r'\\w+', a) if len(w) >= 3]\n    rwords = set([w for w in re.findall(r'\\w+', r) if len(w) >= 3])\n    if not awords or not rwords:\n        return False\n    common = sum(1 for w in awords if w in rwords)\n    frac = common / len(awords)\n    if frac >= threshold:\n        return True\n    sm = SequenceMatcher(None, a, r).quick_ratio()\n    return sm >= 0.70\n\n# =========================\n# 4. System prefix (short + strict)\n# =========================\nSYSTEM_PREFIX = (\n    \"Bạn là trợ lý y tế AI. TUYỆT ĐỐI chỉ trả lời DỰA TRÊN các nội dung tham khảo sau (KHÔNG sáng tạo thêm):\\n\"\n    f\"{REFERENCES_TEXT}\\n\\n\"\n    \"Khi trả lời, PHẢI TRẢ VỀ DUY NHẤT MỘT JSON HỢP LỆ (KHÔNG VĂN BẢN KHÁC):\\n\"\n    '{\"specialty\":\"<Chuyên khoa>\", \"advice\":\"<Lời khuyên - phải chỉ dùng các câu/chữ có trong tham khảo [n]>\", \"reference\":\"[n]\"}\\n\\n'\n    \"Quy tắc bắt buộc:\\n\"\n    \"- JSON phải có đủ 3 trường: specialty, advice, reference (reference dạng [n] với n từ 1 đến 5).\\n\"\n    \"- advice PHẢI được rút từ nội dung tương ứng của tham khảo [n] (không tạo thêm dữ liệu).\\n\"\n    \"- Nếu câu hỏi KHÔNG liên quan đến [1-5] hoặc không thể tạo JSON hợp lệ theo quy tắc trên, \"\n    \"trả về duy nhất chuỗi:\\n\"\n    '\"Không đủ thông tin từ các tài liệu tham khảo để tư vấn; nên khám bác sĩ chuyên khoa.\"\\n\\n'\n    \"Trả lời ngắn gọn, khách quan.\"\n)\n\n# =========================\n# 5. Expected specialty map (for validation)\n# =========================\nREFERENCE_SPECIALTY = {\n    1: \"Tai–Mũi–Họng\",     # viêm tai giữa, viêm xoang mạn\n    2: \"Tai–Mũi–Họng\",     # ngạt mũi, cảm cúm, rối loạn vị giác\n    3: \"Tai–Mũi–Họng\",     # nghẹt mũi kéo dài, viêm xoang, polyp\n    4: \"Tiêu hóa / Tai–Mũi–Họng\",  # khó thở do trào ngược, có liên quan hô hấp\n    5: \"Cơ xương khớp\"      # đau chân khi chạy, viêm cân gan chân\n}\n\n# Strict fallback text\nFALLBACK = \"Không đủ thông tin từ các tài liệu tham khảo để tư vấn; nên khám bác sĩ chuyên khoa.\"\n\n# -------------------------\n# Pre-match deterministic (fast & reliable) - IMPROVED\n# -------------------------\ndef _normalize_vn(s: str) -> str:\n    s = s.lower().strip()\n    s = unicodedata.normalize(\"NFD\", s)\n    s = \"\".join(ch for ch in s if unicodedata.category(ch) != \"Mn\")\n    s = re.sub(r'\\s+', ' ', s)\n    return s\n\ndef _fuzzy_match(a: str, b: str, thresh=0.72) -> bool:\n    return SequenceMatcher(None, a, b).ratio() >= thresh\n\ndef _select_advice_sentence_from_ref(ref_text: str) -> str:\n    \"\"\"\n    Chọn câu phù hợp nhất từ ref_text, ưu tiên câu có hành động (nên, rửa, nhỏ, nghỉ, bổ sung, chườm, đổi).\n    Nếu không tìm thấy, trả về câu đầu tiên.\n    \"\"\"\n    # split into sentences (keep Vietnamese punctuation . ? !)\n    sentences = re.split(r'(?<=[.!?])\\s+', ref_text.strip())\n    action_keywords = ['nên', 'rửa', 'nhỏ', 'nghỉ', 'bổ sung', \n                   'chườm', 'đổi', 'khám', 'dùng', 'xịt', \n                   'đeo', 'tránh', 'uống', 'kê gối cao', 'nội soi', \n                   'tăng_cường', 'xông hơi', 'hút rửa tai', 'giữ ấm']\n    for s in sentences:\n        sl = s.lower()\n        for kw in action_keywords:\n            if kw in sl:\n                # clean trailing punctuation\n                return s.strip().rstrip('.').strip()\n    # fallback: first sentence (without trailing dot)\n    if sentences:\n        return sentences[0].strip().rstrip('.').strip()\n    return ref_text.strip().rstrip('.').strip()\n\ndef match_reference_and_build_answer(user_text: str):\n    \"\"\"\n    Nếu tìm thấy từ khóa gợi ý reference rõ ràng trong user_text,\n    trả về human_answer trực tiếp mà KHÔNG gọi model.\n    Trả về None nếu không tìm match rõ ràng.\n    \"\"\"\n    txt_norm = _normalize_vn(user_text)\n\n    # map some keywords -> ref index (mở rộng khi cần)\n    keyword_map = {\n        1: [\"viem tai giua\", \"tai giua u dich\", \"tai u dich\", \"viem xoang\", \"dau dau\", \"nghe kem\", \"lung bung\", \"co hong vuong\", \"dau sau gay\"],  \n        2: [\"ngat mui\", \"ngat mui mot ben\", \"viem mui\", \"viem hong\", \"co sung\", \"amidan\", \"roi loan cam giac\", \"viem xoang\"],  \n        3: [\"nghet mui\", \"viem xoang man tinh\", \"polyp mui\", \"dau mat\", \"mat ngu\", \"met moi\"],  \n        4: [\"kho tho\", \"trao nguoc da day\", \"bung day\", \"an mau no\", \"hoi tho khong sau\"],  \n        5: [\"dau chan khi chay\", \"viem can gan chan\", \"dau chan\", \"gai got chan\", \"te chan\", \"dau khop chan\"]\n    }\n\n\n    # exact substring match first (fast)\n    for ref_idx, keys in keyword_map.items():\n        for kw in keys:\n            if kw in txt_norm:\n                ref_text = REFERENCES[ref_idx]\n                advice_candidate = _select_advice_sentence_from_ref(ref_text)\n                specialty = REFERENCE_SPECIALTY.get(ref_idx, \"Khám chuyên khoa\")\n                # Build more instructive human_answer:\n                # 1) recommendation to visit specialty\n                # 2) specific advice to reduce/handle symptoms (from reference)\n                human_answer = f\"Bạn nên đến thăm khám chuyên khoa {specialty}. {advice_candidate} (tham khảo: [{ref_idx}]).\"\n                human_answer = dedupe_consecutive_sentences(remove_self_claims(human_answer))\n                return human_answer\n\n    # fuzzy match: compare normalized user_text with keywords\n    for ref_idx, keys in keyword_map.items():\n        for kw in keys:\n            if _fuzzy_match(txt_norm, kw, thresh=0.70) or _fuzzy_match(kw, txt_norm, thresh=0.70):\n                ref_text = REFERENCES[ref_idx]\n                advice_candidate = _select_advice_sentence_from_ref(ref_text)\n                specialty = REFERENCE_SPECIALTY.get(ref_idx, \"Khám chuyên khoa\")\n                human_answer = f\"Bạn nên đến thăm khám chuyên khoa {specialty}. {advice_candidate} (tham khảo: [{ref_idx}]).\"\n                human_answer = dedupe_consecutive_sentences(remove_self_claims(human_answer))\n                return human_answer\n\n    return None\n\n\n# =========================\n# 6. Generation + validation function\n# =========================\ndef generate_answer(user_text: str, max_new_tokens: int = 160) -> str:\n    # termination\n    if user_text.strip().lower() == \"kết thúc\":\n        return \"Cảm ơn bạn — phiên làm việc đã kết thúc. Chào bạn!\"\n\n    # JSON instruction appended to prompt\n    json_instruction = (\n        \"TRẢ VỀ DUY NHẤT MỘT JSON HỢP LỆ (KHÔNG VĂN BẢN KHÁC): \"\n        '{\"specialty\":\"<Chuyên khoa>\", \"advice\":\"<Lời khuyên - phải chỉ dùng các câu/chữ có trong tham khảo [n]>\", \"reference\":\"[n]\"}\\n\\n'\n        f'Nếu không thể, trả về duy nhất chuỗi: \"{FALLBACK}\"'\n    )\n\n    prompt = SYSTEM_PREFIX + \"\\n\\n\" + f\"Người bệnh mô tả: \\\"{user_text}\\\"\\n\\n\" + json_instruction\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    gen_kwargs = dict(\n        **inputs,\n        max_new_tokens=140,      # giảm nếu cần\n        do_sample=True,          # bật sampling nhẹ (thường nhanh hơn beam)\n        temperature=0.28,\n        top_p=0.92,\n        num_beams=1,             # ensure beams=1\n        repetition_penalty=1.15, # vẫn giữ để giảm lặp\n        no_repeat_ngram_size=3,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    with torch.no_grad():\n        output_ids = model.generate(**gen_kwargs)\n\n    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n\n    # Try extract JSON from output\n    try:\n        json_start = output_text.find('{')\n        json_end = output_text.rfind('}')\n        if json_start != -1 and json_end != -1 and json_end > json_start:\n            json_candidate = output_text[json_start:json_end+1]\n        else:\n            json_candidate = output_text\n\n        parsed = json.loads(json_candidate)\n\n        # Validate keys presence\n        if not all(k in parsed for k in (\"specialty\", \"advice\", \"reference\")):\n            return FALLBACK\n\n        specialty = _clean_text(parsed[\"specialty\"])\n        advice = _clean_text(parsed[\"advice\"])\n        reference_raw = _clean_text(parsed[\"reference\"])\n\n        # Validate reference format\n        m = re.match(r'^\\[([1-5])\\]$', reference_raw)\n        if not m:\n            return FALLBACK\n        ref_idx = int(m.group(1))\n\n        # Validate specialty roughly matches expected specialty\n        expected_spec = REFERENCE_SPECIALTY.get(ref_idx, \"\").lower()\n        if expected_spec:\n            spec_tokens = set(re.findall(r'\\w+', specialty.lower()))\n            exp_tokens = set(re.findall(r'\\w+', expected_spec))\n            if len(exp_tokens & spec_tokens) / max(1, len(exp_tokens)) < 0.4:\n                return FALLBACK\n\n        # Validate advice derived from reference\n        ref_text = REFERENCES[ref_idx]\n        if not _is_substring_or_similar(advice, ref_text, threshold=0.60):\n            return FALLBACK\n\n        # Build readable answer: combine 3 required parts\n        human_answer = f\"{specialty}. {advice} (tham khảo: [{ref_idx}]).\"\n        human_answer = dedupe_consecutive_sentences(remove_self_claims(human_answer))\n        return human_answer\n\n    except Exception:\n        return FALLBACK\n\n# =========================\n# 7. CLI loop + Progress Bar\n# =========================\ndef progress_bar_running(stop_event, est_seconds=ESTIMATED_RESPONSE_TIME):\n    \"\"\"\n    Show a CLI progress bar that advances based on estimated time.\n    If model finishes earlier, set stop_event to end and the bar will jump to 100%.\n    \"\"\"\n    bar_length = 30\n    start = time.perf_counter()\n    while not stop_event.is_set():\n        elapsed = time.perf_counter() - start\n        frac = min(elapsed / est_seconds, 0.99)  # keep <100% until done\n        filled = int(round(bar_length * frac))\n        bar = \"█\" * filled + \"-\" * (bar_length - filled)\n        eta = max(0.0, est_seconds - elapsed)\n        print(f\"\\rĐang xử lý: |{bar}| {int(frac*100):3d}%  ETA: {eta:4.1f}s\", end=\"\", flush=True)\n        time.sleep(0.12)\n    # On finish, show 100% and short pause to make it visible\n    bar = \"█\" * bar_length\n    print(f\"\\rĐang xử lý: |{bar}| 100%  ETA:   0.0s\")\n    # small pause so user sees 100% before printing result\n    time.sleep(0.08)\n    # clear line for neat output (optional)\n    print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n\nprint(\"=== Chatbot PhoGPT-4B (strict, JSON-validated) đã sẵn sàng ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\nChatbot: Tạm biệt!\")\n        break\n\n    if user_input is None:\n        continue\n\n    if user_input.strip().lower() == \"kết thúc\":\n        print(\"Chatbot: Tạm biệt!\")\n        break\n\n    # Try deterministic pre-match first\n    quick_reply = match_reference_and_build_answer(user_input)\n    if quick_reply is not None:\n        # we skip model; show immediate 100% progress bar briefly for UX\n        stop_event = threading.Event()\n        progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 0.4), daemon=True)\n        progress_thread.start()\n        time.sleep(0.15)  # tiny pause so user sees progress\n        stop_event.set()\n        progress_thread.join()\n        print(\"Chatbot:\", quick_reply, \"\\n\")\n        continue\n\n    # Otherwise call model as before\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, ESTIMATED_RESPONSE_TIME), daemon=True)\n    progress_thread.start()\n    try:\n        reply = generate_answer(user_input)\n    finally:\n        stop_event.set()\n        progress_thread.join()\n\n    print(\"Chatbot:\", reply, \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FULL PIPELINE","metadata":{}},{"cell_type":"markdown","source":"## pho_retrieval_gpt4b_combined_progress_final_textoutput.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_gpt4b_combined_progress_final_textoutput.py\n# ====================================================\n# Giữ nguyên pipeline nhưng trả về PLAIN TEXT (không JSON).\n# Bố cục trả lời luôn phải chứa 3 phần:\n#   Chuyên khoa: ...\n#   Lời khuyên: ...\n#   Tham khảo: [n]\n# ====================================================\n\nimport os\nimport re\nimport time\nimport threading\nimport json\nfrom html import unescape\nfrom difflib import SequenceMatcher\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV & tiền xử lý\n# =========================\ncsv_path = \"/kaggle/input/bacsituvan/bacsituvan.csv\"  # chỉnh nếu cần\ndf = pd.read_csv(csv_path)\nprint(\"Số bản ghi:\", len(df))\n\ndef preprocess_text(s):\n    if not isinstance(s, str):\n        return \"\"\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\nfor col in [\"question\", \"answer\", \"department\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Load PhoBERT embedding\n# =========================\nMODEL_NAME = \"vinai/phobert-base\"\ntokenizer_phobert = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    if not text:\n        return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        last_hidden = out.last_hidden_state\n        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n        masked = last_hidden * attention_mask\n        summed = masked.sum(dim=1)\n        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n        mean_pooled = (summed / counts).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 3. Build embeddings + hnswlib index\n# =========================\ntexts = df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")\ntexts = texts.tolist()\n\ndef progress_bar_running(stop_event, est_seconds=10.0, prefix=\"Đang xử lý\"):\n    bar_length = 30\n    start = time.perf_counter()\n    while not stop_event.is_set():\n        elapsed = time.perf_counter() - start\n        frac = min(elapsed / est_seconds, 0.99)\n        filled = int(round(bar_length * frac))\n        bar = \"█\" * filled + \"-\" * (bar_length - filled)\n        eta = max(0.0, est_seconds - elapsed)\n        print(f\"\\r{prefix}: |{bar}| {int(frac*100):3d}%  ETA: {eta:4.1f}s\", end=\"\", flush=True)\n        time.sleep(0.12)\n    bar = \"█\" * bar_length\n    print(f\"\\r{prefix}: |{bar}| 100%  ETA:   0.0s\")\n    time.sleep(0.06)\n    print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n\nprint(\"Preparing embeddings for\", len(texts), \"documents...\")\nembeddings = []\nstop_event = threading.Event()\nprogress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, max(5, len(texts)/20), \"Embedding Top-K\"), daemon=True)\nprogress_thread.start()\nfor t in texts:\n    embeddings.append(sentence_embedding(t))\nstop_event.set()\nprogress_thread.join()\n\nembeddings = np.vstack(embeddings).astype(\"float32\")\ndim = embeddings.shape[1]\nnum_elements = embeddings.shape[0]\n\nindex = hnswlib.Index(space=\"cosine\", dim=dim)\nindex.init_index(max_elements=num_elements, ef_construction=200, M=16)\nindex.add_items(embeddings, np.arange(num_elements))\nindex.set_ef(50)\nprint(\"hnswlib index built. Num elements:\", index.get_current_count())\n\n# =========================\n# 4. Top-K retrieval\n# =========================\ndef preprocess_query(s: str) -> str:\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\ndef retrieve_topk(query_text: str, k: int = 5):\n    q = preprocess_query(query_text)\n    q_emb = sentence_embedding(q).astype(\"float32\").reshape(1, -1)\n    labels, distances = index.knn_query(q_emb, k=k)\n    results = []\n    for dist, idx in zip(distances[0], labels[0]):\n        if idx < 0:\n            continue\n        score = float(1.0 - dist)\n        results.append({\"score\": score, \"index\": int(idx), \"text\": texts[idx], \"row\": df.iloc[idx].to_dict()})\n    return results\n\n# =========================\n# 5. Tiện ích trích xuất câu trả lời (deterministic -> trả plain text)\n# =========================\ndef _clean_text(t: str) -> str:\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\ndef _select_advice_sentence_from_ref(ref_text: str):\n    if not ref_text:\n        return \"\"\n    sentences = re.split(r'(?<=[.!?])\\s+', ref_text.strip())\n    action_keywords = ['nên', 'rửa', 'nhỏ', 'nghỉ', 'bổ sung', 'chườm', 'đổi', 'khám', 'dùng', 'xịt', 'đeo', 'tránh', 'uống', 'đi khám']\n    for s in sentences:\n        sl = s.lower()\n        for kw in action_keywords:\n            if kw in sl:\n                return s.strip().rstrip('.').strip()\n    if sentences:\n        sentences_sorted = sorted(sentences, key=lambda x: len(x), reverse=True)\n        return sentences_sorted[0].strip().rstrip('.').strip()\n    return _clean_text(ref_text)\n\ndef _similar(a: str, b: str) -> float:\n    return SequenceMatcher(None, a, b).ratio()\n\ndef deterministic_answer_from_refs_text(user_text: str, refs: dict, ref_specialty: dict, topk_rows: List[dict], sim_thresh=0.65):\n    \"\"\"\n    Cố gắng xác định reference phù hợp và trả về PLAIN TEXT gồm 3 phần:\n      Chuyên khoa: ...\n      Lời khuyên: ...\n      Tham khảo: [n]\n    Nếu không tìm được -> trả về (None, None)\n    \"\"\"\n    user_norm = _clean_text(user_text).lower()\n    best_idx = None\n    best_score = 0.0\n    for i, r in enumerate(topk_rows, start=1):\n        q = r[\"row\"].get(\"question\", \"\") or \"\"\n        q = _clean_text(q).lower()\n        s1 = _similar(user_norm, q) if q else 0.0\n        a = r[\"row\"].get(\"answer\", \"\") or \"\"\n        a = _clean_text(a).lower()\n        s2 = _similar(user_norm, a) if a else 0.0\n        score = max(s1, s2)\n        if score > best_score:\n            best_score = score\n            best_idx = i\n    if best_score >= sim_thresh and best_idx is not None:\n        ref_text = refs.get(best_idx, \"\")\n        advice = _select_advice_sentence_from_ref(ref_text)\n        specialty = ref_specialty.get(best_idx, \"Khám chuyên khoa\")\n        out_text = f\"Chuyên khoa: {specialty}\\nLời khuyên: {advice}\\nTham khảo: [{best_idx}]\"\n        return out_text, best_idx\n    return None, None\n\n# =========================\n# 6. PhoGPT-4B lazy load + gọi với yêu cầu trả PLAIN TEXT 3 phần\n# =========================\nrepo_path = \"/kaggle/working/PhoGPT-4B\"  # chỉnh nếu cần\ntokenizer_gpt = None\nmodel_gpt = None\ndevice_gpt = None\n\ndef ensure_gpt_loaded():\n    global tokenizer_gpt, model_gpt, device_gpt\n    if tokenizer_gpt is None or model_gpt is None:\n        tokenizer_gpt = AutoTokenizer.from_pretrained(repo_path, trust_remote_code=True)\n        if tokenizer_gpt.pad_token_id is None:\n            tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n            tokenizer_gpt.pad_token_id = tokenizer_gpt.eos_token_id\n        model_gpt = AutoModelForCausalLM.from_pretrained(\n            repo_path,\n            trust_remote_code=True,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True\n        )\n        device_gpt = next(model_gpt.parameters()).device\n        print(f\"PhoGPT-4B loaded on device: {device_gpt}\")\n\ndef call_gpt_short_prompt_text(refs_text: str, user_text: str, max_new_tokens=140):\n    \"\"\"\n    Gọi model và yêu cầu TRẢ VỀ PLAIN TEXT gồm 3 phần (Chuyên khoa / Lời khuyên / Tham khảo).\n    Không yêu cầu JSON, không lặp prompt.\n    \"\"\"\n    ensure_gpt_loaded()\n    system_block = (\n        \"Bạn là trợ lý y tế AI. CHỈ DỰA TRÊN CÁC THAM KHẢO LIỆT KÊ DƯỚI.\\n\"\n        \"KHÔNG LẶP LẠI PROMPT NÀY.\\n\"\n        \"KHÔNG THÊM THÔNG TIN NGOÀI NHỮNG THÔNG TIN ĐƯỢC GHI TRONG ĐOẠN THAM KHẢO.\\n\"\n        \"TRẢ VỀ DUY NHẤT PLAIN TEXT NGẮN GỌN VÀ RÕ RÀNG GỒM 3 PHẦN (Không kèm giải thích dài):\\n\"\n        \"Chuyên khoa: <tên chuyên khoa>\\n\"\n        \"Lời khuyên: <một câu hoặc vài cụm từ lấy đúng từ tham khảo>\\n\"\n        \"Tham khảo: [n]\\n\"\n        \"Nếu không đủ dữ liệu, trả: \\\"Không đủ thông tin từ các tài liệu tham khảo để tư vấn; nên khám bác sĩ chuyên khoa.\\\"\"\n    )\n    prompt = system_block + \"\\n\\n\" + refs_text + \"\\n\\n\" + f\"Người bệnh mô tả: \\\"{user_text}\\\"\\n\\nTrả lời: \"\n    inputs = tokenizer_gpt(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    inputs = {k: v.to(device_gpt) for k, v in inputs.items()}\n    with torch.no_grad():\n        out_ids = model_gpt.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.2,\n            top_p=0.9,\n            num_beams=1,\n            repetition_penalty=1.1,\n            no_repeat_ngram_size=3,\n            eos_token_id=tokenizer_gpt.eos_token_id,\n            pad_token_id=tokenizer_gpt.pad_token_id\n        )\n    output_text = tokenizer_gpt.decode(out_ids[0], skip_special_tokens=True).strip()\n    return output_text\n\n# =========================\n# 7. Vòng lặp chính\n# =========================\nprint(\"=== Chatbot (text output) đang chạy ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\nChatbot: Tạm biệt!\")\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        print(\"Chatbot: Tạm biệt!\")\n        break\n\n    # 1) Top-K\n    k = 1\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 2.5, \"Retrieving Top-K\"), daemon=True)\n    progress_thread.start()\n    topk_results = retrieve_topk(user_input, k=k)\n    stop_event.set()\n    progress_thread.join()\n\n    # 2) Cập nhật REFERENCES / keyword_map / action_keywords\n    REFERENCES = {}\n    REFERENCE_SPECIALTY = {}\n    keyword_map = {}\n    action_keywords = set()\n\n    for i, r in enumerate(topk_results, start=1):\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        m = re.search(r'@[^:]{0,40}:\\s*(.*)', raw_answer, flags=re.DOTALL)\n        answer_clean = m.group(1).strip() if m else raw_answer.strip()\n        REFERENCES[i] = answer_clean\n        REFERENCE_SPECIALTY[i] = r[\"row\"].get(\"department\", \"Khám chuyên khoa\")\n        q_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        concat = (q_text + \" \" + raw_answer).lower()\n        tokens = re.findall(r'\\w+', concat)\n        kw = [t for t in tokens if len(t) >= 4 and not t.isdigit()]\n        seen = []\n        for t in kw:\n            if t not in seen:\n                seen.append(t)\n            if len(seen) >= 12:\n                break\n        keyword_map[i] = seen\n        for act in [\"uống\", \"rửa\", \"xịt\", \"nhỏ\", \"đi khám\", \"khám\", \"chườm\", \"bổ sung\", \"đeo\", \"tránh\", \"dùng\"]:\n            if act in concat:\n                action_keywords.add(act)\n\n    # Debug prints\n    print(\"\\n--- Top-K ---\")\n    for i, r in enumerate(topk_results, start=1):\n        print(f\"[{i}] score={r['score']:.4f} | question: {r['row'].get('question','')[:120]}\")\n    print(\"\\nREFERENCES:\")\n    for kidx, v in REFERENCES.items():\n        print(f\"[{kidx}] {v[:220]}{'...' if len(v)>220 else ''}\")\n    print(\"\\nkeyword_map:\", keyword_map)\n    print(\"action_keywords:\", sorted(list(action_keywords)))\n    print(\"------------------\\n\")\n\n    # 3) Deterministic first -> trả plain text\n    deterministic_text, matched_ref = deterministic_answer_from_refs_text(user_input, REFERENCES, REFERENCE_SPECIALTY, topk_results, sim_thresh=0.62)\n    if deterministic_text is not None:\n        print(\"Chatbot:\\n\" + deterministic_text + \"\\n\")\n        continue\n\n    # 4) Gọi model (nếu cần)\n    refs_text = \"\\n\".join([f\"[{k}] {v}\" for k, v in REFERENCES.items()])\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 8.0, \"Generating Answer\"), daemon=True)\n    progress_thread.start()\n    try:\n        output_text = call_gpt_short_prompt_text(refs_text, user_input, max_new_tokens=180)\n    except Exception as e:\n        output_text = f\"ERROR_CALLING_MODEL: {e}\"\n    finally:\n        stop_event.set()\n        progress_thread.join()\n\n    # 5) In trực tiếp plain text trả về từ model\n    # Nếu model trả lời dài/không đúng form, người vận hành có thể điều chỉnh prompt hệ thống.\n    print(\"Chatbot:\\n\" + output_text + \"\\n\")","metadata":{"trusted":true,"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## pho_retrieval_gpt4b_combined_progress_final_textoutput_v2.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_gpt4b_combined_progress_final_textoutput.py\n# ====================================================\n# Giữ nguyên pipeline nhưng trả về PLAIN TEXT (không JSON).\n# Bố cục trả lời luôn phải chứa 3 phần:\n#   Chuyên khoa: ...\n#   Lời khuyên: ...\n#   Tham khảo: [n]\n# ====================================================\n\nimport os\nimport re\nimport time\nimport threading\nimport json\nfrom html import unescape\nfrom difflib import SequenceMatcher\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV & tiền xử lý\n# =========================\ncsv_path = \"/kaggle/input/bacsituvan/bacsituvan.csv\"  # chỉnh nếu cần\ndf = pd.read_csv(csv_path)\nprint(\"Số bản ghi:\", len(df))\n\ndef preprocess_text(s):\n    if not isinstance(s, str):\n        return \"\"\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\nfor col in [\"question\", \"answer\", \"department\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Load PhoBERT embedding\n# =========================\nMODEL_NAME = \"vinai/phobert-base\"\ntokenizer_phobert = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    if not text:\n        return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        last_hidden = out.last_hidden_state\n        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n        masked = last_hidden * attention_mask\n        summed = masked.sum(dim=1)\n        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n        mean_pooled = (summed / counts).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 3. Build embeddings + hnswlib index\n# =========================\ntexts = df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")\ntexts = texts.tolist()\n\ndef progress_bar_running(stop_event, est_seconds=10.0, prefix=\"Đang xử lý\"):\n    bar_length = 30\n    start = time.perf_counter()\n    while not stop_event.is_set():\n        elapsed = time.perf_counter() - start\n        frac = min(elapsed / est_seconds, 0.99)\n        filled = int(round(bar_length * frac))\n        bar = \"█\" * filled + \"-\" * (bar_length - filled)\n        eta = max(0.0, est_seconds - elapsed)\n        print(f\"\\r{prefix}: |{bar}| {int(frac*100):3d}%  ETA: {eta:4.1f}s\", end=\"\", flush=True)\n        time.sleep(0.12)\n    bar = \"█\" * bar_length\n    print(f\"\\r{prefix}: |{bar}| 100%  ETA:   0.0s\")\n    time.sleep(0.06)\n    print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n\nprint(\"Preparing embeddings for\", len(texts), \"documents...\")\nembeddings = []\nstop_event = threading.Event()\nprogress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, max(5, len(texts)/20), \"Embedding Top-K\"), daemon=True)\nprogress_thread.start()\nfor t in texts:\n    embeddings.append(sentence_embedding(t))\nstop_event.set()\nprogress_thread.join()\n\nembeddings = np.vstack(embeddings).astype(\"float32\")\ndim = embeddings.shape[1]\nnum_elements = embeddings.shape[0]\n\nindex = hnswlib.Index(space=\"cosine\", dim=dim)\nindex.init_index(max_elements=num_elements, ef_construction=200, M=16)\nindex.add_items(embeddings, np.arange(num_elements))\nindex.set_ef(50)\nprint(\"hnswlib index built. Num elements:\", index.get_current_count())\n\n# =========================\n# 4. Top-K retrieval\n# =========================\ndef preprocess_query(s: str) -> str:\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\ndef retrieve_topk(query_text: str, k: int = 5):\n    q = preprocess_query(query_text)\n    q_emb = sentence_embedding(q).astype(\"float32\").reshape(1, -1)\n    labels, distances = index.knn_query(q_emb, k=k)\n    results = []\n    for dist, idx in zip(distances[0], labels[0]):\n        if idx < 0:\n            continue\n        score = float(1.0 - dist)\n        results.append({\"score\": score, \"index\": int(idx), \"text\": texts[idx], \"row\": df.iloc[idx].to_dict()})\n    return results\n\n# =========================\n# 5. Tiện ích trích xuất câu trả lời (deterministic -> trả plain text)\n# =========================\ndef _clean_text(t: str) -> str:\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\ndef _select_advice_sentence_from_ref(ref_text: str):\n    if not ref_text:\n        return \"\"\n    sentences = re.split(r'(?<=[.!?])\\s+', ref_text.strip())\n    action_keywords = ['nên', 'rửa', 'nhỏ', 'nghỉ', 'bổ sung', 'chườm', 'đổi', 'khám', 'dùng', 'xịt', 'đeo', 'tránh', 'uống', 'đi khám']\n    for s in sentences:\n        sl = s.lower()\n        for kw in action_keywords:\n            if kw in sl:\n                return s.strip().rstrip('.').strip()\n    if sentences:\n        sentences_sorted = sorted(sentences, key=lambda x: len(x), reverse=True)\n        return sentences_sorted[0].strip().rstrip('.').strip()\n    return _clean_text(ref_text)\n\ndef _similar(a: str, b: str) -> float:\n    return SequenceMatcher(None, a, b).ratio()\n\ndef deterministic_answer_from_refs_text(user_text: str, refs: dict, ref_specialty: dict, topk_rows: List[dict], sim_thresh=0.65):\n    \"\"\"\n    Cố gắng xác định reference phù hợp và trả về PLAIN TEXT gồm 3 phần:\n      Chuyên khoa: ...\n      Lời khuyên: ...\n      Tham khảo: [n]\n    Nếu không tìm được -> trả về (None, None)\n    \"\"\"\n    user_norm = _clean_text(user_text).lower()\n    best_idx = None\n    best_score = 0.0\n    for i, r in enumerate(topk_rows, start=1):\n        q = r[\"row\"].get(\"question\", \"\") or \"\"\n        q = _clean_text(q).lower()\n        s1 = _similar(user_norm, q) if q else 0.0\n        a = r[\"row\"].get(\"answer\", \"\") or \"\"\n        a = _clean_text(a).lower()\n        s2 = _similar(user_norm, a) if a else 0.0\n        score = max(s1, s2)\n        if score > best_score:\n            best_score = score\n            best_idx = i\n    if best_score >= sim_thresh and best_idx is not None:\n        ref_text = refs.get(best_idx, \"\")\n        advice = _select_advice_sentence_from_ref(ref_text)\n        specialty = ref_specialty.get(best_idx, \"Khám chuyên khoa\")\n        out_text = f\"Chuyên khoa: {specialty}\\nLời khuyên: {advice}\\nTham khảo: [{best_idx}]\"\n        return out_text, best_idx\n    return None, None\n\n# =========================\n# 6. PhoGPT-4B lazy load + gọi với yêu cầu trả PLAIN TEXT 3 phần\n# =========================\nrepo_path = \"/kaggle/working/PhoGPT-4B\"  # chỉnh nếu cần\ntokenizer_gpt = None\nmodel_gpt = None\ndevice_gpt = None\n\ndef ensure_gpt_loaded():\n    global tokenizer_gpt, model_gpt, device_gpt\n    if tokenizer_gpt is None or model_gpt is None:\n        tokenizer_gpt = AutoTokenizer.from_pretrained(repo_path, trust_remote_code=True)\n        if tokenizer_gpt.pad_token_id is None:\n            tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n            tokenizer_gpt.pad_token_id = tokenizer_gpt.eos_token_id\n        model_gpt = AutoModelForCausalLM.from_pretrained(\n            repo_path,\n            trust_remote_code=True,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True\n        )\n        device_gpt = next(model_gpt.parameters()).device\n        print(f\"PhoGPT-4B loaded on device: {device_gpt}\")\n\ndef call_gpt_short_prompt_text(refs_text: str, user_text: str, max_new_tokens=140):\n    \"\"\"\n    Gọi model và yêu cầu TRẢ VỀ PLAIN TEXT gồm 3 phần (Chuyên khoa / Lời khuyên / Tham khảo).\n    Không yêu cầu JSON, không lặp prompt.\n    \"\"\"\n    ensure_gpt_loaded()\n    system_block = (\n        \"Bạn là trợ lý y tế AI. CHỈ DỰA TRÊN CÁC THAM KHẢO LIỆT KÊ DƯỚI.\\n\"\n        \"KHÔNG THÊM THÔNG TIN NGOÀI NHỮNG GHI TRONG THAM KHẢO.\\n\"\n        \"TRẢ VỀ DUY NHẤT PLAIN TEXT NGẮN GỌN VÀ RÕ RÀNG GỒM 3 PHẦN (Không kèm giải thích dài):\\n\"\n        \"Chuyên khoa: <tên chuyên khoa>\\n\"\n        \"Lời khuyên: <một câu hoặc vài cụm từ lấy đúng từ tham khảo>\\n\"\n        \"Tham khảo: [n]\\n\"\n        \"Nếu không đủ dữ liệu, trả: \\\"Không đủ thông tin từ các tài liệu tham khảo để tư vấn; nên khám bác sĩ chuyên khoa.\\\"\"\n    )\n    prompt = system_block + \"\\n\\n\" + refs_text + \"\\n\\n\" + f\"Người bệnh mô tả: \\\"{user_text}\\\"\\n\\nTrả lời:\"\n    inputs = tokenizer_gpt(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n    inputs = {k: v.to(device_gpt) for k, v in inputs.items()}\n    with torch.no_grad():\n        out_ids = model_gpt.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.2,\n            top_p=0.9,\n            num_beams=1,\n            repetition_penalty=1.1,\n            no_repeat_ngram_size=3,\n            eos_token_id=tokenizer_gpt.eos_token_id,\n            pad_token_id=tokenizer_gpt.pad_token_id\n        )\n    output_text = tokenizer_gpt.decode(out_ids[0], skip_special_tokens=True).strip()\n    return output_text\n\n# =========================\n# 7. Vòng lặp chính\n# =========================\nprint(\"=== Chatbot (text output) đang chạy ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\nChatbot: Tạm biệt!\")\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        print(\"Chatbot: Tạm biệt!\")\n        break\n\n    # 1) Top-K\n    k = 5\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 2.5, \"Retrieving Top-K\"), daemon=True)\n    progress_thread.start()\n    topk_results = retrieve_topk(user_input, k=k)\n    stop_event.set()\n    progress_thread.join()\n\n    # 2) Cập nhật REFERENCES / keyword_map / action_keywords\n    REFERENCES = {}\n    REFERENCE_SPECIALTY = {}\n    keyword_map = {}\n    action_keywords = set()\n\n    for i, r in enumerate(topk_results, start=1):\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        m = re.search(r'@[^:]{0,40}:\\s*(.*)', raw_answer, flags=re.DOTALL)\n        answer_clean = m.group(1).strip() if m else raw_answer.strip()\n        REFERENCES[i] = answer_clean\n        REFERENCE_SPECIALTY[i] = r[\"row\"].get(\"department\", \"Khám chuyên khoa\")\n        q_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        concat = (q_text + \" \" + raw_answer).lower()\n        tokens = re.findall(r'\\w+', concat)\n        kw = [t for t in tokens if len(t) >= 4 and not t.isdigit()]\n        seen = []\n        for t in kw:\n            if t not in seen:\n                seen.append(t)\n            if len(seen) >= 12:\n                break\n        keyword_map[i] = seen\n        for act in [\"uống\", \"rửa\", \"xịt\", \"nhỏ\", \"đi khám\", \"khám\", \"chườm\", \"bổ sung\", \"đeo\", \"tránh\", \"dùng\"]:\n            if act in concat:\n                action_keywords.add(act)\n\n    # Debug prints\n    print(\"\\n--- Top-K ---\")\n    for i, r in enumerate(topk_results, start=1):\n        print(f\"[{i}] score={r['score']:.4f} | question: {r['row'].get('question','')[:120]}\")\n    print(\"\\nREFERENCES:\")\n    for kidx, v in REFERENCES.items():\n        print(f\"[{kidx}] {v[:220]}{'...' if len(v)>220 else ''}\")\n    print(\"\\nkeyword_map:\", keyword_map)\n    print(\"action_keywords:\", sorted(list(action_keywords)))\n    print(\"------------------\\n\")\n\n    # 3) Deterministic first -> trả plain text\n    deterministic_text, matched_ref = deterministic_answer_from_refs_text(user_input, REFERENCES, REFERENCE_SPECIALTY, topk_results, sim_thresh=0.62)\n    if deterministic_text is not None:\n        print(\"Chatbot:\\n\" + deterministic_text + \"\\n\")\n        continue\n\n    # 4) Gọi model (nếu cần)\n    refs_text = \"\\n\".join([f\"[{k}] {v}\" for k, v in REFERENCES.items()])\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 8.0, \"Generating Answer\"), daemon=True)\n    progress_thread.start()\n    try:\n        output_text = call_gpt_short_prompt_text(refs_text, user_input, max_new_tokens=180)\n    except Exception as e:\n        output_text = f\"ERROR_CALLING_MODEL: {e}\"\n    finally:\n        stop_event.set()\n        progress_thread.join()\n\n    # 5) In trực tiếp plain text trả về từ model\n    # Nếu model trả lời dài/không đúng form, người vận hành có thể điều chỉnh prompt hệ thống.\n    print(\"Chatbot:\\n\" + output_text + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" ## pho_retrieval_textonly_no_model.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_textonly_no_model.py\n# ====================================================\n# Pipeline chỉ dùng PhoBERT embeddings + Top-K retrieval + sentence-level extraction.\n# KHÔNG gọi mô hình sinh. Nếu không tìm câu phù hợp, trả về fallback an toàn.\n#\n# Output: PLAIN TEXT gồm 3 phần:\n#   Chuyên khoa: ...\n#   Lời khuyên: ...\n#   Tham khảo: [n]\n# ====================================================\n\nimport re\nimport time\nimport threading\nfrom html import unescape\nfrom typing import List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV & tiền xử lý\n# =========================\ncsv_path = \"/kaggle/input/bacsituvan/bacsituvan.csv\"  # <- chỉnh lại nếu cần\ndf = pd.read_csv(csv_path)\nprint(\"Số bản ghi:\", len(df))\n\ndef preprocess_text(s: str) -> str:\n    \"\"\"Tiền xử lý: thay '_' -> space, trim, tokenize tiếng Việt (underthesea).\"\"\"\n    if not isinstance(s, str):\n        return \"\"\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\nfor col in [\"question\", \"answer\", \"department\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Load PhoBERT để tạo embedding\n# =========================\nMODEL_NAME = \"vinai/phobert-base\"\ntokenizer_phobert = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    \"\"\"\n    Lấy embedding cho một câu/đoạn:\n    - Tokenize rồi mean-pooling trên last_hidden_state.\n    - Trả về numpy float32.\n    \"\"\"\n    if not text:\n        return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        last_hidden = out.last_hidden_state                     # (1, seq_len, hidden)\n        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1) # (1, seq_len, 1)\n        masked = last_hidden * attention_mask\n        summed = masked.sum(dim=1)                             # (1, hidden)\n        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n        mean_pooled = (summed / counts).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 3. Tạo embeddings cho corpus + hnswlib index (document-level)\n# =========================\ntexts = df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")\ntexts = texts.tolist()\n\ndef progress_bar_running(stop_event, est_seconds=10.0, prefix=\"Đang xử lý\"):\n    \"\"\"Progress bar đơn giản (in trên terminal).\"\"\"\n    bar_length = 30\n    start = time.perf_counter()\n    while not stop_event.is_set():\n        elapsed = time.perf_counter() - start\n        frac = min(elapsed / est_seconds, 0.99)\n        filled = int(round(bar_length * frac))\n        bar = \"█\" * filled + \"-\" * (bar_length - filled)\n        eta = max(0.0, est_seconds - elapsed)\n        print(f\"\\r{prefix}: |{bar}| {int(frac*100):3d}%  ETA: {eta:4.1f}s\", end=\"\", flush=True)\n        time.sleep(0.12)\n    bar = \"█\" * bar_length\n    print(f\"\\r{prefix}: |{bar}| 100%  ETA:   0.0s\")\n    time.sleep(0.06)\n    print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n\nprint(\"Preparing embeddings for\", len(texts), \"documents...\")\nembeddings = []\nstop_event = threading.Event()\nprogress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, max(5, len(texts)/20), \"Embedding Top-K\"), daemon=True)\nprogress_thread.start()\nfor t in texts:\n    embeddings.append(sentence_embedding(t))\nstop_event.set()\nprogress_thread.join()\n\nembeddings = np.vstack(embeddings).astype(\"float32\")\ndim = embeddings.shape[1]\nnum_elements = embeddings.shape[0]\n\nindex = hnswlib.Index(space=\"cosine\", dim=dim)\nindex.init_index(max_elements=num_elements, ef_construction=200, M=16)\nindex.add_items(embeddings, np.arange(num_elements))\nindex.set_ef(50)\nprint(\"hnswlib index built. Num elements:\", index.get_current_count())\n\n# =========================\n# 4. Top-K retrieval\n# =========================\ndef preprocess_query(s: str) -> str:\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\ndef retrieve_topk(query_text: str, k: int = 5):\n    \"\"\"Trả về list các dict {score, index, text, row}\"\"\"\n    q = preprocess_query(query_text)\n    q_emb = sentence_embedding(q).astype(\"float32\").reshape(1, -1)\n    labels, distances = index.knn_query(q_emb, k=k)\n    results = []\n    for dist, idx in zip(distances[0], labels[0]):\n        if idx < 0:\n            continue\n        score = float(1.0 - dist)\n        results.append({\"score\": score, \"index\": int(idx), \"text\": texts[idx], \"row\": df.iloc[idx].to_dict()})\n    return results\n\n# =========================\n# 5. Hàm trợ giúp trích xuất ở cấp câu (sentence-level)\n# =========================\ndef _clean_text(t: str) -> str:\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\ndef split_sentences(text: str) -> List[str]:\n    \"\"\"Chia text thành câu (đơn giản, phù hợp VN)\"\"\"\n    if not text:\n        return []\n    sent = re.split(r'(?<=[.!?])\\s+', text.strip())\n    if len(sent) == 1:\n        parts = [s.strip() for s in text.split('\\n') if s.strip()]\n        if parts:\n            return parts\n    return [s.strip().rstrip('.') for s in sent if s.strip()]\n\ndef cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Cosine similarity (vectors are 1D numpy)\"\"\"\n    if a is None or b is None:\n        return 0.0\n    na = np.linalg.norm(a)\n    nb = np.linalg.norm(b)\n    if na == 0 or nb == 0:\n        return 0.0\n    return float(np.dot(a, b) / (na * nb))\n\ndef find_best_sentence_by_embedding(user_text: str, references: dict, ref_specialty: dict, topk_rows: List[dict], sent_sim_thresh=0.68) -> Tuple[str, int]:\n    \"\"\"\n    1) Tách tất cả câu trong các tham khảo (chỉ top-k).\n    2) Lấy embedding cho từng câu và so sánh cosine với embedding user.\n    3) Nếu câu có similarity >= sent_sim_thresh -> chọn câu tốt nhất (cao nhất).\n    Trả về (out_text, ref_idx) hoặc (None, None) nếu không đủ tương đồng.\n    \"\"\"\n    user_emb = sentence_embedding(preprocess_query(user_text)).astype(\"float32\")\n    best_score = 0.0\n    best_sentence = None\n    best_ref_idx = None\n\n    for i, r in enumerate(topk_rows, start=1):\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        sents = split_sentences(raw_answer)\n        for s in sents:\n            s_clean = _clean_text(s)\n            if len(s_clean) < 6:\n                continue\n            s_emb = sentence_embedding(s_clean).astype(\"float32\")\n            sim = cosine_sim(user_emb, s_emb)\n            if sim > best_score:\n                best_score = sim\n                best_sentence = s_clean\n                best_ref_idx = i\n    if best_score >= sent_sim_thresh and best_sentence:\n        spec = ref_specialty.get(best_ref_idx, \"Khám chuyên khoa\")\n        out_text = f\"Chuyên khoa: {spec}\\nLời khuyên: {best_sentence}\\nTham khảo: [{best_ref_idx}]\"\n        return out_text, best_ref_idx\n    return None, None\n\n# =========================\n# 6. Vòng lặp chính (KHÔNG GỌI MÔ HÌNH SINH)\n# =========================\nFALLBACK = \"Không đủ thông tin từ các tài liệu tham khảo để tư vấn; nên khám bác sĩ chuyên khoa.\"\n\nprint(\"=== Chatbot (text-only, no model) đang chạy ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\nChatbot: Tạm biệt!\")\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        print(\"Chatbot: Tạm biệt!\")\n        break\n\n    # 1) Lấy top-K (k=1)\n    k = 1\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 2.5, \"Retrieving Top-K\"), daemon=True)\n    progress_thread.start()\n    topk_results = retrieve_topk(user_input, k=k)\n    stop_event.set()\n    progress_thread.join()\n\n    # 2) Cập nhật REFERENCES / REFERENCE_SPECIALTY / keyword_map / action_keywords (debug info)\n    REFERENCES = {}\n    REFERENCE_SPECIALTY = {}\n    keyword_map = {}\n    action_keywords = set()\n    for i, r in enumerate(topk_results, start=1):\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        m = re.search(r'@[^:]{0,40}:\\s*(.*)', raw_answer, flags=re.DOTALL)\n        answer_clean = m.group(1).strip() if m else raw_answer.strip()\n        REFERENCES[i] = answer_clean\n        REFERENCE_SPECIALTY[i] = r[\"row\"].get(\"department\", \"Khám chuyên khoa\")\n        q_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        concat = (q_text + \" \" + raw_answer).lower()\n        tokens = re.findall(r'\\w+', concat)\n        kw = [t for t in tokens if len(t) >= 4 and not t.isdigit()]\n        seen = []\n        for t in kw:\n            if t not in seen:\n                seen.append(t)\n            if len(seen) >= 12:\n                break\n        keyword_map[i] = seen\n        for act in [\"uống\", \"rửa\", \"xịt\", \"nhỏ\", \"đi khám\", \"khám\", \"chườm\", \"bổ sung\", \"đeo\", \"tránh\", \"dùng\"]:\n            if act in concat:\n                action_keywords.add(act)\n\n    # In debug (có thể tắt khi production)\n    print(\"\\n--- Top-K ---\")\n    for i, r in enumerate(topk_results, start=1):\n        print(f\"[{i}] score={r['score']:.4f} | question: {r['row'].get('question','')[:120]}\")\n    print(\"\\nREFERENCES:\")\n    for kidx, v in REFERENCES.items():\n        print(f\"[{kidx}] {v[:220]}{'...' if len(v)>220 else ''}\")\n    print(\"\\nkeyword_map:\", keyword_map)\n    print(\"action_keywords:\", sorted(list(action_keywords)))\n    print(\"------------------\\n\")\n\n    # 3) Cố gắng trích câu phù hợp bằng embedding (sentence-level)\n    deterministic_text, matched_ref = find_best_sentence_by_embedding(user_input, REFERENCES, REFERENCE_SPECIALTY, topk_results, sent_sim_thresh=0.68)\n    if deterministic_text is not None:\n        print(\"Chatbot:\\n\" + deterministic_text + \"\\n\")\n        continue\n\n    # 4) Nếu không tìm được câu phù hợp -> TRẢ FALLBACK (KHÔNG GỌI MÔ HÌNH)\n    print(\"Chatbot:\\n\" + FALLBACK + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  pho_retrieval_textonly_actionfiltered.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_textonly_actionfiltered_final.py\n# ====================================================\n# Pipeline: PhoBERT embeddings + Top-5 retrieval\n# + Sentence-level extraction (Gộp các câu hành động tốt nhất cùng reference)\n# + Preprocessing nâng cao (lọc câu hỏi, lọc tên riêng)\n# KHÔNG gọi mô hình sinh (Generative AI).\n# ====================================================\n\nimport re\nimport time\nimport threading\nfrom html import unescape\nfrom typing import List, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV & tiền xử lý (tokenize tiếng Việt)\n# =========================\n# Lưu ý: Thay đổi đường dẫn file nếu chạy trên môi trường khác\ncsv_path = \"/kaggle/working/filtered-question-answers.csv\" \ntry:\n    df = pd.read_csv(csv_path)\n    print(\"Số bản ghi:\", len(df))\nexcept Exception as e:\n    print(f\"Lỗi đọc file CSV: {e}\")\n    # Tạo dummy data để code không crash nếu không có file thật\n    df = pd.DataFrame(columns=[\"question\", \"answer\", \"topic\", \"advice\"])\n\ndef preprocess_text(s: str) -> str:\n    \"\"\"Tiền xử lý: thay '_' -> space, trim, tokenize tiếng Việt (underthesea).\"\"\"\n    if not isinstance(s, str):\n        return \"\"\n    # Xử lý sơ bộ khoảng trắng\n    s = re.sub(r'\\s+', ' ', s.strip())\n    # underthesea.word_tokenize trả về chuỗi token có khoảng trắng (các từ ghép nối bằng _)\n    return word_tokenize(s, format=\"text\")\n\n# Áp dụng preprocess cho các cột cần thiết\nfor col in [\"question\", \"answer\", \"topic\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Load PhoBERT để tạo embedding\n# =========================\nMODEL_NAME = \"vinai/phobert-base\"\nprint(f\"Loading model {MODEL_NAME}...\")\ntokenizer_phobert = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    \"\"\"\n    Lấy embedding cho một câu/đoạn:\n    - Tokenize rồi mean-pooling trên last_hidden_state.\n    - Trả về numpy float32.\n    \"\"\"\n    if not text:\n        return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        last_hidden = out.last_hidden_state                 # (1, seq_len, hidden)\n        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)  # (1, seq_len, 1)\n        masked = last_hidden * attention_mask\n        summed = masked.sum(dim=1)                          # (1, hidden)\n        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n        mean_pooled = (summed / counts).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 3. Tạo embeddings cho corpus + hnswlib index (document-level)\n# =========================\ntexts = df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")\ntexts = texts.tolist()\n\ndef progress_bar_running(stop_event, est_seconds=10.0, prefix=\"Đang xử lý\"):\n    \"\"\"Progress bar đơn giản (in trên terminal).\"\"\"\n    bar_length = 30\n    start = time.perf_counter()\n    while not stop_event.is_set():\n        elapsed = time.perf_counter() - start\n        frac = min(elapsed / est_seconds, 0.99)\n        filled = int(round(bar_length * frac))\n        bar = \"█\" * filled + \"-\" * (bar_length - filled)\n        eta = max(0.0, est_seconds - elapsed)\n        print(f\"\\r{prefix}: |{bar}| {int(frac*100):3d}%  ETA: {eta:4.1f}s\", end=\"\", flush=True)\n        time.sleep(0.12)\n    bar = \"█\" * bar_length\n    print(f\"\\r{prefix}: |{bar}| 100%  ETA:   0.0s\")\n    time.sleep(0.06)\n    print(\"\\r\" + \" \" * 80 + \"\\r\", end=\"\", flush=True)\n\nprint(\"Preparing embeddings for\", len(texts), \"documents...\")\nembeddings = []\nstop_event = threading.Event()\nprogress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, max(5, len(texts)/20), \"Embedding Top-K\"), daemon=True)\nprogress_thread.start()\n\n# Batch processing could be faster, but keeping per-item for simplicity as per original code\nfor t in texts:\n    embeddings.append(sentence_embedding(t))\n\nstop_event.set()\nprogress_thread.join()\n\nif embeddings:\n    embeddings = np.vstack(embeddings).astype(\"float32\")\n    dim = embeddings.shape[1]\n    num_elements = embeddings.shape[0]\n\n    index = hnswlib.Index(space=\"cosine\", dim=dim)\n    index.init_index(max_elements=num_elements, ef_construction=200, M=16)\n    index.add_items(embeddings, np.arange(num_elements))\n    index.set_ef(50)\n    print(\"hnswlib index built. Num elements:\", index.get_current_count())\nelse:\n    print(\"Warning: No embeddings created (empty data).\")\n\n# =========================\n# 4. Top-K retrieval (chỉ dùng top 5, với lọc question similarity)\n# =========================\ndef preprocess_query(s: str) -> str:\n    s = s.strip().replace(\"_\", \" \")\n    return word_tokenize(s, format=\"text\")\n\ndef cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Cosine similarity (vectors are 1D numpy)\"\"\"\n    if a is None or b is None:\n        return 0.0\n    na = np.linalg.norm(a)\n    nb = np.linalg.norm(b)\n    if na == 0 or nb == 0:\n        return 0.0\n    return float(np.dot(a, b) / (na * nb))\n\ndef retrieve_topk_with_question_threshold(query_text: str, k: int = 5, question_sim_thresh: float = 0.5):\n    \"\"\"\n    Lấy top-k document từ index dựa trên embedding (document-level),\n    sau đó kiểm tra từng candidate: compute embedding cho 'question' của record\n    và chỉ giữ những record mà cosine(user_emb, question_emb) >= question_sim_thresh.\n    \"\"\"\n    q = preprocess_query(query_text)\n    user_emb = sentence_embedding(q).astype(\"float32\").reshape(1, -1)\n    \n    # Lấy nhiều hơn k candidate để lọc\n    multiplier = 3\n    raw_k = max(k, k * multiplier)\n    \n    if index.get_current_count() == 0:\n        return []\n\n    labels, distances = index.knn_query(user_emb, k=min(raw_k, index.get_current_count()))\n    labels = labels[0]\n    distances = distances[0]\n\n    filtered_results = []\n    for dist, idx in zip(distances, labels):\n        if idx < 0:\n            continue\n        question_text = df.iloc[int(idx)].get(\"question\", \"\") or \"\"\n        if not question_text:\n            continue\n        \n        # Check similarity với câu hỏi gốc\n        q_emb = sentence_embedding(preprocess_query(question_text)).astype(\"float32\")\n        sim_q = cosine_sim(user_emb.reshape(-1), q_emb)\n        \n        if sim_q >= question_sim_thresh:\n            score = float(1.0 - dist)\n            filtered_results.append({\n                \"score\": score,\n                \"index\": int(idx),\n                \"text\": texts[int(idx)],\n                \"row\": df.iloc[int(idx)].to_dict(),\n                \"question_sim\": sim_q\n            })\n        \n        if len(filtered_results) >= k:\n            break\n            \n    return filtered_results[:k]\n\n# =========================\n# 5. Hàm trợ giúp trích xuất ở cấp câu & preprocessing câu\n# =========================\ndef _clean_text(t: str) -> str:\n    \"\"\"Làm sạch whitespace / HTML entitles.\"\"\"\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\n# Loại bỏ phần bắt đầu dạng \"@tên: ...\"\n_re_at_prefix = re.compile(r'^@[^:]{0,60}:\\s*', flags=re.IGNORECASE)\n\n# Các từ xưng hô cần chuẩn hóa\n_PRONOUNS = [\n    r'\\bcháu\\b', r'\\bem\\b', r'\\btớ\\b', r'\\bmình\\b', r'\\bcon\\b', r'\\banh\\b', r'\\bchị\\b'\n]\n_pronoun_pattern = re.compile(\"|\".join(_PRONOUNS), flags=re.IGNORECASE)\n\n# Các liên từ nối câu thường gặp\nCONNECTIVES = [\n    r'vì vậy', r'vì thế', r'vậy nên', r'do vậy', r'vì vậy nên', r'vì thế nên', r'cho nên',\n    r'tóm lại', r'tóm tắt', r'nhưng', r'tuy nhiên'\n]\n_connective_pattern = re.compile(\"|\".join([re.escape(x) for x in CONNECTIVES]), flags=re.IGNORECASE)\n\n# Regex phát hiện tên riêng (Word tokenized thường nối bằng dấu gạch dưới hoặc Viết Hoa liên tiếp)\n# Ví dụ: Vũ_Công_Thắng, Bác_sĩ A...\n_name_pattern = re.compile(r'\\b([A-ZÀ-Ỹ][a-zà-ỹ]+(?:_[A-ZÀ-Ỹ][a-zà-ỹ]+)+)\\b') \n# Regex phát hiện danh xưng bác sĩ + tên (ví dụ: BS. Nguyễn Văn A)\n_doctor_pattern = re.compile(r'\\b(BS|Bác sĩ|Lương y|Dr)\\.?\\s+([A-ZÀ-Ỹ][a-zà-ỹ_]+(\\s+[A-ZÀ-Ỹ][a-zà-ỹ_]+)*)', flags=re.IGNORECASE)\n\ndef preprocess_reference_sentence_for_embedding(s: str) -> str:\n    \"\"\"\n    Tiền xử lý câu TRƯỚC khi tính embedding/score:\n    - Loại bỏ câu hỏi (kết thúc bằng ?)\n    - Loại bỏ cụm \"Trả lời\" ở đầu câu.\n    - Loại bỏ tên riêng, danh xưng bác sĩ\n    - Chuẩn hóa đại từ, loại bỏ từ nối\n    \"\"\"\n    if not s:\n        return \"\"\n    s = s.strip()\n    \n    # 1. Loại bỏ câu hỏi\n    if s.endswith('?'):\n        return \"\" # Trả về rỗng để code phía sau loại bỏ câu này\n\n    # 2. Xử lý rác đặc biệt (prefix @)\n    s = _re_at_prefix.sub(\"\", s)\n\n    # [MỚI] 2.1. Xóa cụm \"Trả lời\" / \"Trả_lời\" ở đầu câu\n    # Giải thích Regex:\n    # ^         : Bắt đầu chuỗi\n    # trả       : Chữ \"trả\"\n    # [_\\s]     : Dấu gạch dưới (_) hoặc khoảng trắng (do word_tokenize có thể tạo ra Trả_lời)\n    # lời       : Chữ \"lời\"\n    # \\s* : Khoảng trắng tùy ý\n    # [:.]?     : Dấu hai chấm hoặc dấu chấm (có thể có hoặc không)\n    # \\s* : Khoảng trắng sau dấu câu\n    s = re.sub(r'^trả[_\\s]lời\\s*[:.]?\\s*', '', s, flags=re.IGNORECASE)\n\n    # 3. Loại bỏ tên riêng (Vũ_Công_Thắng) và danh xưng bác sĩ\n    s = _name_pattern.sub(\"\", s)  # Xóa tên dạng tokenized (Vũ_Công_Thắng)\n    s = _doctor_pattern.sub(\"\", s) # Xóa dạng \"BS Nguyễn Văn A\"\n\n    # 4. Thay đại từ xưng hô -> 'bạn'\n    s = _pronoun_pattern.sub(\"bạn\", s)\n\n    # 5. Xóa từ nối\n    s = _connective_pattern.sub(\"\", s)\n\n    # 6. Chuẩn hóa khoảng trắng\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\ndef split_sentences(text: str) -> List[str]:\n    \"\"\"Chia text thành câu (dựa trên dấu chấm câu).\"\"\"\n    if not text:\n        return []\n    # Tách câu dựa trên . ! ?\n    sent = re.split(r'(?<=[.!?])\\s+', text.strip())\n    # Nếu tách không được (ít dấu chấm), thử tách theo dòng\n    if len(sent) == 1:\n        parts = [s.strip() for s in text.split('\\n') if s.strip()]\n        if parts:\n            return parts\n    return [s.strip().rstrip('.') for s in sent if s.strip()]\n\n# =========================\n# 6. Danh sách động từ hành động y tế (ĐÃ CẬP NHẬT MỞ RỘNG)\n# =========================\nACTION_VERBS = set([\n    # Nhóm dùng thuốc / điều trị\n    \"uống\", \"uống thuốc\", \"dùng\", \"dùng thuốc\", \"xịt\", \"bôi\", \"thoa\", \"nhỏ\", \"ngậm\", \n    \"tiêm\", \"chích\", \"truyền\", \"phẫu thuật\", \"mổ\", \"tiểu phẫu\", \"kê đơn\", \"điều trị\",\n    \"chườm\", \"chườm nóng\", \"chườm lạnh\", \"băng bó\", \"sát trùng\", \"rửa vết thương\",\n    \"hút rửa\", \"xông\", \"khí dung\", \"châm cứu\", \"bấm huyệt\", \"massage\", \"xoa bóp\",\n    \n    # Nhóm khám / xét nghiệm\n    \"khám\", \"đi khám\", \"tái khám\", \"thăm khám\", \"kiểm tra\", \"xét nghiệm\", \"lấy mẫu\",\n    \"siêu âm\", \"chụp\", \"chụp x-quang\", \"chụp ct\", \"chụp mri\", \"nội soi\", \"đo huyết áp\",\n    \"đo đường huyết\", \"theo dõi\", \"đánh giá\", \"tầm soát\",\n    \n    # Nhóm sinh hoạt / dinh dưỡng\n    \"ăn\", \"ăn kiêng\", \"kiêng\", \"tránh\", \"hạn chế\", \"bổ sung\", \"tăng cường\", \"giảm\",\n    \"uống nước\", \"ngủ\", \"nghỉ ngơi\", \"kê gối\", \"nằm nghiêng\", \"tập\", \"tập luyện\", \n    \"vận động\", \"tập vật lý trị liệu\", \"thể dục\", \"vệ sinh\", \"súc miệng\", \"súc họng\",\n    \"rửa tay\", \"rửa mũi\", \"đeo khẩu trang\", \"cách ly\", \"nhập viện\", \"cấp cứu\"\n])\n\ndef sentence_has_action(s: str) -> bool:\n    \"\"\"Kiểm tra xem s chứa động từ hành động y tế hay không.\"\"\"\n    if not s:\n        return False\n    sl = s.lower()\n    for act in ACTION_VERBS: # Không cần sort mỗi lần gọi, set truy cập nhanh\n        act_norm = act.replace(\"_\", \" \").lower()\n        # Tìm từ nguyên vẹn (word boundary)\n        if re.search(r'\\b' + re.escape(act_norm) + r'\\b', sl):\n            return True\n    return False\n\n# =========================\n# 7. Hàm chính: Tính điểm và Gộp câu (Combined Logic)\n# =========================\ndef find_best_action_sentence_by_embedding_combined(\n    user_text: str,\n    topk_rows: List[dict],\n    ref_specialty: dict,\n    sent_sim_thresh: float = 0.6,\n    combined_thresh: float = 0.68,\n    alpha: float = 0.7,\n    beta: float = 0.25,\n    gamma: float = 0.05,\n    max_debug_show: int = 15\n) -> Tuple[Optional[str], Optional[int], Optional[str]]:\n    \"\"\"\n    Tìm các câu 'hành động' tốt nhất.\n    Logic:\n    1. Tính combined score cho TẤT CẢ câu trong top-k documents.\n    2. Sắp xếp giảm dần.\n    3. Chọn câu tốt nhất (Top 1) thỏa mãn điều kiện để xác định 'Reference tốt nhất' (best_ref).\n    4. Gom tất cả các câu khác CÙNG best_ref mà cũng thỏa mãn điều kiện ngưỡng.\n    5. Nối các câu đó lại thành câu trả lời cuối cùng.\n    \"\"\"\n    print(\"\\n=== [DEBUG] RUN find_best_action_sentence_by_embedding_combined ===\")\n    \n    if not topk_rows:\n        print(\"[DEBUG] topk_rows rỗng -> trả (None, None, None)\")\n        return None, None, None\n\n    # 1) Thu thập và tiền xử lý tất cả các câu\n    all_sents = []  # list of (ref_pos, question_text, sentence_original, sentence_preprocessed)\n    orig_qa_map = {} \n    \n    for ref_pos, r in enumerate(topk_rows, start=1):\n        question_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        orig_qa_map[ref_pos] = f\"Q: {question_text}\\nA: {raw_answer}\"\n        \n        sents = split_sentences(raw_answer)\n        kept = 0\n        for s in sents:\n            s_orig = _clean_text(s)\n            # Preprocess: xóa tên, xóa câu hỏi, chuẩn hóa\n            s_proc = preprocess_reference_sentence_for_embedding(s_orig)\n            \n            # Chỉ lấy câu có độ dài nhất định và không rỗng\n            if len(s_proc) >= 6: \n                all_sents.append((ref_pos, question_text, s_orig, s_proc))\n                kept += 1\n                \n    if not all_sents:\n        print(\"[DEBUG] Không tìm thấy câu hợp lệ sau khi preprocess.\")\n        return None, None, None\n\n    # 2) Embedding user query\n    user_q = preprocess_query(user_text)\n    user_emb = sentence_embedding(user_q).astype(\"float32\")\n    user_tokens_set = set([t.lower() for t in re.findall(r'\\w+', user_text) if len(t) >= 2])\n\n    # 3) Tính điểm cho từng câu\n    question_emb_cache = {}\n    scored = [] # (combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos)\n\n    for ref_pos, question_text, sent_orig, sent_proc in all_sents:\n        # Cache question embedding theo ref_pos\n        if ref_pos not in question_emb_cache:\n            q_text_proc = preprocess_query(question_text) if question_text else \"\"\n            if q_text_proc:\n                question_emb_cache[ref_pos] = sentence_embedding(q_text_proc).astype(\"float32\")\n            else:\n                question_emb_cache[ref_pos] = np.zeros(user_emb.shape, dtype=np.float32)\n\n        # Sentence Embedding\n        s_emb = sentence_embedding(sent_proc).astype(\"float32\")\n        sim_sent = cosine_sim(user_emb, s_emb)\n        sim_q = cosine_sim(user_emb, question_emb_cache[ref_pos])\n\n        # Lexical Overlap\n        sent_tokens_set = set([t.lower() for t in re.findall(r'\\w+', sent_proc) if len(t) >= 2])\n        lex_overlap = 0.0\n        if user_tokens_set:\n            common = len(user_tokens_set & sent_tokens_set)\n            lex_overlap = float(common) / max(1, len(user_tokens_set))\n\n        combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap\n        scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))\n\n    # 4) Sắp xếp giảm dần theo combined score\n    scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)\n\n    # In Debug Top candidates\n    print(\"\\n=== [DEBUG] TOP candidates sorted by combined score ===\")\n    for idx, (comb, sim_s, sim_q, lex, s_orig, s_proc, rf) in enumerate(scored_sorted[:max_debug_show], start=1):\n        print(f\"[TOP{idx}] combined={comb:.4f} | sim_sent={sim_s:.4f} | sim_q={sim_q:.4f} | lex={lex:.3f} | ref={rf} | sent_proc='{s_proc[:50]}...'\")\n\n    # 5) Tìm Reference tốt nhất (Clustering Logic)\n    best_ref_pos = None\n    \n    # Duyệt để tìm câu Top 1 thỏa mãn điều kiện -> xác định best_ref_pos\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n             if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                 best_ref_pos = ref_pos\n                 print(f\"\\n[DEBUG] Đã tìm thấy Best Reference ID: {best_ref_pos} từ câu có score cao nhất.\")\n                 break\n    \n    if best_ref_pos is None:\n        print(\"[DEBUG] Không tìm thấy câu nào thỏa mãn ngưỡng và có từ hành động.\")\n        return None, None, None\n\n    # 6) Gom tất cả các câu thuộc best_ref_pos thỏa mãn điều kiện\n    final_sentences_list = []\n    \n    print(f\"\\n[DEBUG] Đang gom các câu thuộc Ref {best_ref_pos}...\")\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        # Chỉ xét các câu thuộc cùng bài viết tốt nhất\n        if ref_pos == best_ref_pos:\n            # Kiểm tra lại điều kiện ngưỡng cho từng câu phụ\n            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n                if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                    # Chuẩn hóa đại từ lần cuối cho câu output\n                    s_final = _pronoun_pattern.sub(\"bạn\", sent_orig)\n                    s_final = re.sub(r'\\s+', ' ', s_final).strip()\n                    final_sentences_list.append(s_final)\n                    print(f\"  -> Chọn: [{combined:.4f}] {s_final[:60]}...\")\n\n    # Gộp các câu lại thành đoạn văn\n    if not final_sentences_list:\n        return None, None, None\n\n    # Nối các câu theo thứ tự rank (điểm cao đứng trước) như yêu cầu output list\n    # Lưu ý: Nếu muốn nối theo thứ tự xuất hiện trong văn bản gốc thì cần logic khác, \n    # nhưng ở đây tuân thủ logic \"danh sách các câu tìm được\".\n    final_paragraph = \" \".join(final_sentences_list)\n    \n    # Lấy thông tin meta\n    spec = ref_specialty.get(best_ref_pos, \"Khám chuyên khoa\")\n    orig_qa = orig_qa_map.get(best_ref_pos, \"\")\n\n    out_text = (\n        f\"Chuyên khoa: {spec}\\n\"\n        f\"Lời khuyên: {final_paragraph}\\n\"\n        f\"Tham khảo:\\n{orig_qa}\\n\\n\"\n        f\"Câu trả lời chỉ mang tính chất tham khảo.\"\n    )\n    \n    return out_text, best_ref_pos, orig_qa\n\n\n# =========================\n# 8. Vòng lặp chính\n# =========================\nFALLBACK = \"Không đủ thông tin từ các tài liệu tham khảo để tư vấn; nên khám bác sĩ chuyên khoa.\"\n\nprint(\"\\n=== Chatbot (text-only, action-filtered, CLUSTERED FINAL) ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\nChatbot: Tạm biệt!\")\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        print(\"Chatbot: Tạm biệt!\")\n        break\n\n    # 1) Lấy top-5 (lọc theo question similarity)\n    stop_event = threading.Event()\n    progress_thread = threading.Thread(target=progress_bar_running, args=(stop_event, 2.5, \"Retrieving Top-5\"), daemon=True)\n    progress_thread.start()\n    \n    topk_results = retrieve_topk_with_question_threshold(user_input, k=5, question_sim_thresh=0.60)\n    \n    stop_event.set()\n    progress_thread.join()\n\n    # 2) Chuẩn bị dữ liệu Reference & Debug\n    REFERENCE_SPECIALTY = {}\n    action_keywords_found = set()\n    \n    print(\"\\n--- Top-5 Candidates ---\")\n    for i, r in enumerate(topk_results, start=1):\n        q_text = r['row'].get('question', '')\n        ans_text = r['row'].get('answer', '')\n        topic = r['row'].get('topic', 'Chung')\n        REFERENCE_SPECIALTY[i] = topic\n        \n        print(f\"[{i}] score={r['score']:.4f} | q_sim={r.get('question_sim',0):.4f} | Q: {q_text[:80]}...\")\n        \n        # Check action verbs for debug\n        full_text = (q_text + \" \" + ans_text).lower()\n        for act in ACTION_VERBS:\n            act_norm = act.replace(\"_\", \" \").lower()\n            if act_norm in full_text:\n                action_keywords_found.add(act_norm)\n\n    print(f\"\\nAction keywords found in Top-K: {sorted(list(action_keywords_found))}\")\n    print(\"------------------\\n\")\n\n    # 3) Tìm và gộp câu trả lời\n    deterministic_text, matched_ref, orig_qa_text = find_best_action_sentence_by_embedding_combined(\n        user_input, topk_results, REFERENCE_SPECIALTY,\n        sent_sim_thresh=0.60, combined_thresh=0.68, alpha=0.7, beta=0.25, gamma=0.05\n    )\n\n    if deterministic_text is not None:\n        print(\"\\nChatbot:\\n\" + deterministic_text + \"\\n\")\n    else:\n        print(\"\\nChatbot:\\n\" + FALLBACK + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## pho_retrieval_textonly_actionfiltered_final_llm.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_textonly_actionfiltered_final_llm.py\n# ====================================================\n# Pipeline RAG (Retrieval-Augmented Generation) Hoàn Chỉnh:\n# 1. Retrieval: PhoBERT embeddings + Top-5 retrieval + Filter Question Similarity\n# 2. Extraction: Chọn lọc các câu hành động (Action Sentences) tốt nhất.\n# 3. Generation: Tích hợp LLM để viết lại câu trả lời tự nhiên.\n# ====================================================\n\nimport re\nimport time\nimport threading\nfrom html import unescape\nfrom typing import List, Tuple, Optional\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, pipeline\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị & Môi trường\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV & tiền xử lý\n# =========================\n'''csv_path = \"filtered-question-answers.csv\" # Đổi đường dẫn phù hợp\ntry:\n    # Nếu chưa có file csv, bạn có thể convert từ json như các bước trước\n    # Ở đây giả định đã có file csv hoặc dataframe\n    # df = pd.read_csv(csv_path) \n    \n    # [DEMO] Tạo dummy data để code chạy được ngay nếu không có file\n    import json\n    if os.path.exists(\"/kaggle/input/filtered-question-answers/filtered-question-answers.json\"):\n         with open(\"/kaggle/input/filtered-question-answers/filtered-question-answers.json\", \"r\", encoding=\"utf-8\") as f:\n            data_json = json.load(f)\n         df = pd.DataFrame.from_dict(data_json, orient='index')\n    else:\n         print(\"Không tìm thấy file dữ liệu, tạo dữ liệu giả lập để test code...\")\n         df = pd.DataFrame({\n             \"question\": [\"đau đầu uống gì\", \"bị gãy tay làm sao\"],\n             \"answer\": [\"Bạn nên uống paracetamol 500mg và nghỉ ngơi.\", \"Cần đi chụp X-quang và bó bột ngay.\"],\n             \"topic\": [\"Thần kinh\", \"Chấn thương chỉnh hình\"]\n         })\n         \n    print(\"Số bản ghi:\", len(df))\nexcept Exception as e:\n    print(f\"Lỗi khởi tạo dữ liệu: {e}\")\n    df = pd.DataFrame(columns=[\"question\", \"answer\", \"topic\", \"advice\"])\n    '''\n# Lưu ý: Thay đổi đường dẫn file nếu chạy trên môi trường khác\ncsv_path = \"/kaggle/working/filtered-question-answers.csv\" \ntry:\n    df = pd.read_csv(csv_path)\n    print(\"Số bản ghi:\", len(df))\nexcept Exception as e:\n    print(f\"Lỗi đọc file CSV: {e}\")\n    # Tạo dummy data để code không crash nếu không có file thật\n    df = pd.DataFrame(columns=[\"question\", \"answer\", \"topic\", \"advice\"])\n\n\ndef preprocess_text(s: str) -> str:\n    if not isinstance(s, str): return \"\"\n    s = re.sub(r'\\s+', ' ', s.strip())\n    return word_tokenize(s, format=\"text\")\n\nfor col in [\"question\", \"answer\", \"topic\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Load PhoBERT (Retrieval Model)\n# =========================\nRETRIEVAL_MODEL_NAME = \"vinai/phobert-base\"\nprint(f\"Loading Retrieval model {RETRIEVAL_MODEL_NAME}...\")\ntokenizer_phobert = AutoTokenizer.from_pretrained(RETRIEVAL_MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(RETRIEVAL_MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    if not text: return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        mean_pooled = out.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 3. Tạo Embeddings & Index\n# =========================\ntexts = df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")\ntexts = texts.tolist()\n\n# (Giữ nguyên logic tạo embedding cũ của bạn)\nprint(\"Generating embeddings...\")\nembeddings = []\n# Batch processing để nhanh hơn chút\nbatch_size = 32\nfor i in range(0, len(texts), batch_size):\n    batch_texts = texts[i:i+batch_size]\n    # Lưu ý: Code gốc của bạn chạy từng cái, ở đây demo nên mình giữ đơn giản\n    for t in batch_texts:\n        embeddings.append(sentence_embedding(t))\n    if i % 100 == 0: print(f\"\\rEmbedded {i}/{len(texts)}\", end=\"\")\nprint(\"\\nDone embedding.\")\n\nembeddings = np.vstack(embeddings).astype(\"float32\")\nindex = hnswlib.Index(space=\"cosine\", dim=embeddings.shape[1])\nindex.init_index(max_elements=embeddings.shape[0], ef_construction=200, M=16)\nindex.add_items(embeddings, np.arange(embeddings.shape[0]))\nindex.set_ef(50)\n\n# =========================\n# 4 - 6. Các hàm Tiền xử lý & Trợ giúp (Giữ nguyên logic cũ)\n# =========================\ndef preprocess_query(s: str) -> str:\n    return word_tokenize(s.strip().replace(\"_\", \" \"), format=\"text\")\n\ndef cosine_sim(a, b):\n    if a is None or b is None: return 0.0\n    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n\ndef retrieve_topk(query_text, k=5, thresh=0.5):\n    q = preprocess_query(query_text)\n    user_emb = sentence_embedding(q).reshape(1, -1)\n    if index.get_current_count() == 0: return []\n    labels, distances = index.knn_query(user_emb, k=min(k*3, index.get_current_count()))\n    \n    results = []\n    for dist, idx in zip(distances[0], labels[0]):\n        if idx < 0: continue\n        q_text = df.iloc[int(idx)].get(\"question\", \"\")\n        sim_q = cosine_sim(user_emb.reshape(-1), sentence_embedding(preprocess_query(q_text)))\n        if sim_q >= thresh:\n            results.append({\n                \"score\": 1.0 - dist, \n                \"index\": int(idx), \n                \"row\": df.iloc[int(idx)].to_dict(), \n                \"question_sim\": sim_q\n            })\n        if len(results) >= k: break\n    return results[:k]\n\ndef _clean_text(t: str) -> str:\n    \"\"\"Làm sạch whitespace / HTML entitles.\"\"\"\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\n# Loại bỏ phần bắt đầu dạng \"@tên: ...\"\n_re_at_prefix = re.compile(r'^@[^:]{0,60}:\\s*', flags=re.IGNORECASE)\n\n# Các từ xưng hô cần chuẩn hóa\n_PRONOUNS = [\n    r'\\bcháu\\b', r'\\bem\\b', r'\\btớ\\b', r'\\bmình\\b', r'\\bcon\\b', r'\\banh\\b', r'\\bchị\\b'\n]\n_pronoun_pattern = re.compile(\"|\".join(_PRONOUNS), flags=re.IGNORECASE)\n\n# Các liên từ nối câu thường gặp\nCONNECTIVES = [\n    r'vì vậy', r'vì thế', r'vậy nên', r'do vậy', r'vì vậy nên', r'vì thế nên', r'cho nên',\n    r'tóm lại', r'tóm tắt', r'nhưng', r'tuy nhiên'\n]\n_connective_pattern = re.compile(\"|\".join([re.escape(x) for x in CONNECTIVES]), flags=re.IGNORECASE)\n\n# Regex phát hiện tên riêng (Word tokenized thường nối bằng dấu gạch dưới hoặc Viết Hoa liên tiếp)\n# Ví dụ: Vũ_Công_Thắng, Bác_sĩ A...\n_name_pattern = re.compile(r'\\b([A-ZÀ-Ỹ][a-zà-ỹ]+(?:_[A-ZÀ-Ỹ][a-zà-ỹ]+)+)\\b') \n# Regex phát hiện danh xưng bác sĩ + tên (ví dụ: BS. Nguyễn Văn A)\n_doctor_pattern = re.compile(r'\\b(BS|Bác sĩ|Lương y|Dr)\\.?\\s+([A-ZÀ-Ỹ][a-zà-ỹ_]+(\\s+[A-ZÀ-Ỹ][a-zà-ỹ_]+)*)', flags=re.IGNORECASE)\n\ndef preprocess_reference_sentence_for_embedding(s: str) -> str:\n    \"\"\"\n    Tiền xử lý câu TRƯỚC khi tính embedding/score:\n    - Loại bỏ câu hỏi (kết thúc bằng ?)\n    - Loại bỏ tên riêng, danh xưng bác sĩ\n    - Chuẩn hóa đại từ, loại bỏ từ nối\n    \"\"\"\n    if not s:\n        return \"\"\n    s = s.strip()\n    \n    # 1. Loại bỏ câu hỏi\n    if s.endswith('?'):\n        return \"\" # Trả về rỗng để code phía sau loại bỏ câu này\n\n    # 2. Xử lý rác đặc biệt (prefix @)\n    s = _re_at_prefix.sub(\"\", s)\n\n    # 3. Loại bỏ tên riêng (Vũ_Công_Thắng) và danh xưng bác sĩ\n    s = _name_pattern.sub(\"\", s)  # Xóa tên dạng tokenized (Vũ_Công_Thắng)\n    s = _doctor_pattern.sub(\"\", s) # Xóa dạng \"BS Nguyễn Văn A\"\n\n    # 4. Thay đại từ xưng hô -> 'bạn'\n    s = _pronoun_pattern.sub(\"bạn\", s)\n\n    # 5. Xóa từ nối\n    s = _connective_pattern.sub(\"\", s)\n\n    # 6. Chuẩn hóa khoảng trắng\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\ndef split_sentences(text: str) -> List[str]:\n    \"\"\"Chia text thành câu (dựa trên dấu chấm câu).\"\"\"\n    if not text:\n        return []\n    # Tách câu dựa trên . ! ?\n    sent = re.split(r'(?<=[.!?])\\s+', text.strip())\n    # Nếu tách không được (ít dấu chấm), thử tách theo dòng\n    if len(sent) == 1:\n        parts = [s.strip() for s in text.split('\\n') if s.strip()]\n        if parts:\n            return parts\n    return [s.strip().rstrip('.') for s in sent if s.strip()]\n# Để code ngắn gọn, mình tóm tắt lại các regex quan trọng\n_re_at_prefix = re.compile(r'^@[^:]{0,60}:\\s*', flags=re.IGNORECASE)\n_name_pattern = re.compile(r'\\b([A-ZÀ-Ỹ][a-zà-ỹ]+(?:_[A-ZÀ-Ỹ][a-zà-ỹ]+)+)\\b') \n_doctor_pattern = re.compile(r'\\b(BS|Bác sĩ|Lương y|Dr)\\.?\\s+([A-ZÀ-Ỹ][a-zà-ỹ_]+(\\s+[A-ZÀ-Ỹ][a-zà-ỹ_]+)*)', flags=re.IGNORECASE)\n_pronoun_pattern = re.compile(r'\\b(cháu|em|tớ|mình|con|anh|chị)\\b', flags=re.IGNORECASE)\nACTION_VERBS = set([\"uống\", \"khám\", \"ăn\", \"kiêng\", \"tập\", \"theo dõi\", \"điều trị\", \"phẫu thuật\", \"xét nghiệm\", \"chụp\", \"bôi\", \"rửa\", \"ngủ\", \"nghỉ\"]) \n# =========================\n# 6. Danh sách động từ hành động y tế (ĐÃ CẬP NHẬT MỞ RỘNG)\n# =========================\nACTION_VERBS = set([\n    # Nhóm dùng thuốc / điều trị\n    \"uống\", \"uống thuốc\", \"dùng\", \"dùng thuốc\", \"xịt\", \"bôi\", \"thoa\", \"nhỏ\", \"ngậm\", \n    \"tiêm\", \"chích\", \"truyền\", \"phẫu thuật\", \"mổ\", \"tiểu phẫu\", \"kê đơn\", \"điều trị\",\n    \"chườm\", \"chườm nóng\", \"chườm lạnh\", \"băng bó\", \"sát trùng\", \"rửa vết thương\",\n    \"hút rửa\", \"xông\", \"khí dung\", \"châm cứu\", \"bấm huyệt\", \"massage\", \"xoa bóp\",\n    \n    # Nhóm khám / xét nghiệm\n    \"khám\", \"đi khám\", \"tái khám\", \"thăm khám\", \"kiểm tra\", \"xét nghiệm\", \"lấy mẫu\",\n    \"siêu âm\", \"chụp\", \"chụp x-quang\", \"chụp ct\", \"chụp mri\", \"nội soi\", \"đo huyết áp\",\n    \"đo đường huyết\", \"theo dõi\", \"đánh giá\", \"tầm soát\",\n    \n    # Nhóm sinh hoạt / dinh dưỡng\n    \"ăn\", \"ăn kiêng\", \"kiêng\", \"tránh\", \"hạn chế\", \"bổ sung\", \"tăng cường\", \"giảm\",\n    \"uống nước\", \"ngủ\", \"nghỉ ngơi\", \"kê gối\", \"nằm nghiêng\", \"tập\", \"tập luyện\", \n    \"vận động\", \"tập vật lý trị liệu\", \"thể dục\", \"vệ sinh\", \"súc miệng\", \"súc họng\",\n    \"rửa tay\", \"rửa mũi\", \"đeo khẩu trang\", \"cách ly\", \"nhập viện\", \"cấp cứu\",\n\n    # Bổ sung\n    'khám', 'đi', 'uống', 'ăn', 'đi khám', 'tiêm', 'siêu', 'dùng', 'siêu âm', 'nội', 'tránh', 'đặt', 'uống thuốc', 'nhỏ', 'bổ', 'khám và', 'tránh thai', 'bổ sung', 'dùng thuốc', 'khám bệnh', 'chụp', 'ăn uống', 'khám bác sĩ', 'khám sức', 'khám sức khỏe', 'khám bác', 'truyền', 'đi ngoài', 'kê', 'nội soi', 'khám tại', 'đi siêu âm', 'khám thai', 'đặt lịch', 'đi siêu', 'khám lại', 'nội tiết', 'khám để', 'hút', 'tiêm chủng', 'khám ở', 'đi khám bác', 'khám phụ khoa', 'nội mạc', 'đi khám để', 'khám chuyên khoa', 'khám phụ', 'đi khám và', 'khám chuyên', 'nội mạc tử', 'đặt lịch khám', 'tiêm ngừa', 'khám và tư', 'kiêng', 'đi lại', 'rửa', 'khám thì', 'đi tiểu', 'đi khám ở', 'nội khoa', 'đi khám lại', 'quá lo lắng', 'uống sữa', 'ăn dặm', 'uống nước', 'tránh thai khẩn', 'bôi', 'thai khẩn cấp', 'kê đơn', 'khám tại bệnh', 'khám online', 'khám bệnh viện', 'tiêm mũi', 'khám trực', 'chụp x quang', 'tiêm vacxin', 'khám và điều', 'đi xét nghiệm', 'đi khám thai', 'đi khám thì', 'đi khám chuyên', 'tiêm phòng', 'truyền nhiễm', 'uống nhiều', 'chụp x', 'cho bé đi', 'đi tái', 'đưa bé đi', 'đi xét', 'siêu âm và', 'ăn nhiều', 'khám bệnh bv', 'bổ sung vitamin', 'đi phân', 'khám tư', 'bổ sung thêm', 'uống thuốc tránh', 'uống thêm', 'đi khám phụ', 'nội soi dạ', 'khám ngay', 'đi cầu', 'uống thuốc gì', 'khám bệnh bệnh', 'đeo', 'dùng thuốc gì', 'đi ngoài phân', 'đi tái khám', 'đến bệnh viện', 'khám thai định', 'làm sao', 'khám với', 'tiêm vaccine', 'uống được', 'uống nhiều nước', 'khám ở bệnh', 'siêu âm thì', 'đi kiểm tra', 'đi tiêm', 'đi tiêu', 'đưa bé đến', 'khám trực tiếp', 'khám được', 'đặt khám', 'đi khám ngay', 'tiêm vắc xin', 'đặt tư vấn', 'khám em', 'siêu vi', 'đặt thuốc', 'khám với bác', 'tiêm vắc', 'khám để được', 'thiết', 'đi làm', 'hút thai', 'siêu âm tim', 'đi khám tại', 'uống 1', 'tiêm được', 'tránh thai hàng', 'khám không', 'ăn và', 'uống đủ', 'hút thuốc', 'dùng thuốc tránh', 'làm gì', 'đi khám bệnh', 'đi kiểm', 'siêu âm lại', 'khám định', 'thai hàng ngày', 'đến khám tại', 'đi vệ', 'đi vệ sinh', 'tiêm thuốc', 'dùng biện pháp', 'tiêm chủng chuyên', 'siêu âm thai', 'khám định kỳ', 'khám thì bác', 'kê đơn thuốc', 'ăn được', 'ăn không', 'liên hệ với', 'khám để bác', 'kê thuốc', 'uống đủ nước', 'khám tại khoa', 'đặt vòng', 'khám bệnh chuyên', 'khám hiếm muộn', 'đi khám không', 'thai', 'dùng biện', 'đặt câu', 'đặt câu hỏi', 'uống và', 'chụp mri', 'khám sớm', 'khám cho', 'uống có', 'làm gì để', 'nội tiết tố', 'uống bổ sung', 'nội tổng', 'khám và làm', 'kê toa', 'siêu âm ở', 'nội soi bóc', 'siêu âm thấy', 'khám tư vấn', 'khám hiếm', 'dùng cho', 'đi kèm', 'ăn đủ', 'ăn của', 'khám tổng', 'khám tổng quát', 'khám và siêu'\n])\n\ndef sentence_has_action(s):\n    for act in ACTION_VERBS:\n        if re.search(r'\\b' + re.escape(act.replace(\"_\", \" \")) + r'\\b', s.lower()): return True\n    return False\n\ndef preprocess_reference_sentence_for_embedding(s: str) -> str:\n    \"\"\"\n    Tiền xử lý câu TRƯỚC khi tính embedding/score:\n    - Loại bỏ câu hỏi (kết thúc bằng ?)\n    - Loại bỏ cụm 'Trả lời' ở đầu câu (MỚI BỔ SUNG)\n    - Loại bỏ tên riêng, danh xưng bác sĩ\n    - Chuẩn hóa đại từ, loại bỏ từ nối\n    \"\"\"\n    if not s:\n        return \"\"\n    s = s.strip()\n    \n    # 1. Loại bỏ câu hỏi\n    if s.endswith('?'):\n        return \"\" # Trả về rỗng để code phía sau loại bỏ câu này\n\n    # 2. Xử lý rác đặc biệt (prefix @)\n    s = _re_at_prefix.sub(\"\", s)\n\n    # [QUAN TRỌNG] 2.1. Xóa cụm \"Trả lời\" / \"Trả_lời\" ở đầu câu\n    # Đoạn này bị thiếu trong bản code bạn vừa gửi\n    s = re.sub(r'^trả[_\\s]lời\\s*[:.]?\\s*', '', s, flags=re.IGNORECASE)\n\n    # 3. Loại bỏ tên riêng (Vũ_Công_Thắng) và danh xưng bác sĩ\n    s = _name_pattern.sub(\"\", s)  # Xóa tên dạng tokenized (Vũ_Công_Thắng)\n    s = _doctor_pattern.sub(\"\", s) # Xóa dạng \"BS Nguyễn Văn A\"\n\n    # 4. Thay đại từ xưng hô -> 'bạn'\n    s = _pronoun_pattern.sub(\"bạn\", s)\n\n    # 5. Xóa từ nối\n    s = _connective_pattern.sub(\"\", s)\n\n    # 6. Chuẩn hóa khoảng trắng\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\ndef split_sentences(text):\n    if not text: return []\n    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n\n# =========================\n# 7. Logic Gộp câu (Retrieval Logic)\n# =========================\ndef find_best_action_sentence_by_embedding_combined(\n    user_text: str,\n    topk_rows: List[dict],\n    ref_specialty: dict,\n    sent_sim_thresh: float = 0.6,\n    combined_thresh: float = 0.68,\n    alpha: float = 0.7,\n    beta: float = 0.25,\n    gamma: float = 0.05,\n    max_debug_show: int = 15\n) -> Tuple[Optional[str], Optional[int], Optional[str]]:\n    \"\"\"\n    Tìm và trích xuất thông tin hành động tốt nhất.\n    Logic:\n    1. Tính combined score (Câu + Question similarity + Lexical overlap).\n    2. Xác định 'Bài viết tốt nhất' (Best Reference) dựa trên câu có điểm cao nhất thỏa mãn điều kiện.\n    3. Gom (Cluster) tất cả các câu hành động khác thuộc cùng bài viết đó.\n    4. Trả về đoạn văn tổng hợp.\n    \"\"\"\n    print(\"\\n=== [DEBUG] RUN find_best_action_sentence_by_embedding_combined ===\")\n    \n    if not topk_rows:\n        print(\"[DEBUG] topk_rows rỗng -> trả (None, None, None)\")\n        return None, None, None\n\n    # -------------------------------------------------------\n    # 1. Thu thập và tiền xử lý tất cả các câu từ Top-K Documents\n    # -------------------------------------------------------\n    all_sents = []  # list of (ref_pos, question_text, sentence_original, sentence_preprocessed)\n    orig_qa_map = {} \n    \n    for ref_pos, r in enumerate(topk_rows, start=1):\n        question_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        # Lưu lại bản gốc để hiển thị phần \"Tham khảo\"\n        orig_qa_map[ref_pos] = f\"Q: {question_text}\\nA: {raw_answer}\"\n        \n        # Tách câu\n        sents = split_sentences(raw_answer)\n        kept = 0\n        for s in sents:\n            s_orig = _clean_text(s)\n            # Preprocess: xóa tên, xóa câu hỏi, chuẩn hóa để tính embedding\n            s_proc = preprocess_reference_sentence_for_embedding(s_orig)\n            \n            # Chỉ lấy câu có độ dài >= 6 ký tự và không rỗng\n            if len(s_proc) >= 6: \n                all_sents.append((ref_pos, question_text, s_orig, s_proc))\n                kept += 1\n                \n    if not all_sents:\n        print(\"[DEBUG] Không tìm thấy câu hợp lệ sau khi preprocess.\")\n        return None, None, None\n\n    # -------------------------------------------------------\n    # 2. Embedding User Query & Chuẩn bị dữ liệu so sánh\n    # -------------------------------------------------------\n    user_q = preprocess_query(user_text)\n    user_emb = sentence_embedding(user_q).astype(\"float32\")\n    # Tập token của user để tính Lexical Overlap\n    user_tokens_set = set([t.lower() for t in re.findall(r'\\w+', user_text) if len(t) >= 2])\n\n    # -------------------------------------------------------\n    # 3. Tính điểm Combined Score cho từng câu\n    # -------------------------------------------------------\n    question_emb_cache = {}\n    scored = [] # List chứa kết quả: (combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos)\n\n    for ref_pos, question_text, sent_orig, sent_proc in all_sents:\n        # Cache embedding của câu hỏi gốc (Question Embedding) để tránh tính lại nhiều lần\n        if ref_pos not in question_emb_cache:\n            q_text_proc = preprocess_query(question_text) if question_text else \"\"\n            if q_text_proc:\n                question_emb_cache[ref_pos] = sentence_embedding(q_text_proc).astype(\"float32\")\n            else:\n                question_emb_cache[ref_pos] = np.zeros(user_emb.shape, dtype=np.float32)\n\n        # A. Sentence Similarity\n        s_emb = sentence_embedding(sent_proc).astype(\"float32\")\n        sim_sent = cosine_sim(user_emb, s_emb)\n        \n        # B. Question Similarity (User Query vs Reference Question)\n        sim_q = cosine_sim(user_emb, question_emb_cache[ref_pos])\n\n        # C. Lexical Overlap (Độ trùng lặp từ khóa)\n        sent_tokens_set = set([t.lower() for t in re.findall(r'\\w+', sent_proc) if len(t) >= 2])\n        lex_overlap = 0.0\n        if user_tokens_set:\n            common = len(user_tokens_set & sent_tokens_set)\n            lex_overlap = float(common) / max(1, len(user_tokens_set))\n\n        # D. Công thức tổng hợp\n        combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap\n        \n        scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))\n\n    # -------------------------------------------------------\n    # 4. Sắp xếp giảm dần theo Combined Score\n    # -------------------------------------------------------\n    scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)\n\n    # In Debug Top candidates (để kiểm tra)\n    print(\"\\n=== [DEBUG] TOP candidates sorted by combined score ===\")\n    for idx, (comb, sim_s, sim_q, lex, s_orig, s_proc, rf) in enumerate(scored_sorted[:max_debug_show], start=1):\n        print(f\"[TOP{idx}] combined={comb:.4f} | sim_sent={sim_s:.4f} | sim_q={sim_q:.4f} | ref={rf} | '{s_proc[:50]}...'\")\n\n    # -------------------------------------------------------\n    # 5. Xác định Reference (Bài viết) tốt nhất\n    # -------------------------------------------------------\n    best_ref_pos = None\n    \n    # Duyệt từ trên xuống, tìm câu đầu tiên thỏa mãn ngưỡng VÀ có chứa Action Verb\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n             if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                 best_ref_pos = ref_pos\n                 print(f\"\\n[DEBUG] Đã tìm thấy Best Reference ID: {best_ref_pos} từ câu có score cao nhất.\")\n                 break\n    \n    if best_ref_pos is None:\n        print(\"[DEBUG] Không tìm thấy câu nào thỏa mãn ngưỡng và có từ hành động.\")\n        return None, None, None\n\n    # -------------------------------------------------------\n    # 6. Gom (Cluster) các câu thuộc Best Reference\n    # -------------------------------------------------------\n    final_sentences_list = []\n    print(f\"\\n[DEBUG] Đang gom các câu thuộc Ref {best_ref_pos}...\")\n    \n    # Duyệt lại danh sách đã sắp xếp\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        # Chỉ xét các câu thuộc cùng bài viết tốt nhất (best_ref_pos)\n        if ref_pos == best_ref_pos:\n            # Kiểm tra lại điều kiện ngưỡng\n            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n                if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                    # Xử lý hậu kỳ: Thay 'cháu/em' thành 'bạn'\n                    s_final = _pronoun_pattern.sub(\"bạn\", sent_orig)\n                    s_final = re.sub(r'\\s+', ' ', s_final).strip()\n                    \n                    # Tránh trùng lặp nội dung\n                    if s_final not in final_sentences_list:\n                        final_sentences_list.append(s_final)\n                        print(f\"  -> Chọn: [{combined:.4f}] {s_final[:60]}...\")\n\n    if not final_sentences_list:\n        return None, None, None\n\n    # Nối các câu lại thành đoạn văn bản\n    final_paragraph = \" \".join(final_sentences_list)\n    \n    # Lấy thông tin meta để trả về\n    orig_qa = orig_qa_map.get(best_ref_pos, \"\")\n\n    # Trả về 3 giá trị để dùng cho phần LLM Generator\n    return final_paragraph, best_ref_pos, orig_qa\n\n# ==============================================================================\n# 8. [NEW] MODULE LLM GENERATION - TÍCH HỢP MÔ HÌNH NGÔN NGỮ\n# ==============================================================================\n\ndef generate_natural_response(user_query: str, retrieved_content: str, specialty: str) -> str:\n    # --- PROMPT (Giữ nguyên) ---\n    prompt = f\"\"\"\nBạn là một bác sĩ tư vấn trực tuyến tận tâm, chuyên nghiệp thuộc chuyên khoa {specialty}.\nDưới đây là thông tin y khoa đã được tra cứu từ cơ sở dữ liệu tin cậy:\n---\n{retrieved_content}\n---\nYêu cầu:\n1. Trả lời câu hỏi: \"{user_query}\" dựa trên thông tin trên.\n2. Diễn đạt tự nhiên, ân cần. Xưng hô \"Bác sĩ\" - \"bạn\".\n3. Cuối câu nhắc: \"Thông tin chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa {specialty}\"\n\"\"\"\n    print(\"\\n\" + \"=\"*20 + \" [DEBUG] LLM INPUT PROMPT \" + \"=\"*20)\n    print(prompt)\n    print(\"=\"*60 + \"\\n\")\n\n    # --- SỬ DỤNG API ---\n    try:\n        import google.generativeai as genai\n        \n        API_KEY = \"AIzaSyB4kQmT9uYLt-b0mKNL4ReUs8uVAx_bCpI\" \n        \n        if API_KEY.startswith(\"DIEN_API\"):\n             # Nếu chưa điền key, nhảy xuống mock\n             raise ValueError(\"Chưa điền API Key\")\n\n        genai.configure(api_key=API_KEY) \n        \n        # --- CẬP NHẬT MODEL THEO DANH SÁCH KHẢ DỤNG ---\n        # Ưu tiên 1: Gemini 2.0 Flash (Nhanh, thông minh, đời mới nhất)\n        target_model = 'gemini-2.0-flash'\n        \n        try:\n            model = genai.GenerativeModel(target_model)\n            response = model.generate_content(prompt)\n            if response and response.text:\n                return response.text\n                \n        except Exception as e_primary:\n            print(f\"[LLM Info] '{target_model}' gặp lỗi: {e_primary}\")\n            print(\"Đang thử model dự phòng 'gemini-2.0-flash-lite'...\")\n            \n            # Ưu tiên 2: Gemini 2.0 Flash Lite (Nhẹ hơn, dự phòng)\n            try:\n                model = genai.GenerativeModel('gemini-2.0-flash-lite')\n                response = model.generate_content(prompt)\n                if response and response.text:\n                    return response.text\n            except Exception as e_secondary:\n                 print(f\"[LLM Error] Cả 2 model đều lỗi. Chi tiết: {e_secondary}\")\n            \n    except ImportError:\n        print(\"[LLM Warning] Chưa cài thư viện 'google-generativeai'.\")\n    except Exception as e:\n        print(f\"[LLM Error] Gọi API thất bại: {e}\")\n        # Đoạn code dưới đây giúp bạn xem mình được quyền dùng model nào\n        # Chỉ chạy khi debug để biết tên model đúng\n        try:\n            print(\"Danh sách model khả dụng với Key của bạn:\")\n            for m in genai.list_models():\n                if 'generateContent' in m.supported_generation_methods:\n                    print(f\"- {m.name}\")\n        except:\n            pass\n        print(\"-> Chuyển sang chế độ MOCK response.\")\n\n    # --- MOCK RESPONSE (Fallback) ---\n    print(\"[DEBUG] Đang chạy chế độ MOCK (Fallback)...\")\n    time.sleep(1.0)\n    \n    mock_response = (\n        f\"Chào bạn, bác sĩ chuyên khoa {specialty} xin giải đáp:\\n\\n\"\n        f\"{retrieved_content}\\n\\n\"\n        f\"(Câu trả lời được tổng hợp tự động, bạn nên đi khám trực tiếp).\"\n    )\n    \n    return mock_response\n\n# =========================\n# 9. Vòng lặp chính (Đã cập nhật)\n# =========================\nFALLBACK = \"Xin lỗi, hiện tại hệ thống không tìm thấy thông tin phù hợp trong dữ liệu tham khảo. Bạn vui lòng đi khám trực tiếp tại cơ sở y tế.\"\n\nprint(\"\\n=== Chatbot RAG (PhoBERT Retrieval + LLM Generation) ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        break\n\n    # BƯỚC 1: Retrieval (Tìm kiếm)\n    print(\"--- [1] Đang tìm kiếm thông tin... ---\")\n    topk_results = retrieve_topk(user_input, k=5, thresh=0.55)\n    \n    # Tạo metadata mapping\n    ref_specialty = {}\n    for i, r in enumerate(topk_results, 1):\n        ref_specialty[i] = r['row'].get('topic')\n\n    # BƯỚC 2: Extraction (Trích xuất thông tin thô)\n    print(\"--- [2] Đang trích xuất & lọc thông tin... ---\")\n    raw_advice_text, matched_ref_id, orig_qa = find_best_action_sentence_by_embedding_combined(\n        user_input, topk_results, ref_specialty\n    )\n\n    # BƯỚC 3: Generation (Sinh câu trả lời với LLM)\n    if raw_advice_text:\n        # Lấy chuyên khoa của bài viết tốt nhất\n        current_spec = ref_specialty.get(matched_ref_id)\n        \n        print(f\"[DEBUG] Raw extracted text found from Ref {matched_ref_id}: {raw_advice_text[:50]}...\")\n        print(\"--- [3] Đang gọi mô hình LLM để tổng hợp... ---\")\n        \n        # GỌI HÀM LLM\n        final_response = generate_natural_response(user_input, raw_advice_text, current_spec)\n        \n        print(\"\\n\" + \"*\"*10 + \" CHATBOT TRẢ LỜI \" + \"*\"*10)\n        print(final_response)\n        print(\"-\" * 40)\n        # Tùy chọn: In ra nguồn tham khảo gốc để debug\n        # print(f\"\\n[Nguồn tham khảo - ID {matched_ref_id}]:\\n{orig_qa}\\n\")\n    else:\n        print(\"\\nChatbot: \" + FALLBACK + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## [DEBUG] pho_retrieval_textonly_actionfiltered_final_with_articles.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_textonly_actionfiltered_final_with_articles.py\n# ===========================================================\n# Pipeline RAG:\n# - Retrieval: PhoBERT embeddings cho corpus Q/A + index bài viết (link,title,txt)\n# - Extraction: tìm câu \"hành động\" tốt nhất từ top-k Q/A (combined score + lex overlap)\n# - Article retrieval: tìm bài viết liên quan nhất và trả nguyên văn trong mục \"Tham khảo\"\n# - Generation: tích hợp LLM (mock nếu không có API) để viết lại câu trả lời tự nhiên\n# Output: PLAIN TEXT (lời khuyên, chuyên khoa, tham khảo nguyên văn)\n# ===========================================================\n\nimport re\nimport time\nimport threading\nfrom html import unescape\nfrom typing import List, Tuple, Optional\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị & môi trường\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV Q/A (corpus) và tiền xử lý\n#    - file Q/A theo cấu trúc: question, answer, topic (có thể đặt khác => chỉnh csv_path)\n# =========================\ncsv_path = \"/kaggle/working/filtered-question-answers.csv\"  # chỉnh lại nếu cần\nif not os.path.exists(csv_path):\n    csv_path = \"/mnt/data/filtered-question-answers.csv\"\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"[DATA] Đã đọc Q/A từ: {csv_path} (bản ghi: {len(df)})\")\nelse:\n    print(f\"[DATA] Không tìm thấy Q/A file tại {csv_path}. Tạo dataframe rỗng.\")\n    df = pd.DataFrame(columns=[\"question\", \"answer\", \"topic\", \"advice\"])\n\ndef preprocess_text(s: str) -> str:\n    if not isinstance(s, str):\n        return \"\"\n    s = re.sub(r'\\s+', ' ', s.strip())\n    return word_tokenize(s, format=\"text\")\n\nfor col in [\"question\", \"answer\", \"topic\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Đọc file bài viết (articles) gồm các cột: link, title, txt\n#    - Mặc định tìm ở /mnt/data/articles.csv hoặc /kaggle/working/articles.csv\n# =========================\narticles_paths = [\"/kaggle/input/articles/bloomax.csv\"]\narticles_df = None\nfor p in articles_paths:\n    if os.path.exists(p):\n        try:\n            articles_df = pd.read_csv(p)\n            print(f\"[ARTICLES] Đã đọc articles từ: {p} (bản ghi: {len(articles_df)})\")\n            break\n        except Exception as e:\n            print(f\"[ARTICLES] Lỗi đọc {p}: {e}\")\nif articles_df is None:\n    # tạo dataframe rỗng để pipeline không lỗi\n    print(\"[ARTICLES] Không tìm thấy file bài viết. Article index sẽ rỗng.\")\n    articles_df = pd.DataFrame(columns=[\"link\", \"title\", \"txt\"])\n\nfor col in [\"link\", \"title\", \"txt\"]:\n    if col in articles_df.columns:\n        articles_df[col] = articles_df[col].fillna(\"\").astype(str).apply(lambda s: re.sub(r'\\s+', ' ', s.strip()))\n\n# =========================\n# 3. Load PhoBERT (retrieval encoder)\n# =========================\nRETRIEVAL_MODEL_NAME = \"vinai/phobert-base\"\nprint(f\"[MODEL] Loading retrieval model {RETRIEVAL_MODEL_NAME} ...\")\ntokenizer_phobert = AutoTokenizer.from_pretrained(RETRIEVAL_MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(RETRIEVAL_MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    \"\"\"\n    Embedding bằng PhoBERT: mean-pooling trên last_hidden_state.\n    Trả về numpy float32 vector.\n    \"\"\"\n    if not text:\n        return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        last_hidden = out.last_hidden_state  # (1, seq_len, hidden)\n        att = inputs.get(\"attention_mask\", None)\n        if att is None:\n            mean_pooled = last_hidden.mean(dim=1).squeeze().cpu().numpy().astype(\"float32\")\n        else:\n            attention_mask = att.unsqueeze(-1)\n            masked = last_hidden * attention_mask\n            summed = masked.sum(dim=1)\n            counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n            mean_pooled = (summed / counts).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 4. Tạo embeddings cho Q/A corpus (document-level) và bài viết (article-level)\n#    - Q/A: dùng texts = question + \" \" + answer\n#    - Articles: dùng title + \"\\n\\n\" + txt\n# =========================\nprint(\"[INDEX] Chuẩn bị văn bản cho index...\")\n\nqa_texts = (df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")).tolist() if not df.empty else []\narticle_texts = []\nif not articles_df.empty:\n    # combine title + content\n    for _, row in articles_df.iterrows():\n        title = row.get(\"title\", \"\") or \"\"\n        txt = row.get(\"txt\", \"\") or \"\"\n        article_texts.append((row.get(\"link\", \"\"), title, title + \"\\n\\n\" + txt))\nelse:\n    article_texts = []\n\n# Build embeddings with batching and build two indices\ndef build_hnsw_index(vectors: np.ndarray, space: str = \"cosine\") -> hnswlib.Index:\n    dim = vectors.shape[1]\n    idx = hnswlib.Index(space=space, dim=dim)\n    idx.init_index(max_elements=vectors.shape[0], ef_construction=200, M=16)\n    ids = np.arange(vectors.shape[0])\n    idx.add_items(vectors, ids)\n    idx.set_ef(50)\n    return idx\n\n# Build QA embeddings\nprint(\"[INDEX] Tạo embeddings cho Q/A ...\")\nqa_embeddings = []\nbatch_size = 32\nfor i in range(0, len(qa_texts), batch_size):\n    for t in qa_texts[i:i+batch_size]:\n        qa_embeddings.append(sentence_embedding(t))\n    if i % 200 == 0:\n        print(f\"[INDEX] Embedded QA {i}/{len(qa_texts)}\")\nif qa_embeddings:\n    qa_embeddings = np.vstack(qa_embeddings).astype(\"float32\")\n    qa_index = build_hnsw_index(qa_embeddings)\n    print(f\"[INDEX] QA index built. Num elements: {qa_index.get_current_count()}\")\nelse:\n    qa_embeddings = np.zeros((0, model_phobert.config.hidden_size), dtype=\"float32\")\n    qa_index = None\n    print(\"[INDEX] QA index is empty.\")\n\n# Build Article embeddings\nprint(\"[INDEX] Tạo embeddings cho Articles ...\")\narticle_embeddings = []\nfor i, (_, title, content) in enumerate(article_texts):\n    article_embeddings.append(sentence_embedding(content))\n    if i % 100 == 0:\n        print(f\"[INDEX] Embedded Articles {i}/{len(article_texts)}\")\nif article_embeddings:\n    article_embeddings = np.vstack(article_embeddings).astype(\"float32\")\n    article_index = build_hnsw_index(article_embeddings)\n    print(f\"[INDEX] Article index built. Num elements: {article_index.get_current_count()}\")\nelse:\n    article_embeddings = np.zeros((0, model_phobert.config.hidden_size), dtype=\"float32\")\n    article_index = None\n    print(\"[INDEX] Article index is empty.\")\n\n# =========================\n# 5. Các hàm tiền xử lý & trợ giúp (cũ + nâng cấp)\n# =========================\ndef preprocess_query(s: str) -> str:\n    return word_tokenize(s.strip().replace(\"_\", \" \"), format=\"text\")\n\ndef cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n    if a is None or b is None:\n        return 0.0\n    na = np.linalg.norm(a)\n    nb = np.linalg.norm(b)\n    if na == 0 or nb == 0:\n        return 0.0\n    return float(np.dot(a, b) / (na * nb))\n\ndef retrieve_topk_qa(query_text: str, k: int = 5, question_sim_thresh: float = 0.55):\n    \"\"\"\n    Truy vấn QA index (document-level). Sau khi lấy raw candidates (k*3),\n    kiểm tra similarity giữa user và field 'question' của record rồi lọc.\n    Trả về list tối đa k item giống định dạng trước đó.\n    \"\"\"\n    print(f\"[RETRIEVE_QA] Query: '{query_text[:80]}' | k={k} | question_sim_thresh={question_sim_thresh}\")\n    if qa_index is None or qa_index.get_current_count() == 0:\n        print(\"[RETRIEVE_QA] QA index rỗng -> trả []\")\n        return []\n    q_proc = preprocess_query(query_text)\n    user_emb = sentence_embedding(q_proc).reshape(1, -1)\n    raw_k = min(k * 3, qa_index.get_current_count())\n    labels, distances = qa_index.knn_query(user_emb, k=raw_k)\n    labels = labels[0]\n    distances = distances[0]\n\n    results = []\n    for dist, idx in zip(distances, labels):\n        if idx < 0:\n            continue\n        # lấy question từ df (đã tiền xử lý)\n        q_text = df.iloc[int(idx)].get(\"question\", \"\") if not df.empty else \"\"\n        if not q_text:\n            continue\n        q_emb = sentence_embedding(preprocess_query(q_text)).astype(\"float32\")\n        sim_q = cosine_sim(user_emb.reshape(-1), q_emb)\n        if sim_q >= question_sim_thresh:\n            results.append({\n                \"score\": float(1.0 - dist),\n                \"index\": int(idx),\n                \"row\": df.iloc[int(idx)].to_dict(),\n                \"question_sim\": sim_q\n            })\n        if len(results) >= k:\n            break\n    print(f\"[RETRIEVE_QA] Found {len(results)} candidates.\")\n    return results\n\n# --------- BẮT ĐẦU: hàm / utils mới cho article retrieval & re-rank ----------\nimport math\n\ndef _token_set(s: str):\n    \"\"\"Đơn giản: lấy token chữ/ số, lowercase. Dùng cho lexical overlap.\"\"\"\n    return set([t.lower() for t in re.findall(r'\\w+', s) if len(t) >= 2])\n\ndef chunk_text_into_passages(text: str, max_chars: int = 500, overlap_chars: int = 80):\n    \"\"\"\n    Chia bài thành các đoạn (passages) kích thước ~max_chars với overlap.\n    Trả list các đoạn (nguyên văn).\n    \"\"\"\n    if not text:\n        return []\n    text = text.strip()\n    passages = []\n    start = 0\n    L = len(text)\n    while start < L:\n        end = start + max_chars\n        if end >= L:\n            passages.append(text[start:L].strip())\n            break\n        # cố gắng cắt ở dấu câu gần end để dễ đọc\n        cut = text.rfind('.', start, end)\n        if cut <= start:\n            cut = text.rfind('\\n', start, end)\n        if cut <= start:\n            cut = end\n        passages.append(text[start:cut].strip())\n        start = max(cut - overlap_chars, cut)  # overlap một chút\n    return [p for p in passages if p]\n\ndef re_rank_article_candidates(user_emb: np.ndarray, q_tokens_set: set, raw_candidates: List[dict],\n                               article_texts_local: List[tuple],\n                               topn_return: int = 3,\n                               w_sim: float = 0.75, w_lex: float = 0.20, w_title_boost: float = 0.05):\n    \"\"\"\n    Re-rank raw candidate list (từ hnswlib) bằng combined score:\n      combined = w_sim * sim_article + w_lex * lex_overlap + w_title_boost * title_boost_flag\n    Trả về danh sách các candidate đã bổ sung trường 'combined_score' và 'best_passage'.\n    - raw_candidates: list các item giống format trước: {'score':..., 'index': idx, ...}\n    - article_texts_local: list of tuples (link, title, content)\n    \"\"\"\n    reranked = []\n    for c in raw_candidates:\n        idx = int(c['index'])\n        link, title, content = article_texts_local[idx]\n        # 1) sim_article: nếu bạn muốn chính xác, hãy tính cosine giữa user_emb và article embedding\n        #    nhưng ở đây raw_candidates cung cấp 'score' = 1-dist; vẫn tốt để dùng lại như baseline_sim\n        baseline_sim = float(c.get('score', 0.0))\n        # 2) lexical overlap: tokens in (title + first 1000 chars of content)\n        article_snippet_for_tokens = title + \" \" + (content[:1000] if content else \"\")\n        art_tokens = _token_set(article_snippet_for_tokens)\n        if not q_tokens_set:\n            lex_overlap = 0.0\n        else:\n            common = len(q_tokens_set & art_tokens)\n            lex_overlap = common / max(1, len(q_tokens_set))\n        # 3) title_boost: nếu tiêu đề chứa >=1 token của query -> 1 else 0\n        title_tokens = _token_set(title)\n        title_boost_flag = 1.0 if (q_tokens_set & title_tokens) else 0.0\n\n        combined = w_sim * baseline_sim + w_lex * lex_overlap + w_title_boost * title_boost_flag\n\n        # 4) best_passage: tìm passage có sim cao nhất (chi tiết hơn)\n        passages = chunk_text_into_passages(content, max_chars=600, overlap_chars=120)\n        best_passage = \"\"\n        best_passage_sim = -1.0\n        # compute embedding for a few top passages (limiting để không quá chậm)\n        for p in passages[:6]:  # chỉ check tối đa 6 đoạn đầu để tiết kiệm time\n            p_proc = preprocess_query(p)\n            p_emb = sentence_embedding(p_proc).astype(\"float32\")\n            sim_p = cosine_sim(user_emb, p_emb)\n            if sim_p > best_passage_sim:\n                best_passage_sim = sim_p\n                best_passage = p\n        # fallback: nếu không có passage, lấy đoạn đầu content\n        if not best_passage and content:\n            best_passage = content[:600]\n\n        reranked.append({\n            \"index\": idx,\n            \"link\": link,\n            \"title\": title,\n            \"txt\": content,\n            \"baseline_sim\": baseline_sim,\n            \"lex_overlap\": lex_overlap,\n            \"title_boost\": title_boost_flag,\n            \"combined_score\": combined,\n            \"best_passage\": best_passage,\n            \"best_passage_sim\": best_passage_sim\n        })\n\n    # Sắp xếp giảm dần theo combined_score, trả top-N\n    reranked_sorted = sorted(reranked, key=lambda x: x[\"combined_score\"], reverse=True)\n    return reranked_sorted[:topn_return]\n\n\ndef retrieve_top_article(query_text: str, k: int = 1, raw_k_multiplier: int = 3,\n                         article_texts_local: List[tuple] = None,\n                         article_index_local = None,\n                         min_combined_score: float = 0.25):\n    \"\"\"\n    HÀM CHÍNH (thay thế):\n    1) Lấy raw candidates từ article_index_local (hnswlib)\n    2) Re-rank bằng re_rank_article_candidates\n    3) Trả về top-k articles với trường 'combined_score' + 'best_passage'\n    \"\"\"\n    print(f\"[RETRIEVE_ART_OPT] Query article for: '{query_text[:120]}' | k={k}\")\n    if article_index_local is None or article_index_local.get_current_count() == 0:\n        print(\"[RETRIEVE_ART_OPT] Article index rỗng -> trả []\")\n        return []\n\n    q_proc = preprocess_query(query_text)\n    user_emb = sentence_embedding(q_proc).reshape(1, -1).astype(\"float32\")\n    q_tokens = _token_set(query_text)\n\n    raw_k = min(k * raw_k_multiplier, article_index_local.get_current_count())\n    # 1) knn query lấy raw_k candidate\n    labels, distances = article_index_local.knn_query(user_emb, k=raw_k)\n    labels = labels[0]\n    distances = distances[0]\n\n    raw_candidates = []\n    for dist, idx in zip(distances, labels):\n        if idx < 0:\n            continue\n        raw_candidates.append({\n            \"index\": int(idx),\n            \"score\": float(1.0 - dist)  # baseline sim heuristic\n        })\n    print(f\"[RETRIEVE_ART_OPT] raw candidates from HNSW: {len(raw_candidates)}\")\n\n    # 2) Re-rank candidates bằng combined score\n    reranked = re_rank_article_candidates(user_emb.reshape(-1), q_tokens, raw_candidates,\n                                          article_texts_local, topn_return=max(k, 3))\n    print(f\"[RETRIEVE_ART_OPT] reranked top candidates count: {len(reranked)}\")\n    # Debug print top few\n    for i, r in enumerate(reranked[:min(5, len(reranked))], start=1):\n        print(f\"[RE-RANK TOP{i}] idx={r['index']} | title='{r['title'][:80]}' | combined={r['combined_score']:.4f} | baseline={r['baseline_sim']:.4f} | lex={r['lex_overlap']:.3f} | title_boost={r['title_boost']} | best_passage_sim={r['best_passage_sim']:.4f}\")\n\n    # 3) Filter theo min_combined_score nếu cần\n    final = [r for r in reranked if r[\"combined_score\"] >= min_combined_score]\n    if not final:\n        # Nếu không có ai vượt ngưỡng, trả top 1 reranked (một fallback)\n        # if reranked:\n        #    print(\"[RETRIEVE_ART_OPT] Không có article đạt ngưỡng -> trả top1 reranked như fallback\")\n        #    return [ {**reranked[0], \"note\": \"fallback_no_threshold\"} ]\n        return []\n\n    # Trả tối đa k items, convert field names giống format cũ (score, index, link, title, txt)\n    out = []\n    for r in final[:k]:\n        out.append({\n            \"score\": r[\"combined_score\"],\n            \"index\": r[\"index\"],\n            \"link\": r[\"link\"],\n            \"title\": r[\"title\"],\n            \"txt\": r[\"txt\"],\n            \"best_passage\": r[\"best_passage\"],\n            \"best_passage_sim\": r[\"best_passage_sim\"]\n        })\n    return out\n\n# --------- KẾT THÚC: hàm / utils mới cho article retrieval & re-rank ----------\n\n# -------------------------\n# text cleaning utils & action verbs (giữ nguyên / mở rộng)\n# -------------------------\ndef _clean_text(t: str) -> str:\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\n_re_at_prefix = re.compile(r'^@[^:]{0,60}:\\s*', flags=re.IGNORECASE)\n_name_pattern = re.compile(r'\\b([A-ZÀ-Ỹ][a-zà-ỹ]+(?:_[A-ZÀ-Ỹ][a-zà-ỹ]+)+)\\b')\n_doctor_pattern = re.compile(r'\\b(BS|Bác sĩ|Lương y|Dr)\\.?\\s+([A-ZÀ-Ỹ][a-zà-ỹ_]+(\\s+[A-ZÀ-Ỹ][a-zà-ỹ_]+)*)', flags=re.IGNORECASE)\n_pronoun_pattern = re.compile(r'\\b(cháu|em|tớ|mình|con|anh|chị)\\b', flags=re.IGNORECASE)\nCONNECTIVES = [\n    r'vì vậy', r'vì thế', r'vậy nên', r'do vậy', r'vì vậy nên', r'vì thế nên', r'cho nên',\n    r'tóm lại', r'tóm tắt', r'nhưng', r'tuy nhiên'\n]\n_connective_pattern = re.compile(\"|\".join([re.escape(x) for x in CONNECTIVES]), flags=re.IGNORECASE)\n\ndef preprocess_reference_sentence_for_embedding(s: str) -> str:\n    \"\"\"\n    Tiền xử lý câu TRƯỚC khi tính embedding/score:\n    - Loại bỏ câu hỏi (ending ?)\n    - Loại bỏ prefix @..., 'Trả lời'\n    - Loại bỏ tên riêng, danh xưng\n    - Chuẩn hóa đại từ -> 'bạn'\n    - Loại bỏ connectives\n    \"\"\"\n    if not s:\n        return \"\"\n    s = s.strip()\n    if s.endswith('?'):\n        return \"\"\n    s = _re_at_prefix.sub(\"\", s)\n    s = re.sub(r'^trả[_\\s]lời\\s*[:.]?\\s*', '', s, flags=re.IGNORECASE)\n    s = _name_pattern.sub(\"\", s)\n    s = _doctor_pattern.sub(\"\", s)\n    s = _pronoun_pattern.sub(\"bạn\", s)\n    s = _connective_pattern.sub(\"\", s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\n# ACTION_VERBS: giữ phiên bản mở rộng như trước (cắt ngắn ở đây, dùng list dài trong code thật)\nACTION_VERBS = set([\n    # Nhóm dùng thuốc / điều trị\n    \"uống\", \"uống thuốc\", \"dùng\", \"dùng thuốc\", \"xịt\", \"bôi\", \"thoa\", \"nhỏ\", \"ngậm\", \n    \"tiêm\", \"chích\", \"truyền\", \"phẫu thuật\", \"mổ\", \"tiểu phẫu\", \"kê đơn\", \"điều trị\",\n    \"chườm\", \"chườm nóng\", \"chườm lạnh\", \"băng bó\", \"sát trùng\", \"rửa vết thương\",\n    \"hút rửa\", \"xông\", \"khí dung\", \"châm cứu\", \"bấm huyệt\", \"massage\", \"xoa bóp\",\n    \n    # Nhóm khám / xét nghiệm\n    \"khám\", \"đi khám\", \"tái khám\", \"thăm khám\", \"kiểm tra\", \"xét nghiệm\", \"lấy mẫu\",\n    \"siêu âm\", \"chụp\", \"chụp x-quang\", \"chụp ct\", \"chụp mri\", \"nội soi\", \"đo huyết áp\",\n    \"đo đường huyết\", \"theo dõi\", \"đánh giá\", \"tầm soát\",\n    \n    # Nhóm sinh hoạt / dinh dưỡng\n    \"ăn\", \"ăn kiêng\", \"kiêng\", \"tránh\", \"hạn chế\", \"bổ sung\", \"tăng cường\", \"giảm\",\n    \"uống nước\", \"ngủ\", \"nghỉ ngơi\", \"kê gối\", \"nằm nghiêng\", \"tập\", \"tập luyện\", \n    \"vận động\", \"tập vật lý trị liệu\", \"thể dục\", \"vệ sinh\", \"súc miệng\", \"súc họng\",\n    \"rửa tay\", \"rửa mũi\", \"đeo khẩu trang\", \"cách ly\", \"nhập viện\", \"cấp cứu\",\n\n    # Bổ sung\n    'khám', 'đi', 'uống', 'ăn', 'đi khám', 'tiêm', 'siêu', 'dùng', 'siêu âm', 'nội', 'tránh', 'đặt', 'uống thuốc', 'nhỏ', 'bổ', 'khám và', 'tránh thai', 'bổ sung', 'dùng thuốc', 'khám bệnh', 'chụp', 'ăn uống', 'khám bác sĩ', 'khám sức', 'khám sức khỏe', 'khám bác', 'truyền', 'đi ngoài', 'kê', 'nội soi', 'khám tại', 'đi siêu âm', 'khám thai', 'đặt lịch', 'đi siêu', 'khám lại', 'nội tiết', 'khám để', 'hút', 'tiêm chủng', 'khám ở', 'đi khám bác', 'khám phụ khoa', 'nội mạc', 'đi khám để', 'khám chuyên khoa', 'khám phụ', 'đi khám và', 'khám chuyên', 'nội mạc tử', 'đặt lịch khám', 'tiêm ngừa', 'khám và tư', 'kiêng', 'đi lại', 'rửa', 'khám thì', 'đi tiểu', 'đi khám ở', 'nội khoa', 'đi khám lại', 'quá lo lắng', 'uống sữa', 'ăn dặm', 'uống nước', 'tránh thai khẩn', 'bôi', 'thai khẩn cấp', 'kê đơn', 'khám tại bệnh', 'khám online', 'khám bệnh viện', 'tiêm mũi', 'khám trực', 'chụp x quang', 'tiêm vacxin', 'khám và điều', 'đi xét nghiệm', 'đi khám thai', 'đi khám thì', 'đi khám chuyên', 'tiêm phòng', 'truyền nhiễm', 'uống nhiều', 'chụp x', 'cho bé đi', 'đi tái', 'đưa bé đi', 'đi xét', 'siêu âm và', 'ăn nhiều', 'khám bệnh bv', 'bổ sung vitamin', 'đi phân', 'khám tư', 'bổ sung thêm', 'uống thuốc tránh', 'uống thêm', 'đi khám phụ', 'nội soi dạ', 'khám ngay', 'đi cầu', 'uống thuốc gì', 'khám bệnh bệnh', 'đeo', 'dùng thuốc gì', 'đi ngoài phân', 'đi tái khám', 'đến bệnh viện', 'khám thai định', 'làm sao', 'khám với', 'tiêm vaccine', 'uống được', 'uống nhiều nước', 'khám ở bệnh', 'siêu âm thì', 'đi kiểm tra', 'đi tiêm', 'đi tiêu', 'đưa bé đến', 'khám trực tiếp', 'khám được', 'đặt khám', 'đi khám ngay', 'tiêm vắc xin', 'đặt tư vấn', 'khám em', 'siêu vi', 'đặt thuốc', 'khám với bác', 'tiêm vắc', 'khám để được', 'thiết', 'đi làm', 'hút thai', 'siêu âm tim', 'đi khám tại', 'uống 1', 'tiêm được', 'tránh thai hàng', 'khám không', 'ăn và', 'uống đủ', 'hút thuốc', 'dùng thuốc tránh', 'làm gì', 'đi khám bệnh', 'đi kiểm', 'siêu âm lại', 'khám định', 'thai hàng ngày', 'đến khám tại', 'đi vệ', 'đi vệ sinh', 'tiêm thuốc', 'dùng biện pháp', 'tiêm chủng chuyên', 'siêu âm thai', 'khám định kỳ', 'khám thì bác', 'kê đơn thuốc', 'ăn được', 'ăn không', 'liên hệ với', 'khám để bác', 'kê thuốc', 'uống đủ nước', 'khám tại khoa', 'đặt vòng', 'khám bệnh chuyên', 'khám hiếm muộn', 'đi khám không', 'thai', 'dùng biện', 'đặt câu', 'đặt câu hỏi', 'uống và', 'chụp mri', 'khám sớm', 'khám cho', 'uống có', 'làm gì để', 'nội tiết tố', 'uống bổ sung', 'nội tổng', 'khám và làm', 'kê toa', 'siêu âm ở', 'nội soi bóc', 'siêu âm thấy', 'khám tư vấn', 'khám hiếm', 'dùng cho', 'đi kèm', 'ăn đủ', 'ăn của', 'khám tổng', 'khám tổng quát', 'khám và siêu'\n])\n\ndef sentence_has_action(s: str) -> bool:\n    sl = s.lower()\n    for act in ACTION_VERBS:\n        act_norm = act.replace(\"_\", \" \").lower()\n        if re.search(r'\\b' + re.escape(act_norm) + r'\\b', sl):\n            return True\n    return False\n\n# =========================\n# 6. Hàm chính: tìm best action sentence (combined scoring) - GIỮ NGUYÊN, TRẢ KÈM ORIG QA\n# =========================\ndef find_best_action_sentence_by_embedding_combined(\n    user_text: str,\n    topk_rows: List[dict],\n    ref_specialty: dict,\n    sent_sim_thresh: float = 0.6,\n    combined_thresh: float = 0.68,\n    alpha: float = 0.7,\n    beta: float = 0.25,\n    gamma: float = 0.05,\n    max_debug_show: int = 15\n) -> Tuple[Optional[str], Optional[int], Optional[str]]:\n    \"\"\"\n    Như mô tả trước: trả về (final_paragraph, best_ref_pos, orig_qa)\n    final_paragraph: đoạn ghép các câu hành động từ best reference, đã chuẩn hóa đại từ (bạn)\n    orig_qa: nguyên văn Q/A (để hiển thị trong \"Tham khảo\")\n    \"\"\"\n    print(\"\\n=== [DEBUG] RUN find_best_action_sentence_by_embedding_combined ===\")\n    if not topk_rows:\n        print(\"[DEBUG] topk_rows rỗng -> trả (None, None, None)\")\n        return None, None, None\n\n    # 1) thu thập câu và lưu orig QA\n    all_sents = []\n    orig_qa_map = {}\n    for ref_pos, r in enumerate(topk_rows, start=1):\n        question_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        orig_qa_map[ref_pos] = f\"Q: {question_text}\\nA: {raw_answer}\"\n        sents = re.split(r'(?<=[.!?])\\s+', raw_answer.strip()) if raw_answer else []\n        kept = 0\n        for s in sents:\n            s_orig = _clean_text(s)\n            s_proc = preprocess_reference_sentence_for_embedding(s_orig)\n            if len(s_proc) >= 6:\n                all_sents.append((ref_pos, question_text, s_orig, s_proc))\n                kept += 1\n        print(f\"[DEBUG] ref_pos={ref_pos} | kept_sentences={kept}\")\n\n    if not all_sents:\n        print(\"[DEBUG] Không có câu hợp lệ sau preprocess.\")\n        return None, None, None\n\n    # 2) embeddings user\n    user_q = preprocess_query(user_text)\n    user_emb = sentence_embedding(user_q).astype(\"float32\")\n    user_tokens_set = set([t.lower() for t in re.findall(r'\\w+', user_text) if len(t) >= 2])\n\n    # 3) compute combined scores\n    question_emb_cache = {}\n    scored = []\n    for ref_pos, question_text, sent_orig, sent_proc in all_sents:\n        if ref_pos not in question_emb_cache:\n            q_text_proc = preprocess_query(question_text) if question_text else \"\"\n            question_emb_cache[ref_pos] = sentence_embedding(q_text_proc).astype(\"float32\") if q_text_proc else np.zeros(user_emb.shape, dtype=np.float32)\n        s_emb = sentence_embedding(sent_proc).astype(\"float32\")\n        sim_sent = cosine_sim(user_emb, s_emb)\n        sim_q = cosine_sim(user_emb, question_emb_cache[ref_pos])\n        sent_tokens_set = set([t.lower() for t in re.findall(r'\\w+', sent_proc) if len(t) >= 2])\n        lex_overlap = float(len(user_tokens_set & sent_tokens_set)) / max(1, len(user_tokens_set)) if user_tokens_set else 0.0\n        combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap\n        scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))\n\n    scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)\n    print(\"\\n=== [DEBUG] TOP candidates sorted by combined score ===\")\n    for idx, (comb, sim_s, sim_q, lex, s_orig, s_proc, rf) in enumerate(scored_sorted[:max_debug_show], start=1):\n        print(f\"[TOP{idx}] combined={comb:.4f} | sim_sent={sim_s:.4f} | sim_q={sim_q:.4f} | ref={rf} | '{s_proc[:80]}...'\")\n\n    # 4) tìm best ref: câu đầu thỏa sim_sent & combined & có action verb\n    best_ref_pos = None\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n            if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                best_ref_pos = ref_pos\n                print(f\"[DEBUG] Best reference found: ref_pos={best_ref_pos} (combined={combined:.4f}, sim_sent={sim_sent:.4f})\")\n                break\n\n    if best_ref_pos is None:\n        print(\"[DEBUG] Không tìm thấy reference thỏa ngưỡng & có từ hành động.\")\n        return None, None, None\n\n    # 5) gom các câu từ best_ref_pos\n    final_sentences_list = []\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        if ref_pos == best_ref_pos:\n            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n                if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                    s_final = _pronoun_pattern.sub(\"bạn\", sent_orig)  # đổi đại từ\n                    s_final = re.sub(r'\\s+', ' ', s_final).strip()\n                    if s_final not in final_sentences_list:\n                        final_sentences_list.append(s_final)\n                        print(f\"[DEBUG] Selected (ref {ref_pos}): [{combined:.4f}] {s_final[:80]}\")\n\n    if not final_sentences_list:\n        print(\"[DEBUG] Sau khi gom không còn câu hành động -> trả None\")\n        return None, None, None\n\n    final_paragraph = \" \".join(final_sentences_list)\n    orig_qa = orig_qa_map.get(best_ref_pos, \"\")\n    return final_paragraph, best_ref_pos, orig_qa\n\n# =========================\n# 7. Module LLM generation (giữ nguyên mock / debug)\n# =========================\ndef generate_natural_response(user_query: str, retrieved_content: str, specialty: str, article_snippet: str = \"\") -> str:\n    \"\"\"\n    Gọi LLM (nếu có). Nếu không có, trả mock response.\n    - retrieved_content: đoạn text tổng hợp từ Q/A\n    - article_snippet: nội dung bài viết liên quan sẽ được đưa vào prompt và in ra tham khảo\n    \"\"\"\n    prompt = f\"\"\"\nBạn là một bác sĩ tư vấn trực tuyến chuyên khoa {specialty}.\nThông tin tham khảo Q/A trích xuất:\n{retrieved_content}\n\nBài viết tham khảo liên quan:\n{article_snippet}\n\nYêu cầu:\n1) Trả lời câu: \"{user_query}\" dựa trên thông tin trên.\n2) Diễn đạt tự nhiên, xưng hô Bác sĩ - bạn.\n3) Nếu nội dung Q/A tham khảo được trích xuất hoàn toàn KHÔNG liên quan với câu \"{user_query}\" thì trả lời: Xin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo. Vui lòng đi khám trực tiếp. \n3) Kết thúc nhắc: \"Câu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa {specialty}.\"\n\"\"\"\n\n    # --- SỬ DỤNG API ---\n    try:\n        import google.generativeai as genai\n        \n        API_KEY = \"AIzaSyB4kQmT9uYLt-b0mKNL4ReUs8uVAx_bCpI\" \n        \n        if API_KEY.startswith(\"DIEN_API\"):\n             # Nếu chưa điền key, nhảy xuống mock\n             raise ValueError(\"Chưa điền API Key\")\n\n        genai.configure(api_key=API_KEY) \n        \n        # --- CẬP NHẬT MODEL THEO DANH SÁCH KHẢ DỤNG ---\n        # Ưu tiên 1: Gemini 2.0 Flash (Nhanh, thông minh, đời mới nhất)\n        target_model = 'gemini-2.0-flash'\n        \n        try:\n            model = genai.GenerativeModel(target_model)\n            response = model.generate_content(prompt)\n            if response and response.text:\n                return response.text\n                \n        except Exception as e_primary:\n            print(f\"[LLM Info] '{target_model}' gặp lỗi: {e_primary}\")\n            print(\"Đang thử model dự phòng 'gemini-2.0-flash-lite'...\")\n            \n            # Ưu tiên 2: Gemini 2.0 Flash Lite (Nhẹ hơn, dự phòng)\n            try:\n                model = genai.GenerativeModel('gemini-2.0-flash-lite')\n                response = model.generate_content(prompt)\n                if response and response.text:\n                    return response.text\n            except Exception as e_secondary:\n                 print(f\"[LLM Error] Cả 2 model đều lỗi. Chi tiết: {e_secondary}\")\n            \n    except ImportError:\n        print(\"[LLM Warning] Chưa cài thư viện 'google-generativeai'.\")\n    except Exception as e:\n        print(f\"[LLM Error] Gọi API thất bại: {e}\")\n        # Đoạn code dưới đây giúp bạn xem mình được quyền dùng model nào\n        # Chỉ chạy khi debug để biết tên model đúng\n        try:\n            print(\"Danh sách model khả dụng với Key của bạn:\")\n            for m in genai.list_models():\n                if 'generateContent' in m.supported_generation_methods:\n                    print(f\"- {m.name}\")\n        except:\n            pass\n        print(\"-> Chuyển sang chế độ MOCK response.\")\n\n    # --- MOCK RESPONSE (Fallback) ---\n    print(\"[DEBUG] Đang chạy chế độ MOCK (Fallback)...\")\n    time.sleep(1.0)\n\n    # MOCK response (sử dụng retrieved_content trực tiếp + article_snippet)\n    time.sleep(0.6)\n    mock = (\n        f\"Chào bạn, bác sĩ chuyên khoa {specialty} trả lời:\\n\\n\"\n        f\"{retrieved_content}\\n\\n\"\n    )\n    if article_snippet:\n        mock += f\"--- Bài viết tham khảo ---\\n{article_snippet}\\n\\n\"\n    mock += f\"Câu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa {specialty}.\"\n    return mock\n\n# =========================\n# 8. Vòng lặp chính (kết hợp article retrieval)\n# =========================\nFALLBACK = \"Xin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo. Vui lòng đi khám trực tiếp.\"\n\nprint(\"\\n=== Chatbot RAG (PhoBERT Retrieval + Article retrieval + LLM) ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\n[CHATBOT] Kết thúc phiên.\")\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        print(\"\\n[CHATBOT] Kết thúc phiên.\")\n        break\n\n    print(\"\\n[STEP 1] Retrieval Q/A (top-5)\")\n    topk_results = retrieve_topk_qa(user_input, k=5, question_sim_thresh=0.55)\n\n    # ref_specialty mapping (1..k)\n    ref_specialty = {}\n    for i, r in enumerate(topk_results, start=1):\n        ref_specialty[i] = r['row'].get('topic', 'Y tế chung')\n\n    print(\"\\n[STEP 2] Extraction (trích xuất câu hành động từ top-k Q/A)\")\n    raw_advice_text, matched_ref_id, orig_qa = find_best_action_sentence_by_embedding_combined(\n        user_input, topk_results, ref_specialty\n    )\n\n    print(\"\\n[STEP 3] Article retrieval (tìm bài viết liên quan nhất)\")\n    article_meta = None\n    top_articles = retrieve_top_article(query_text=user_input,\n                                        k=1,\n                                        raw_k_multiplier=3,\n                                        article_texts_local=article_texts,\n                                        article_index_local=article_index,\n                                        min_combined_score=0.68)\n    \n    article_snippet = \"\"\n    if top_articles:\n        best_art = top_articles[0]\n        # lưu metadata để dùng lại sau này (in nguyên văn)\n        article_meta = best_art\n\n        # lấy score một cách an toàn với fallback\n        art_score = best_art.get(\"score\",\n                                 best_art.get(\"combined_score\",\n                                              best_art.get(\"baseline_sim\", None)))\n\n        # Nếu không có score, in debug toàn bộ object để kiểm tra schema\n        if art_score is None:\n            print(\"[MAIN][WARN] Article matched but no 'score'/'combined_score'/'baseline_sim' field found. Full object:\")\n            print(best_art)\n            art_score = \"N/A\"\n\n        print(\"[MAIN] Article matched:\", best_art.get(\"title\", \"(no title)\"), \"| score:\", art_score)\n\n        # best_passage là đoạn ngắn nhất phù hợp (đã chọn bằng sim passage) — dùng .get để tránh KeyError\n        article_snippet = f\"Title: {best_art.get('title','')}\\nLink: {best_art.get('link','')}\\n\\n{best_art.get('best_passage','')}\"\n    else:\n        article_snippet = \"\"\n        article_meta = None\n        print(\"[MAIN] Không tìm thấy article phù hợp.\")\n\n    # Nếu tìm được raw_advice_text -> gọi LLM để tổng hợp, kèm article_snippet\n    if raw_advice_text:\n        spec = ref_specialty.get(matched_ref_id, \"Y tế chung\")\n        print(f\"\\n[STEP 4] GỌI LLM để tổng hợp (chuyên khoa đề xuất: {spec})\")\n        final_response = generate_natural_response(user_input, raw_advice_text, spec, article_snippet)\n\n        # In kết quả: theo yêu cầu, đảm bảo chứa 3 phần: Chuyên khoa / Lời khuyên / Tham khảo (nguyên văn)\n        print(\"\\n\" + \"*\"*10 + \" CHATBOT TRẢ LỜI \" + \"*\"*10)\n        # Cố gắng chuẩn hóa đầu ra: nếu LLM trả văn bản đầy đủ, in nguyên; thêm phần \"Tham khảo\" rõ ràng\n        print(final_response)\n        print(\"\\n\" + \"-\"*40)\n        # In tham khảo nguyên văn Q/A và article\n        print(\"[THAM KHẢO NGUYÊN VĂN - Q/A]\" )\n        print(orig_qa or \"(không có)\")\n        if article_meta is not None:\n            print(\"\\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\")\n            print(f\"Title: {article_meta.get('title','')}\")\n            print(f\"Link: {article_meta.get('link','')}\")\n            print(article_meta.get('txt','')[:4000])  # in tối đa 4000 ký tự để tránh quá dài\n        else: \n            print(\"\\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\\nKhông có bài viết liên quan!\")\n        print(\"-\"*40 + \"\\n\")\n        continue\n\n    # Nếu không có raw_advice_text -> vẫn thử dùng article để gợi ý cho LLM (tổng hợp từ bài viết)\n    if article_snippet:\n        print(\"[FALLBACK] Không tìm thấy câu hành động trong Q/A, thử tổng hợp từ bài viết liên quan...\")\n        spec_article = \"Y tế chung\"\n        if article_meta:\n            # cố gắng dự đoán chuyên khoa từ title hoặc leave default\n            spec_article = article_meta.get(\"title\",\"Y tế chung\").split()[0]\n        final_response = generate_natural_response(user_input, article_snippet, spec_article, article_snippet)\n        print(\"\\n\" + \"*\"*10 + \" CHATBOT TRẢ LỜI (dựa trên bài viết) \" + \"*\"*10)\n        print(final_response)\n        print(\"\\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\")\n        print(article_snippet[:8000])\n        print(\"-\"*40 + \"\\n\")\n        continue\n\n    # Nếu không tìm gì -> fallback\n    print(\"\\nChatbot: \" + FALLBACK + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T18:50:28.165715Z","iopub.execute_input":"2025-12-04T18:50:28.166055Z","iopub.status.idle":"2025-12-04T18:51:45.925882Z","shell.execute_reply.started":"2025-12-04T18:50:28.166020Z","shell.execute_reply":"2025-12-04T18:51:45.925062Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n[DATA] Đã đọc Q/A từ: /kaggle/input/bacsituvan/bacsituvan.csv (bản ghi: 73)\n[ARTICLES] Đã đọc articles từ: /kaggle/input/articles/bloomax.csv (bản ghi: 378)\n[MODEL] Loading retrieval model vinai/phobert-base ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[INDEX] Chuẩn bị văn bản cho index...\n[INDEX] Tạo embeddings cho Q/A ...\n[INDEX] Embedded QA 0/73\n[INDEX] QA index built. Num elements: 73\n[INDEX] Tạo embeddings cho Articles ...\n[INDEX] Embedded Articles 0/378\n[INDEX] Embedded Articles 100/378\n[INDEX] Embedded Articles 200/378\n[INDEX] Embedded Articles 300/378\n[INDEX] Article index built. Num elements: 378\n\n=== Chatbot RAG (PhoBERT Retrieval + Article retrieval + LLM) ===\nNhập 'Kết thúc' để dừng.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Người dùng:  e năm nay 26 tuổi bị lõm lồng ngực bẩm sinh, thường hay bị hai bên cạnh sườn, cảm giác mỏi ở vùng ức và có cảm giác khó chịu khó tả. \n"},{"name":"stdout","text":"\n[STEP 1] Retrieval Q/A (top-5)\n[RETRIEVE_QA] Query: 'e năm nay 26 tuổi bị lõm lồng ngực bẩm sinh, thường hay bị hai bên cạnh sườn, cả' | k=5 | question_sim_thresh=0.55\n[RETRIEVE_QA] Found 5 candidates.\n\n[STEP 2] Extraction (trích xuất câu hành động từ top-k Q/A)\n\n=== [DEBUG] RUN find_best_action_sentence_by_embedding_combined ===\n[DEBUG] ref_pos=1 | kept_sentences=11\n[DEBUG] ref_pos=2 | kept_sentences=25\n[DEBUG] ref_pos=3 | kept_sentences=17\n[DEBUG] ref_pos=4 | kept_sentences=10\n[DEBUG] ref_pos=5 | kept_sentences=17\n\n=== [DEBUG] TOP candidates sorted by combined score ===\n[TOP1] combined=0.7868 | sim_sent=0.8367 | sim_q=0.7813 | ref=5 | 'Chào bạn , Theo mô_tả , bạn đã bị cùng một lúc mấy vấn_đề là loãng xương , thoái...'\n[TOP2] combined=0.7601 | sim_sent=0.8013 | sim_q=0.7813 | ref=5 | 'Người bị gai đôi cột_sống có_thể bị đau mạn_tính , thỉnh_thoảng có những đợt đau...'\n[TOP3] combined=0.7570 | sim_sent=0.7992 | sim_q=0.7750 | ref=2 | 'Thoái_hóa cột sống cổ gây chèn_ép dây thần_kinh và các tổ_chức xung_quanh , gây ...'\n[TOP4] combined=0.7546 | sim_sent=0.7762 | sim_q=0.8222 | ref=3 | 'Biểu_hiện là có sự xuất_hiện các vết lõm trên lồng_ngực , do các xương sường và ...'\n[TOP5] combined=0.7476 | sim_sent=0.7835 | sim_q=0.7813 | ref=5 | 'Loãng xương có rất ít biểu_hiện , có_thể sẽ gây đau mỏi chân dọc xương dài ....'\n[TOP6] combined=0.7399 | sim_sent=0.7752 | sim_q=0.7813 | ref=5 | 'đầu_gối thường_xuyên đau_nhức và kêu lục_cục là do thoái_hóa khớp và bệnh loãng ...'\n[TOP7] combined=0.7385 | sim_sent=0.7586 | sim_q=0.8225 | ref=4 | 'Chào_cháu , theo như bạn mô_tả , bạn bị đau có_thể do vận_động nhiều , do đi già...'\n[TOP8] combined=0.7369 | sim_sent=0.7732 | sim_q=0.7750 | ref=2 | 'Thoái_hóa khớp gối có_thể đau hớp gối 2 bên , đau tăng lên khi vận_động ....'\n[TOP9] combined=0.7364 | sim_sent=0.7675 | sim_q=0.7813 | ref=5 | 'Một nghiên_cứu chỉ ra rằng , trong số những bệnh_nhân bị đau cột_sống , nếu có g...'\n[TOP10] combined=0.7320 | sim_sent=0.7584 | sim_q=0.7813 | ref=5 | 'Gai đôi cột_sống là một dị_tật bẩm_sinh của cột_sống , vị_trí hay gặp là vùng th...'\n[TOP11] combined=0.7312 | sim_sent=0.7677 | sim_q=0.7750 | ref=2 | 'Một điểm lưu_ý khi vận_động trong thoái_hóa khớp là nếu vận_động không đúng thì ...'\n[TOP12] combined=0.7288 | sim_sent=0.7420 | sim_q=0.8300 | ref=1 | 'Ống tai rất gần nền sọ , nên khi bị viêm tại giữa có_thể gây viêm tai xương chũm...'\n[TOP13] combined=0.7220 | sim_sent=0.7377 | sim_q=0.8225 | ref=4 | 'Ngoài_ra nguyên_nhân gây đau có_thể do bệnh_lý như viêm cân gan chân hoặc gai gó...'\n[TOP14] combined=0.7170 | sim_sent=0.7141 | sim_q=0.8222 | ref=3 | 'Bạn đã 26 tuổi rồi , phẫu_thuật có_lẽ là hơi muộn , cũng có_thể bạn bị thể nhẹ ,...'\n[TOP15] combined=0.7166 | sim_sent=0.7442 | sim_q=0.7750 | ref=2 | 'của đau trong thoái_hóa khớp là đau có tính_chất cơ chọc , đau tăng lên khi vận_...'\n[DEBUG] Best reference found: ref_pos=4 (combined=0.7385, sim_sent=0.7586)\n[DEBUG] Selected (ref 4): [0.7385] @ Phạm_Bình_Bình : Chào_cháu , theo như bạn mô_tả , bạn bị đau có_thể do vận_độn\n[DEBUG] Selected (ref 4): [0.6846] Đồng_thời để giảm những cơn đau chân , bạn nên dùng sản_phẩm Vindermen_Plus ngày\n\n[STEP 3] Article retrieval (tìm bài viết liên quan nhất)\n[RETRIEVE_ART_OPT] Query article for: 'e năm nay 26 tuổi bị lõm lồng ngực bẩm sinh, thường hay bị hai bên cạnh sườn, cảm giác mỏi ở vùng ức và có cảm giác khó ' | k=1\n[RETRIEVE_ART_OPT] raw candidates from HNSW: 3\n[RETRIEVE_ART_OPT] reranked top candidates count: 3\n[RE-RANK TOP1] idx=53 | title='Những ai có thể mắc bệnh võng mạc tiểu đường?' | combined=0.6178 | baseline=0.7263 | lex=0.115 | title_boost=1.0 | best_passage_sim=0.6719\n[RE-RANK TOP2] idx=60 | title='Ứng phó với bệnh đau cổ vai gáy như thế nào?' | combined=0.5885 | baseline=0.7231 | lex=0.231 | title_boost=0.0 | best_passage_sim=0.8230\n[RE-RANK TOP3] idx=14 | title='Ung thư vòm họng là gì?' | combined=0.5852 | baseline=0.7393 | lex=0.154 | title_boost=0.0 | best_passage_sim=0.7339\n[MAIN] Không tìm thấy article phù hợp.\n\n[STEP 4] GỌI LLM để tổng hợp (chuyên khoa đề xuất: Y tế chung)\n\n========== [DEBUG] LLM PROMPT ==========\n\nBạn là một bác sĩ tư vấn trực tuyến chuyên khoa Y tế chung.\nThông tin tham khảo Q/A trích xuất:\n@ Phạm_Bình_Bình : Chào_cháu , theo như bạn mô_tả , bạn bị đau có_thể do vận_động nhiều , do đi giày chật khiến máu kém lưu_thông gây đau . Đồng_thời để giảm những cơn đau chân , bạn nên dùng sản_phẩm Vindermen_Plus ngày 2 viên chia 2 lần , duy_trì 3 - 6 tháng , giúp tăng sự dẫn_truyền thần_kinh , phục_hồi hư tổn tại sụn khớp , giúp giảm đau nhức hiệu_quả .\n\nBài viết tham khảo liên quan:\n\n\nYêu cầu:\n1) Trả lời câu: \"e năm nay 26 tuổi bị lõm lồng ngực bẩm sinh, thường hay bị hai bên cạnh sườn, cảm giác mỏi ở vùng ức và có cảm giác khó chịu khó tả. \" dựa trên thông tin trên.\n2) Diễn đạt tự nhiên, xưng hô Bác sĩ - bạn.\n3) Nếu nội dung Q/A tham khảo được trích xuất hoàn toàn KHÔNG liên quan với câu \"e năm nay 26 tuổi bị lõm lồng ngực bẩm sinh, thường hay bị hai bên cạnh sườn, cảm giác mỏi ở vùng ức và có cảm giác khó chịu khó tả. \" thì trả lời: Xin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo. Vui lòng đi khám trực tiếp. \n3) Kết thúc nhắc: \"Câu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa Y tế chung.\"\n\n========================================\n\n\n********** CHATBOT TRẢ LỜI **********\nXin chào bạn,\n\nXin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo để trả lời câu hỏi của bạn về tình trạng lõm lồng ngực bẩm sinh và các triệu chứng bạn đang gặp phải. Với tình trạng này, Bác sĩ khuyên bạn nên đi khám trực tiếp tại chuyên khoa Y tế chung hoặc chuyên khoa lồng ngực để được thăm khám và tư vấn cụ thể hơn.\n\nCâu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa Y tế chung.\n\n\n----------------------------------------\n[THAM KHẢO NGUYÊN VĂN - Q/A]\nQ: Cháu là nam ( 18 t ) , hay chạy bộ nhưng dạo gần đây cứ đau chân , nhưng không phải đau kiểu chạy nhiều mà đau . Khi cháu chạy , bàn_chân trái bị hơi tê ở gót_chân và bàn_chân ( cháu nghĩ cái này là bình_thường khi chạy bộ ) , nhưng chân phải cháu bị đau không chỉ vậy mà còn bị đau nguyên khớp dọc từ mắt_cá chân đi lên một khúc và dưới đó một_chút ._Bs cho cháu hỏi như thế là sao ạ , vì khi cháu chạy liên_tục một khoảng thời_gian thì chân phải trở_nên đau_nhói và không_thể dậm chân phải xuống đất được và thường phải co lên ?\nA: @ Phạm_Bình_Bình : Chào_cháu , theo như cháu mô_tả , cháu bị đau có_thể do vận_động nhiều , do đi giày chật khiến máu kém lưu_thông gây đau . Ngoài_ra nguyên_nhân gây đau có_thể do bệnh_lý như viêm cân gan chân hoặc gai gót chân . Tốt nhất cháu nên đi khám tại các bệnh_viện chuyên_khoa xương khớp để xác_định nguyên_nhân và có hướng điều_trị phù_hợp . Đồng_thời để giảm những cơn đau chân , cháu nên dùng sản_phẩm Vindermen_Plus ngày 2 viên chia 2 lần , duy_trì 3 - 6 tháng , giúp tăng sự dẫn_truyền thần_kinh , phục_hồi hư tổn tại sụn khớp , giúp giảm đau nhức hiệu_quả . Kết_hợp với sản_phẩm Vipteen ngày 4 viên chia 2 lần cung_cấp các thành_phần : MK7 , Canxi nano , Zn nano , Magie , Vitamin_D3 , ... có tác_dụng bổ_sung vi_chất cần_thiết cho sự phát_triển của khung xương , giúp cháu có được chiều cao tối_ưu , và giúp xương chắc khỏe , ngăn_ngừa các bệnh về xương khớp . Bên_cạnh đó trong thời_gian này cháu nên nghỉ_ngơi . Tránh đứng ngồi ở một tư_thế quá lâu , hoặc vận_động , chạy_nhảy nhiều , đi giầy đúng kích_cỡ chân . Chườm đá 20 phút từ 3-4 lần mỗi ngày để giảm các cơn đau gót chân . Chúc cháu sức_khỏe !\n\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\nKhông có bài viết liên quan!\n----------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Người dùng:  Kết thúc\n"},{"name":"stdout","text":"\n[CHATBOT] Kết thúc phiên.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## [NO DEBUG] pho_retrieval_textonly_actionfiltered_final_with_articles.py","metadata":{}},{"cell_type":"code","source":"# pho_retrieval_textonly_actionfiltered_final_with_articles.py\n# ===========================================================\n# Pipeline RAG:\n# - Retrieval: PhoBERT embeddings cho corpus Q/A + index bài viết (link,title,txt)\n# - Extraction: tìm câu \"hành động\" tốt nhất từ top-k Q/A (combined score + lex overlap)\n# - Article retrieval: tìm bài viết liên quan nhất và trả nguyên văn trong mục \"Tham khảo\"\n# - Generation: tích hợp LLM (mock nếu không có API) để viết lại câu trả lời tự nhiên\n# Output: PLAIN TEXT (lời khuyên, chuyên khoa, tham khảo nguyên văn)\n# ===========================================================\n\nimport re\nimport time\nimport threading\nfrom html import unescape\nfrom typing import List, Tuple, Optional\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom underthesea import word_tokenize\nimport hnswlib\n\n# =========================\n# 0. Cấu hình thiết bị & môi trường\n# =========================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# =========================\n# 1. Đọc CSV Q/A (corpus) và tiền xử lý\n#    - file Q/A theo cấu trúc: question, answer, topic (có thể đặt khác => chỉnh csv_path)\n# =========================\ncsv_path = \"/kaggle/input/filtered-question-answers/filtered-question-answers.json\"  # chỉnh lại nếu cần\nif not os.path.exists(csv_path):\n    csv_path = \"/mnt/data/filtered-question-answers.csv\"\nif os.path.exists(csv_path):\n    df = pd.read_csv(csv_path)\n    print(f\"[DATA] Đã đọc Q/A từ: {csv_path} (bản ghi: {len(df)})\")\nelse:\n    print(f\"[DATA] Không tìm thấy Q/A file tại {csv_path}. Tạo dataframe rỗng.\")\n    df = pd.DataFrame(columns=[\"question\", \"answer\", \"topic\", \"advice\"])\n\ndef preprocess_text(s: str) -> str:\n    if not isinstance(s, str):\n        return \"\"\n    s = re.sub(r'\\s+', ' ', s.strip())\n    return word_tokenize(s, format=\"text\")\n\nfor col in [\"question\", \"answer\", \"topic\", \"advice\"]:\n    if col in df.columns:\n        df[col] = df[col].fillna(\"\").astype(str).apply(preprocess_text)\n\n# =========================\n# 2. Đọc file bài viết (articles) gồm các cột: link, title, txt\n#    - Mặc định tìm ở /mnt/data/articles.csv hoặc /kaggle/working/articles.csv\n# =========================\narticles_paths = [\"/kaggle/input/articles/bloomax.csv\"]\narticles_df = None\nfor p in articles_paths:\n    if os.path.exists(p):\n        try:\n            articles_df = pd.read_csv(p)\n            print(f\"[ARTICLES] Đã đọc articles từ: {p} (bản ghi: {len(articles_df)})\")\n            break\n        except Exception as e:\n            print(f\"[ARTICLES] Lỗi đọc {p}: {e}\")\nif articles_df is None:\n    # tạo dataframe rỗng để pipeline không lỗi\n    print(\"[ARTICLES] Không tìm thấy file bài viết. Article index sẽ rỗng.\")\n    articles_df = pd.DataFrame(columns=[\"link\", \"title\", \"txt\"])\n\nfor col in [\"link\", \"title\", \"txt\"]:\n    if col in articles_df.columns:\n        articles_df[col] = articles_df[col].fillna(\"\").astype(str).apply(lambda s: re.sub(r'\\s+', ' ', s.strip()))\n\n# =========================\n# 3. Load PhoBERT (retrieval encoder)\n# =========================\nRETRIEVAL_MODEL_NAME = \"vinai/phobert-base\"\nprint(f\"[MODEL] Loading retrieval model {RETRIEVAL_MODEL_NAME} ...\")\ntokenizer_phobert = AutoTokenizer.from_pretrained(RETRIEVAL_MODEL_NAME)\nmodel_phobert = AutoModel.from_pretrained(RETRIEVAL_MODEL_NAME).to(device)\nmodel_phobert.eval()\n\ndef sentence_embedding(text: str) -> np.ndarray:\n    \"\"\"\n    Embedding bằng PhoBERT: mean-pooling trên last_hidden_state.\n    Trả về numpy float32 vector.\n    \"\"\"\n    if not text:\n        return np.zeros(model_phobert.config.hidden_size, dtype=np.float32)\n    inputs = tokenizer_phobert(text, return_tensors=\"pt\", truncation=True, max_length=256)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        out = model_phobert(**inputs)\n        last_hidden = out.last_hidden_state  # (1, seq_len, hidden)\n        att = inputs.get(\"attention_mask\", None)\n        if att is None:\n            mean_pooled = last_hidden.mean(dim=1).squeeze().cpu().numpy().astype(\"float32\")\n        else:\n            attention_mask = att.unsqueeze(-1)\n            masked = last_hidden * attention_mask\n            summed = masked.sum(dim=1)\n            counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n            mean_pooled = (summed / counts).squeeze().cpu().numpy().astype(\"float32\")\n    return mean_pooled\n\n# =========================\n# 4. Tạo embeddings cho Q/A corpus (document-level) và bài viết (article-level)\n#    - Q/A: dùng texts = question + \" \" + answer\n#    - Articles: dùng title + \"\\n\\n\" + txt\n# =========================\nprint(\"[INDEX] Chuẩn bị văn bản cho index...\")\n\nqa_texts = (df[\"question\"].fillna(\"\") + \" \" + df[\"answer\"].fillna(\"\")).tolist() if not df.empty else []\narticle_texts = []\nif not articles_df.empty:\n    # combine title + content\n    for _, row in articles_df.iterrows():\n        title = row.get(\"title\", \"\") or \"\"\n        txt = row.get(\"txt\", \"\") or \"\"\n        article_texts.append((row.get(\"link\", \"\"), title, title + \"\\n\\n\" + txt))\nelse:\n    article_texts = []\n\n# Build embeddings with batching and build two indices\ndef build_hnsw_index(vectors: np.ndarray, space: str = \"cosine\") -> hnswlib.Index:\n    dim = vectors.shape[1]\n    idx = hnswlib.Index(space=space, dim=dim)\n    idx.init_index(max_elements=vectors.shape[0], ef_construction=200, M=16)\n    ids = np.arange(vectors.shape[0])\n    idx.add_items(vectors, ids)\n    idx.set_ef(50)\n    return idx\n\n# Build QA embeddings\nqa_embeddings = []\nbatch_size = 32\nfor i in range(0, len(qa_texts), batch_size):\n    for t in qa_texts[i:i+batch_size]:\n        qa_embeddings.append(sentence_embedding(t))\n    if i % 200 == 0:\n        print(f\"[INDEX] Embedded QA {i}/{len(qa_texts)}\")\nif qa_embeddings:\n    qa_embeddings = np.vstack(qa_embeddings).astype(\"float32\")\n    qa_index = build_hnsw_index(qa_embeddings)\n    print(f\"[INDEX] QA index built. Num elements: {qa_index.get_current_count()}\")\nelse:\n    qa_embeddings = np.zeros((0, model_phobert.config.hidden_size), dtype=\"float32\")\n    qa_index = None\n\n# Build Article embeddings\narticle_embeddings = []\nfor i, (_, title, content) in enumerate(article_texts):\n    article_embeddings.append(sentence_embedding(content))\nif article_embeddings:\n    article_embeddings = np.vstack(article_embeddings).astype(\"float32\")\n    article_index = build_hnsw_index(article_embeddings)\n    print(f\"[INDEX] Article index built. Num elements: {article_index.get_current_count()}\")\nelse:\n    article_embeddings = np.zeros((0, model_phobert.config.hidden_size), dtype=\"float32\")\n    article_index = None\n    print(\"[INDEX] Article index is empty.\")\n\n# =========================\n# 5. Các hàm tiền xử lý & trợ giúp (cũ + nâng cấp)\n# =========================\ndef preprocess_query(s: str) -> str:\n    return word_tokenize(s.strip().replace(\"_\", \" \"), format=\"text\")\n\ndef cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n    if a is None or b is None:\n        return 0.0\n    na = np.linalg.norm(a)\n    nb = np.linalg.norm(b)\n    if na == 0 or nb == 0:\n        return 0.0\n    return float(np.dot(a, b) / (na * nb))\n\ndef retrieve_topk_qa(query_text: str, k: int = 5, question_sim_thresh: float = 0.55):\n    \"\"\"\n    Truy vấn QA index (document-level). Sau khi lấy raw candidates (k*3),\n    kiểm tra similarity giữa user và field 'question' của record rồi lọc.\n    Trả về list tối đa k item giống định dạng trước đó.\n    \"\"\"\n    if qa_index is None or qa_index.get_current_count() == 0:\n        return []\n    q_proc = preprocess_query(query_text)\n    user_emb = sentence_embedding(q_proc).reshape(1, -1)\n    raw_k = min(k * 3, qa_index.get_current_count())\n    labels, distances = qa_index.knn_query(user_emb, k=raw_k)\n    labels = labels[0]\n    distances = distances[0]\n\n    results = []\n    for dist, idx in zip(distances, labels):\n        if idx < 0:\n            continue\n        # lấy question từ df (đã tiền xử lý)\n        q_text = df.iloc[int(idx)].get(\"question\", \"\") if not df.empty else \"\"\n        if not q_text:\n            continue\n        q_emb = sentence_embedding(preprocess_query(q_text)).astype(\"float32\")\n        sim_q = cosine_sim(user_emb.reshape(-1), q_emb)\n        if sim_q >= question_sim_thresh:\n            results.append({\n                \"score\": float(1.0 - dist),\n                \"index\": int(idx),\n                \"row\": df.iloc[int(idx)].to_dict(),\n                \"question_sim\": sim_q\n            })\n        if len(results) >= k:\n            break\n    return results\n\n# --------- BẮT ĐẦU: hàm / utils mới cho article retrieval & re-rank ----------\nimport math\n\ndef _token_set(s: str):\n    \"\"\"Đơn giản: lấy token chữ/ số, lowercase. Dùng cho lexical overlap.\"\"\"\n    return set([t.lower() for t in re.findall(r'\\w+', s) if len(t) >= 2])\n\ndef chunk_text_into_passages(text: str, max_chars: int = 500, overlap_chars: int = 80):\n    \"\"\"\n    Chia bài thành các đoạn (passages) kích thước ~max_chars với overlap.\n    Trả list các đoạn (nguyên văn).\n    \"\"\"\n    if not text:\n        return []\n    text = text.strip()\n    passages = []\n    start = 0\n    L = len(text)\n    while start < L:\n        end = start + max_chars\n        if end >= L:\n            passages.append(text[start:L].strip())\n            break\n        # cố gắng cắt ở dấu câu gần end để dễ đọc\n        cut = text.rfind('.', start, end)\n        if cut <= start:\n            cut = text.rfind('\\n', start, end)\n        if cut <= start:\n            cut = end\n        passages.append(text[start:cut].strip())\n        start = max(cut - overlap_chars, cut)  # overlap một chút\n    return [p for p in passages if p]\n\ndef re_rank_article_candidates(user_emb: np.ndarray, q_tokens_set: set, raw_candidates: List[dict],\n                               article_texts_local: List[tuple],\n                               topn_return: int = 3,\n                               w_sim: float = 0.75, w_lex: float = 0.20, w_title_boost: float = 0.05):\n    \"\"\"\n    Re-rank raw candidate list (từ hnswlib) bằng combined score:\n      combined = w_sim * sim_article + w_lex * lex_overlap + w_title_boost * title_boost_flag\n    Trả về danh sách các candidate đã bổ sung trường 'combined_score' và 'best_passage'.\n    - raw_candidates: list các item giống format trước: {'score':..., 'index': idx, ...}\n    - article_texts_local: list of tuples (link, title, content)\n    \"\"\"\n    reranked = []\n    for c in raw_candidates:\n        idx = int(c['index'])\n        link, title, content = article_texts_local[idx]\n        # 1) sim_article: nếu bạn muốn chính xác, hãy tính cosine giữa user_emb và article embedding\n        #    nhưng ở đây raw_candidates cung cấp 'score' = 1-dist; vẫn tốt để dùng lại như baseline_sim\n        baseline_sim = float(c.get('score', 0.0))\n        # 2) lexical overlap: tokens in (title + first 1000 chars of content)\n        article_snippet_for_tokens = title + \" \" + (content[:1000] if content else \"\")\n        art_tokens = _token_set(article_snippet_for_tokens)\n        if not q_tokens_set:\n            lex_overlap = 0.0\n        else:\n            common = len(q_tokens_set & art_tokens)\n            lex_overlap = common / max(1, len(q_tokens_set))\n        # 3) title_boost: nếu tiêu đề chứa >=1 token của query -> 1 else 0\n        title_tokens = _token_set(title)\n        title_boost_flag = 1.0 if (q_tokens_set & title_tokens) else 0.0\n\n        combined = w_sim * baseline_sim + w_lex * lex_overlap + w_title_boost * title_boost_flag\n\n        # 4) best_passage: tìm passage có sim cao nhất (chi tiết hơn)\n        passages = chunk_text_into_passages(content, max_chars=600, overlap_chars=120)\n        best_passage = \"\"\n        best_passage_sim = -1.0\n        # compute embedding for a few top passages (limiting để không quá chậm)\n        for p in passages[:6]:  # chỉ check tối đa 6 đoạn đầu để tiết kiệm time\n            p_proc = preprocess_query(p)\n            p_emb = sentence_embedding(p_proc).astype(\"float32\")\n            sim_p = cosine_sim(user_emb, p_emb)\n            if sim_p > best_passage_sim:\n                best_passage_sim = sim_p\n                best_passage = p\n        # fallback: nếu không có passage, lấy đoạn đầu content\n        if not best_passage and content:\n            best_passage = content[:600]\n\n        reranked.append({\n            \"index\": idx,\n            \"link\": link,\n            \"title\": title,\n            \"txt\": content,\n            \"baseline_sim\": baseline_sim,\n            \"lex_overlap\": lex_overlap,\n            \"title_boost\": title_boost_flag,\n            \"combined_score\": combined,\n            \"best_passage\": best_passage,\n            \"best_passage_sim\": best_passage_sim\n        })\n\n    # Sắp xếp giảm dần theo combined_score, trả top-N\n    reranked_sorted = sorted(reranked, key=lambda x: x[\"combined_score\"], reverse=True)\n    return reranked_sorted[:topn_return]\n\n\ndef retrieve_top_article(query_text: str, k: int = 1, raw_k_multiplier: int = 3,\n                         article_texts_local: List[tuple] = None,\n                         article_index_local = None,\n                         min_combined_score: float = 0.25):\n    \"\"\"\n    HÀM CHÍNH (thay thế):\n    1) Lấy raw candidates từ article_index_local (hnswlib)\n    2) Re-rank bằng re_rank_article_candidates\n    3) Trả về top-k articles với trường 'combined_score' + 'best_passage'\n    \"\"\"\n    if article_index_local is None or article_index_local.get_current_count() == 0:\n        return []\n\n    q_proc = preprocess_query(query_text)\n    user_emb = sentence_embedding(q_proc).reshape(1, -1).astype(\"float32\")\n    q_tokens = _token_set(query_text)\n\n    raw_k = min(k * raw_k_multiplier, article_index_local.get_current_count())\n    # 1) knn query lấy raw_k candidate\n    labels, distances = article_index_local.knn_query(user_emb, k=raw_k)\n    labels = labels[0]\n    distances = distances[0]\n\n    raw_candidates = []\n    for dist, idx in zip(distances, labels):\n        if idx < 0:\n            continue\n        raw_candidates.append({\n            \"index\": int(idx),\n            \"score\": float(1.0 - dist)  # baseline sim heuristic\n        })\n    # 2) Re-rank candidates bằng combined score\n    reranked = re_rank_article_candidates(user_emb.reshape(-1), q_tokens, raw_candidates,\n                                          article_texts_local, topn_return=max(k, 3))\n    # 3) Filter theo min_combined_score nếu cần\n    final = [r for r in reranked if r[\"combined_score\"] >= min_combined_score]\n    if not final:\n        # Nếu không có ai vượt ngưỡng, trả top 1 reranked (một fallback)\n        # if reranked:\n        #    print(\"[RETRIEVE_ART_OPT] Không có article đạt ngưỡng -> trả top1 reranked như fallback\")\n        #    return [ {**reranked[0], \"note\": \"fallback_no_threshold\"} ]\n        return []\n\n    # Trả tối đa k items, convert field names giống format cũ (score, index, link, title, txt)\n    out = []\n    for r in final[:k]:\n        out.append({\n            \"score\": r[\"combined_score\"],\n            \"index\": r[\"index\"],\n            \"link\": r[\"link\"],\n            \"title\": r[\"title\"],\n            \"txt\": r[\"txt\"],\n            \"best_passage\": r[\"best_passage\"],\n            \"best_passage_sim\": r[\"best_passage_sim\"]\n        })\n    return out\n\n# --------- KẾT THÚC: hàm / utils mới cho article retrieval & re-rank ----------\n\n# -------------------------\n# text cleaning utils & action verbs (giữ nguyên / mở rộng)\n# -------------------------\ndef _clean_text(t: str) -> str:\n    return re.sub(r'\\s+', ' ', unescape(t.strip())).strip()\n\n_re_at_prefix = re.compile(r'^@[^:]{0,60}:\\s*', flags=re.IGNORECASE)\n_name_pattern = re.compile(r'\\b([A-ZÀ-Ỹ][a-zà-ỹ]+(?:_[A-ZÀ-Ỹ][a-zà-ỹ]+)+)\\b')\n_doctor_pattern = re.compile(r'\\b(BS|Bác sĩ|Lương y|Dr)\\.?\\s+([A-ZÀ-Ỹ][a-zà-ỹ_]+(\\s+[A-ZÀ-Ỹ][a-zà-ỹ_]+)*)', flags=re.IGNORECASE)\n_pronoun_pattern = re.compile(r'\\b(cháu|em|tớ|mình|con|anh|chị)\\b', flags=re.IGNORECASE)\nCONNECTIVES = [\n    r'vì vậy', r'vì thế', r'vậy nên', r'do vậy', r'vì vậy nên', r'vì thế nên', r'cho nên',\n    r'tóm lại', r'tóm tắt', r'nhưng', r'tuy nhiên'\n]\n_connective_pattern = re.compile(\"|\".join([re.escape(x) for x in CONNECTIVES]), flags=re.IGNORECASE)\n\ndef preprocess_reference_sentence_for_embedding(s: str) -> str:\n    \"\"\"\n    Tiền xử lý câu TRƯỚC khi tính embedding/score:\n    - Loại bỏ câu hỏi (ending ?)\n    - Loại bỏ prefix @..., 'Trả lời'\n    - Loại bỏ tên riêng, danh xưng\n    - Chuẩn hóa đại từ -> 'bạn'\n    - Loại bỏ connectives\n    \"\"\"\n    if not s:\n        return \"\"\n    s = s.strip()\n    if s.endswith('?'):\n        return \"\"\n    s = _re_at_prefix.sub(\"\", s)\n    s = re.sub(r'^trả[_\\s]lời\\s*[:.]?\\s*', '', s, flags=re.IGNORECASE)\n    s = _name_pattern.sub(\"\", s)\n    s = _doctor_pattern.sub(\"\", s)\n    s = _pronoun_pattern.sub(\"bạn\", s)\n    s = _connective_pattern.sub(\"\", s)\n    s = re.sub(r'\\s+', ' ', s).strip()\n    return s\n\n# ACTION_VERBS: giữ phiên bản mở rộng như trước (cắt ngắn ở đây, dùng list dài trong code thật)\nACTION_VERBS = set([\n    # Nhóm dùng thuốc / điều trị\n    \"uống\", \"uống thuốc\", \"dùng\", \"dùng thuốc\", \"xịt\", \"bôi\", \"thoa\", \"nhỏ\", \"ngậm\", \n    \"tiêm\", \"chích\", \"truyền\", \"phẫu thuật\", \"mổ\", \"tiểu phẫu\", \"kê đơn\", \"điều trị\",\n    \"chườm\", \"chườm nóng\", \"chườm lạnh\", \"băng bó\", \"sát trùng\", \"rửa vết thương\",\n    \"hút rửa\", \"xông\", \"khí dung\", \"châm cứu\", \"bấm huyệt\", \"massage\", \"xoa bóp\",\n    \n    # Nhóm khám / xét nghiệm\n    \"khám\", \"đi khám\", \"tái khám\", \"thăm khám\", \"kiểm tra\", \"xét nghiệm\", \"lấy mẫu\",\n    \"siêu âm\", \"chụp\", \"chụp x-quang\", \"chụp ct\", \"chụp mri\", \"nội soi\", \"đo huyết áp\",\n    \"đo đường huyết\", \"theo dõi\", \"đánh giá\", \"tầm soát\",\n    \n    # Nhóm sinh hoạt / dinh dưỡng\n    \"ăn\", \"ăn kiêng\", \"kiêng\", \"tránh\", \"hạn chế\", \"bổ sung\", \"tăng cường\", \"giảm\",\n    \"uống nước\", \"ngủ\", \"nghỉ ngơi\", \"kê gối\", \"nằm nghiêng\", \"tập\", \"tập luyện\", \n    \"vận động\", \"tập vật lý trị liệu\", \"thể dục\", \"vệ sinh\", \"súc miệng\", \"súc họng\",\n    \"rửa tay\", \"rửa mũi\", \"đeo khẩu trang\", \"cách ly\", \"nhập viện\", \"cấp cứu\",\n\n    # Bổ sung\n    'khám', 'đi', 'uống', 'ăn', 'đi khám', 'tiêm', 'siêu', 'dùng', 'siêu âm', 'nội', 'tránh', 'đặt', 'uống thuốc', 'nhỏ', 'bổ', 'khám và', 'tránh thai', 'bổ sung', 'dùng thuốc', 'khám bệnh', 'chụp', 'ăn uống', 'khám bác sĩ', 'khám sức', 'khám sức khỏe', 'khám bác', 'truyền', 'đi ngoài', 'kê', 'nội soi', 'khám tại', 'đi siêu âm', 'khám thai', 'đặt lịch', 'đi siêu', 'khám lại', 'nội tiết', 'khám để', 'hút', 'tiêm chủng', 'khám ở', 'đi khám bác', 'khám phụ khoa', 'nội mạc', 'đi khám để', 'khám chuyên khoa', 'khám phụ', 'đi khám và', 'khám chuyên', 'nội mạc tử', 'đặt lịch khám', 'tiêm ngừa', 'khám và tư', 'kiêng', 'đi lại', 'rửa', 'khám thì', 'đi tiểu', 'đi khám ở', 'nội khoa', 'đi khám lại', 'quá lo lắng', 'uống sữa', 'ăn dặm', 'uống nước', 'tránh thai khẩn', 'bôi', 'thai khẩn cấp', 'kê đơn', 'khám tại bệnh', 'khám online', 'khám bệnh viện', 'tiêm mũi', 'khám trực', 'chụp x quang', 'tiêm vacxin', 'khám và điều', 'đi xét nghiệm', 'đi khám thai', 'đi khám thì', 'đi khám chuyên', 'tiêm phòng', 'truyền nhiễm', 'uống nhiều', 'chụp x', 'cho bé đi', 'đi tái', 'đưa bé đi', 'đi xét', 'siêu âm và', 'ăn nhiều', 'khám bệnh bv', 'bổ sung vitamin', 'đi phân', 'khám tư', 'bổ sung thêm', 'uống thuốc tránh', 'uống thêm', 'đi khám phụ', 'nội soi dạ', 'khám ngay', 'đi cầu', 'uống thuốc gì', 'khám bệnh bệnh', 'đeo', 'dùng thuốc gì', 'đi ngoài phân', 'đi tái khám', 'đến bệnh viện', 'khám thai định', 'làm sao', 'khám với', 'tiêm vaccine', 'uống được', 'uống nhiều nước', 'khám ở bệnh', 'siêu âm thì', 'đi kiểm tra', 'đi tiêm', 'đi tiêu', 'đưa bé đến', 'khám trực tiếp', 'khám được', 'đặt khám', 'đi khám ngay', 'tiêm vắc xin', 'đặt tư vấn', 'khám em', 'siêu vi', 'đặt thuốc', 'khám với bác', 'tiêm vắc', 'khám để được', 'thiết', 'đi làm', 'hút thai', 'siêu âm tim', 'đi khám tại', 'uống 1', 'tiêm được', 'tránh thai hàng', 'khám không', 'ăn và', 'uống đủ', 'hút thuốc', 'dùng thuốc tránh', 'làm gì', 'đi khám bệnh', 'đi kiểm', 'siêu âm lại', 'khám định', 'thai hàng ngày', 'đến khám tại', 'đi vệ', 'đi vệ sinh', 'tiêm thuốc', 'dùng biện pháp', 'tiêm chủng chuyên', 'siêu âm thai', 'khám định kỳ', 'khám thì bác', 'kê đơn thuốc', 'ăn được', 'ăn không', 'liên hệ với', 'khám để bác', 'kê thuốc', 'uống đủ nước', 'khám tại khoa', 'đặt vòng', 'khám bệnh chuyên', 'khám hiếm muộn', 'đi khám không', 'thai', 'dùng biện', 'đặt câu', 'đặt câu hỏi', 'uống và', 'chụp mri', 'khám sớm', 'khám cho', 'uống có', 'làm gì để', 'nội tiết tố', 'uống bổ sung', 'nội tổng', 'khám và làm', 'kê toa', 'siêu âm ở', 'nội soi bóc', 'siêu âm thấy', 'khám tư vấn', 'khám hiếm', 'dùng cho', 'đi kèm', 'ăn đủ', 'ăn của', 'khám tổng', 'khám tổng quát', 'khám và siêu'\n])\n\ndef sentence_has_action(s: str) -> bool:\n    sl = s.lower()\n    for act in ACTION_VERBS:\n        act_norm = act.replace(\"_\", \" \").lower()\n        if re.search(r'\\b' + re.escape(act_norm) + r'\\b', sl):\n            return True\n    return False\n\n# =========================\n# 6. Hàm chính: tìm best action sentence (combined scoring) - GIỮ NGUYÊN, TRẢ KÈM ORIG QA\n# =========================\ndef find_best_action_sentence_by_embedding_combined(\n    user_text: str,\n    topk_rows: List[dict],\n    ref_specialty: dict,\n    sent_sim_thresh: float = 0.6,\n    combined_thresh: float = 0.68,\n    alpha: float = 0.7,\n    beta: float = 0.25,\n    gamma: float = 0.05,\n    max_debug_show: int = 15\n) -> Tuple[Optional[str], Optional[int], Optional[str]]:\n    \"\"\"\n    Như mô tả trước: trả về (final_paragraph, best_ref_pos, orig_qa)\n    final_paragraph: đoạn ghép các câu hành động từ best reference, đã chuẩn hóa đại từ (bạn)\n    orig_qa: nguyên văn Q/A (để hiển thị trong \"Tham khảo\")\n    \"\"\"\n    if not topk_rows:\n        return None, None, None\n\n    # 1) thu thập câu và lưu orig QA\n    all_sents = []\n    orig_qa_map = {}\n    for ref_pos, r in enumerate(topk_rows, start=1):\n        question_text = r[\"row\"].get(\"question\", \"\") or \"\"\n        raw_answer = r[\"row\"].get(\"answer\", \"\") or \"\"\n        orig_qa_map[ref_pos] = f\"Q: {question_text}\\nA: {raw_answer}\"\n        sents = re.split(r'(?<=[.!?])\\s+', raw_answer.strip()) if raw_answer else []\n        kept = 0\n        for s in sents:\n            s_orig = _clean_text(s)\n            s_proc = preprocess_reference_sentence_for_embedding(s_orig)\n            if len(s_proc) >= 6:\n                all_sents.append((ref_pos, question_text, s_orig, s_proc))\n                kept += 1\n\n    if not all_sents:\n        return None, None, None\n\n    # 2) embeddings user\n    user_q = preprocess_query(user_text)\n    user_emb = sentence_embedding(user_q).astype(\"float32\")\n    user_tokens_set = set([t.lower() for t in re.findall(r'\\w+', user_text) if len(t) >= 2])\n\n    # 3) compute combined scores\n    question_emb_cache = {}\n    scored = []\n    for ref_pos, question_text, sent_orig, sent_proc in all_sents:\n        if ref_pos not in question_emb_cache:\n            q_text_proc = preprocess_query(question_text) if question_text else \"\"\n            question_emb_cache[ref_pos] = sentence_embedding(q_text_proc).astype(\"float32\") if q_text_proc else np.zeros(user_emb.shape, dtype=np.float32)\n        s_emb = sentence_embedding(sent_proc).astype(\"float32\")\n        sim_sent = cosine_sim(user_emb, s_emb)\n        sim_q = cosine_sim(user_emb, question_emb_cache[ref_pos])\n        sent_tokens_set = set([t.lower() for t in re.findall(r'\\w+', sent_proc) if len(t) >= 2])\n        lex_overlap = float(len(user_tokens_set & sent_tokens_set)) / max(1, len(user_tokens_set)) if user_tokens_set else 0.0\n        combined = alpha * sim_sent + beta * sim_q + gamma * lex_overlap\n        scored.append((combined, sim_sent, sim_q, lex_overlap, sent_orig, sent_proc, ref_pos))\n\n    scored_sorted = sorted(scored, key=lambda x: x[0], reverse=True)\n\n    # 4) tìm best ref: câu đầu thỏa sim_sent & combined & có action verb\n    best_ref_pos = None\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n            if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                best_ref_pos = ref_pos\n                break\n\n    if best_ref_pos is None:\n        return None, None, None\n\n    # 5) gom các câu từ best_ref_pos\n    final_sentences_list = []\n    for combined, sim_sent, sim_q, lex, sent_orig, sent_proc, ref_pos in scored_sorted:\n        if ref_pos == best_ref_pos:\n            if sim_sent >= sent_sim_thresh and combined >= combined_thresh:\n                if sentence_has_action(sent_proc) or sentence_has_action(sent_orig):\n                    s_final = _pronoun_pattern.sub(\"bạn\", sent_orig)\n                    s_final = re.sub(r'\\s+', ' ', s_final).strip()\n                    if s_final not in final_sentences_list:\n                        final_sentences_list.append(s_final)\n\n    if not final_sentences_list:\n        return None, None, None\n\n    final_paragraph = \" \".join(final_sentences_list)\n    orig_qa = orig_qa_map.get(best_ref_pos, \"\")\n    return final_paragraph, best_ref_pos, orig_qa\n\n# =========================\n# 7. Module LLM generation (giữ nguyên mock / debug)\n# =========================\ndef generate_natural_response(user_query: str, retrieved_content: str, specialty: str, article_snippet: str = \"\") -> str:\n    \"\"\"\n    Gọi LLM (nếu có). Nếu không có, trả mock response.\n    - retrieved_content: đoạn text tổng hợp từ Q/A\n    - article_snippet: nội dung bài viết liên quan sẽ được đưa vào prompt và in ra tham khảo\n    \"\"\"\n    prompt = f\"\"\"\nBạn là một bác sĩ tư vấn trực tuyến chuyên khoa {specialty}.\nThông tin tham khảo Q/A trích xuất:\n{retrieved_content}\n\nBài viết tham khảo liên quan:\n{article_snippet}\n\nYêu cầu:\n1) Trả lời câu: \"{user_query}\" dựa trên thông tin trên.\n2) Diễn đạt tự nhiên, xưng hô Bác sĩ - bạn.\n3) Nếu nội dung Q/A tham khảo được trích xuất hoàn toàn KHÔNG liên quan với câu \"{user_query}\" thì trả lời: Xin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo. Vui lòng đi khám trực tiếp. \n3) Kết thúc nhắc: \"Câu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa {specialty}.\"\n\"\"\"\n\n    # --- SỬ DỤNG API ---\n    try:\n        import google.generativeai as genai\n        \n        API_KEY = \"AIzaSyB4kQmT9uYLt-b0mKNL4ReUs8uVAx_bCpI\" \n        \n        if API_KEY.startswith(\"DIEN_API\"):\n             # Nếu chưa điền key, nhảy xuống mock\n             raise ValueError(\"Chưa điền API Key\")\n\n        genai.configure(api_key=API_KEY) \n        \n        # --- CẬP NHẬT MODEL THEO DANH SÁCH KHẢ DỤNG ---\n        # Ưu tiên 1: Gemini 2.0 Flash (Nhanh, thông minh, đời mới nhất)\n        target_model = 'gemini-2.0-flash'\n        \n        try:\n            model = genai.GenerativeModel(target_model)\n            response = model.generate_content(prompt)\n            if response and response.text:\n                return response.text\n                \n        except Exception as e_primary:\n            print(f\"[LLM Info] '{target_model}' gặp lỗi: {e_primary}\")\n            print(\"Đang thử model dự phòng 'gemini-2.0-flash-lite'...\")\n            \n            # Ưu tiên 2: Gemini 2.0 Flash Lite (Nhẹ hơn, dự phòng)\n            try:\n                model = genai.GenerativeModel('gemini-2.0-flash-lite')\n                response = model.generate_content(prompt)\n                if response and response.text:\n                    return response.text\n            except Exception as e_secondary:\n                 print(f\"[LLM Error] Cả 2 model đều lỗi. Chi tiết: {e_secondary}\")\n            \n    except ImportError:\n        print(f\"[LLM Error]\")\n    except Exception as e:\n        print(f\"[LLM Error] Gọi API thất bại: {e}\")\n        # Đoạn code dưới đây giúp bạn xem mình được quyền dùng model nào\n        # Chỉ chạy khi debug để biết tên model đúng\n        try:\n            print(\"Danh sách model khả dụng với Key của bạn:\")\n            for m in genai.list_models():\n                if 'generateContent' in m.supported_generation_methods:\n                    print(f\"- {m.name}\")\n        except:\n            pass\n        print(\"-> Chuyển sang chế độ MOCK response.\")\n\n    # --- MOCK RESPONSE (Fallback) ---\n    time.sleep(1.0)\n\n    # MOCK response (sử dụng retrieved_content trực tiếp + article_snippet)\n    time.sleep(0.6)\n    mock = (\n        f\"Chào bạn, bác sĩ chuyên khoa {specialty} trả lời:\\n\\n\"\n        f\"{retrieved_content}\\n\\n\"\n    )\n    if article_snippet:\n        mock += f\"--- Bài viết tham khảo ---\\n{article_snippet}\\n\\n\"\n    mock += f\"Câu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa {specialty}.\"\n    return mock\n\n# =========================\n# 8. Vòng lặp chính (kết hợp article retrieval)\n# =========================\nFALLBACK = \"Xin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo. Vui lòng đi khám trực tiếp.\"\n\nprint(\"\\n=== Chatbot RAG (PhoBERT Retrieval + Article retrieval + LLM) ===\")\nprint(\"Nhập 'Kết thúc' để dừng.\\n\")\n\nwhile True:\n    try:\n        user_input = input(\"Người dùng: \")\n    except (EOFError, KeyboardInterrupt):\n        print(\"\\n[CHATBOT] Kết thúc phiên.\")\n        break\n    if not user_input or user_input.strip().lower() == \"kết thúc\":\n        print(\"\\n[CHATBOT] Kết thúc phiên.\")\n        break\n\n    topk_results = retrieve_topk_qa(user_input, k=5, question_sim_thresh=0.55)\n\n    # ref_specialty mapping (1..k)\n    ref_specialty = {}\n    for i, r in enumerate(topk_results, start=1):\n        ref_specialty[i] = r['row'].get('topic', 'Y tế chung')\n\n    \n    raw_advice_text, matched_ref_id, orig_qa = find_best_action_sentence_by_embedding_combined(\n        user_input, topk_results, ref_specialty\n    )\n\n    \n    article_meta = None\n    top_articles = retrieve_top_article(query_text=user_input,\n                                        k=1,\n                                        raw_k_multiplier=3,\n                                        article_texts_local=article_texts,\n                                        article_index_local=article_index,\n                                        min_combined_score=0.68)\n    \n    article_snippet = \"\"\n    if top_articles:\n        best_art = top_articles[0]\n        # lưu metadata để dùng lại sau này (in nguyên văn)\n        article_meta = best_art\n\n        # lấy score một cách an toàn với fallback\n        art_score = best_art.get(\"score\",\n                                 best_art.get(\"combined_score\",\n                                              best_art.get(\"baseline_sim\", None)))\n\n        # Nếu không có score, in debug toàn bộ object để kiểm tra schema\n        if art_score is None:\n            print(best_art)\n            art_score = \"N/A\"\n\n        # best_passage là đoạn ngắn nhất phù hợp (đã chọn bằng sim passage) — dùng .get để tránh KeyError\n        article_snippet = f\"Title: {best_art.get('title','')}\\nLink: {best_art.get('link','')}\\n\\n{best_art.get('best_passage','')}\"\n    else:\n        article_snippet = \"\"\n        article_meta = None\n\n    # Nếu tìm được raw_advice_text -> gọi LLM để tổng hợp, kèm article_snippet\n    if raw_advice_text:\n        spec = ref_specialty.get(matched_ref_id, \"Y tế chung\")\n        final_response = generate_natural_response(user_input, raw_advice_text, spec, article_snippet)\n\n        # In kết quả: theo yêu cầu, đảm bảo chứa 3 phần: Chuyên khoa / Lời khuyên / Tham khảo (nguyên văn)\n        print(\"\\n\" + \"*\"*10 + \" CHATBOT TRẢ LỜI \" + \"*\"*10)\n        # Cố gắng chuẩn hóa đầu ra: nếu LLM trả văn bản đầy đủ, in nguyên; thêm phần \"Tham khảo\" rõ ràng\n        print(final_response)\n        print(\"\\n\" + \"-\"*40)\n        # In tham khảo nguyên văn Q/A và article\n        print(\"[THAM KHẢO NGUYÊN VĂN - Q/A]\" )\n        print(orig_qa or \"(không có)\")\n        if article_meta is not None:\n            print(\"\\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\")\n            print(f\"Title: {article_meta.get('title','')}\")\n            print(f\"Link: {article_meta.get('link','')}\")\n            print(article_meta.get('txt','')[:4000])  # in tối đa 4000 ký tự để tránh quá dài\n        else: \n            print(\"\\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\\nKhông có bài viết liên quan!\")\n        print(\"-\"*40 + \"\\n\")\n        continue\n\n    # Nếu không có raw_advice_text -> vẫn thử dùng article để gợi ý cho LLM (tổng hợp từ bài viết)\n    if article_snippet:\n        print(\"[FALLBACK] Không tìm thấy câu hành động trong Q/A, thử tổng hợp từ bài viết liên quan...\")\n        spec_article = \"Y tế chung\"\n        if article_meta:\n            # cố gắng dự đoán chuyên khoa từ title hoặc leave default\n            spec_article = article_meta.get(\"title\",\"Y tế chung\").split()[0]\n        final_response = generate_natural_response(user_input, article_snippet, spec_article, article_snippet)\n        print(\"\\n\" + \"*\"*10 + \" CHATBOT TRẢ LỜI (dựa trên bài viết) \" + \"*\"*10)\n        print(final_response)\n        print(\"\\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\")\n        print(article_snippet[:8000])\n        print(\"-\"*40 + \"\\n\")\n        continue\n\n    # Nếu không tìm gì -> fallback\n    print(\"\\nChatbot: \" + FALLBACK + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T19:17:53.522582Z","iopub.execute_input":"2025-12-04T19:17:53.523260Z","iopub.status.idle":"2025-12-04T19:18:33.832474Z","shell.execute_reply.started":"2025-12-04T19:17:53.523236Z","shell.execute_reply":"2025-12-04T19:18:33.831853Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n[DATA] Đã đọc Q/A từ: /kaggle/input/bacsituvan/bacsituvan.csv (bản ghi: 73)\n[ARTICLES] Đã đọc articles từ: /kaggle/input/articles/bloomax.csv (bản ghi: 378)\n[MODEL] Loading retrieval model vinai/phobert-base ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"[INDEX] Chuẩn bị văn bản cho index...\n[INDEX] Embedded QA 0/73\n[INDEX] QA index built. Num elements: 73\n[INDEX] Article index built. Num elements: 378\n\n=== Chatbot RAG (PhoBERT Retrieval + Article retrieval + LLM) ===\nNhập 'Kết thúc' để dừng.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Người dùng:  e năm nay 26 tuổi bị lõm lồng ngực bẩm sinh, thường hay bị hai bên cạnh sườn, cảm giác mỏi ở vùng ức và có cảm giác khó chịu khó tả. \n"},{"name":"stdout","text":"\n********** CHATBOT TRẢ LỜI **********\nXin chào bạn,\n\nXin lỗi, hệ thống chưa tìm được thông tin phù hợp trong dữ liệu tham khảo để trả lời câu hỏi của bạn về lõm lồng ngực bẩm sinh và cảm giác khó chịu ở vùng ức. Với tình trạng của bạn, Bác sĩ khuyên bạn nên đi khám trực tiếp để được chẩn đoán và tư vấn điều trị cụ thể.\n\nCâu trả lời chỉ mang tính chất tham khảo, bạn nên đi khám trực tiếp tại chuyên khoa Y tế chung.\n\n\n----------------------------------------\n[THAM KHẢO NGUYÊN VĂN - Q/A]\nQ: Cháu là nam ( 18 t ) , hay chạy bộ nhưng dạo gần đây cứ đau chân , nhưng không phải đau kiểu chạy nhiều mà đau . Khi cháu chạy , bàn_chân trái bị hơi tê ở gót_chân và bàn_chân ( cháu nghĩ cái này là bình_thường khi chạy bộ ) , nhưng chân phải cháu bị đau không chỉ vậy mà còn bị đau nguyên khớp dọc từ mắt_cá chân đi lên một khúc và dưới đó một_chút ._Bs cho cháu hỏi như thế là sao ạ , vì khi cháu chạy liên_tục một khoảng thời_gian thì chân phải trở_nên đau_nhói và không_thể dậm chân phải xuống đất được và thường phải co lên ?\nA: @ Phạm_Bình_Bình : Chào_cháu , theo như cháu mô_tả , cháu bị đau có_thể do vận_động nhiều , do đi giày chật khiến máu kém lưu_thông gây đau . Ngoài_ra nguyên_nhân gây đau có_thể do bệnh_lý như viêm cân gan chân hoặc gai gót chân . Tốt nhất cháu nên đi khám tại các bệnh_viện chuyên_khoa xương khớp để xác_định nguyên_nhân và có hướng điều_trị phù_hợp . Đồng_thời để giảm những cơn đau chân , cháu nên dùng sản_phẩm Vindermen_Plus ngày 2 viên chia 2 lần , duy_trì 3 - 6 tháng , giúp tăng sự dẫn_truyền thần_kinh , phục_hồi hư tổn tại sụn khớp , giúp giảm đau nhức hiệu_quả . Kết_hợp với sản_phẩm Vipteen ngày 4 viên chia 2 lần cung_cấp các thành_phần : MK7 , Canxi nano , Zn nano , Magie , Vitamin_D3 , ... có tác_dụng bổ_sung vi_chất cần_thiết cho sự phát_triển của khung xương , giúp cháu có được chiều cao tối_ưu , và giúp xương chắc khỏe , ngăn_ngừa các bệnh về xương khớp . Bên_cạnh đó trong thời_gian này cháu nên nghỉ_ngơi . Tránh đứng ngồi ở một tư_thế quá lâu , hoặc vận_động , chạy_nhảy nhiều , đi giầy đúng kích_cỡ chân . Chườm đá 20 phút từ 3-4 lần mỗi ngày để giảm các cơn đau gót chân . Chúc cháu sức_khỏe !\n\n[THAM KHẢO NGUYÊN VĂN - ARTICLE]\nKhông có bài viết liên quan!\n----------------------------------------\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Người dùng:  Kết thúc\n"},{"name":"stdout","text":"\n[CHATBOT] Kết thúc phiên.\n","output_type":"stream"}],"execution_count":18}]}